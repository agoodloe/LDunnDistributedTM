% !TeX document-id = {beb7ced9-b3cd-42b2-b16a-3ed3c633a1d9}
\documentclass[]             % options: RDPonly, coveronly, nocover
{NASA}                       %   plus standard article class options
%\DeclareRobustCommand{\mmodels}{\mathrel{|}\joinrel\Relbar}

\usepackage[utf8]{inputenc}
\usepackage{setspace}
\usepackage{csquotes}
\usepackage{hyperref}
\usepackage{amsmath, amssymb, amscd, amsthm, amsfonts}
\usepackage{mathtools}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{amsthm}
\usepackage[english]{babel}
\usepackage{stmaryrd}
\usepackage{proof}
\usepackage{tikz-cd}
\tikzcdset{scale cd/.style={every label/.append style={scale=#1},
    cells={nodes={scale=#1}}}}
% Added for subfigures
\usepackage{caption}
\usepackage{subcaption}
\usepackage{afterpage}
\usepackage{comment}
\usepackage{rotating}%sidewaysfigure
\usepackage{pdflscape}%alt to sidewaysfigure

\newtheorem{theorem}{Theorem}[section]
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{lemma}[theorem]{Lemma}
\theoremstyle{definition}
\newtheorem{example}[theorem]{Example}
\newtheorem{definition}[theorem]{Definition}
\include{macros.tex}

% Globally redefine pgfpicture to use \Large fonts
\let\origpgfpicture=\pgfpicture
\def\pgfpicture{\origpgfpicture\small}

% Try loading this package to prevent so much hyphenation
% as recommended by https://stackoverflow.com/questions/1609837/latex-breaking-up-too-many-words
\usepackage{microtype}

\title{Distributed Systems Challenges in Public Safety Networks}

\author{Lawrence Dunn and Alwyn E. Goodloe}

\AuthorAffiliation{Lawrence Dunn \\ Department of Computer and Information
  Science \\ University of Pennsylvania \\ Philadelphia, PA \\ Alwyn Goodloe\\                                          % for cover page
  NASA Langley Research Center, Hampton, Virginia
}
\NasaCenter{Langley Research Center\\Hampton, Virginia 23681-2199}
\Type{TM}                    % TM, TP, CR, CP, SP, TT
\SubjectCategory{64}         % two digit number
\LNumber{XXXXX}              % Langley L-number
\Number{XXXXXX}              % Report number
\Month{12}                   % two digit number
\Year{2022}                  % four digit number
\SubjectTerms{Distributed Systems, Formal Methods, Logic, }     % 4-5 comma separated words
\Pages{46}                   % all the pages from the front to back covers
\DatesCovered{}              % 10/2000--9/2002
\ContractNumber{}            % NAS1-12345
\GrantNumber{}               % NAG1-1234
\ProgramElementNumber{}
\ProjectNumber{}             % NCC1-123
\TaskNumber{}                % Task 123
\WorkUnitNumber{}            % 123-45-67-89
\SupplementaryNotes{}
\Acknowledgment{The work was conducted during a summer internship at the NASA Langley Research Center in the Safety-Critical Avionics Systems Branch focusing on distributed computing  issues arising in the Safety Demonstrator challenge in the NASA Aeronautics System Wide Safety (SWS) program.}

%Added for Pandoc
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}


\abstract{The System Wide Safety (SWS) program has been investigating
  how crewed and uncrewed aircraft can safely operate in shared
   airspace. Enforcing safety requirements for distributed agents
  requires coordination by passing messages over a communication
  network. Unfortunately, the operational environment will not admit
  reliable high-bandwidth communication between all agents,
  introducing theoretical and practical obstructions to global
  consistency that make it more difficult to maintain safety-related
  invariants. Taking disaster response scenarios, particularly
  wildfire suppression, as a motivating use case, this self-contained
  memo discusses some of the distributed systems challenges involved
  in system-wide safety through a pragmatic lens. We survey topics
  ranging from consistency models and network architectures to data
  replication and data fusion, in each case focusing on the practical
  relevance of topics in the literature to the sorts of scenarios and
  challenges we expect from our use case.  }

\begin{document}
\newpage
\setcounter{tocdepth}{2}
\tableofcontents
\newpage

\section{Introduction}
\label{sec:introduction}
Civil aviation has traditionally focused primarily on the efficient
and safe transportation of people and goods via the airspace. Despite
inherent risks, the application of sound engineering practices and
conservative operating procedures has made flying the safest mode of
transport today. Now, the industry's strong requirements for safety
make it difficult to integrate unmanned vehicles into the airspace,
accomodate emerging applications, and keep pace with significant
recent growth in commercial aviation. To that end, the NASA
Aeronautics' Airspace Operations and Safety Program (AOSP) has
initiated the System Wide Safety (SWS) project to investigate
technologies and methods to enable crewed and uncrewed aircraft to
safely operate in shared airspace.

This memo surveys topics in computing that are relevant to maintaining
system-wide safety across large, physically distributed data and
communication systems. It is intended to be self-contained and
accessible to a technical audience without a deep background in
distributed systems. Our primary motivating use cases come from civil
emergency response scenarios, especially wildfire suppression and
hurricane relief. These were chosen primarily for three
reasons. First, improved technology for wildfire suppression,
especially related to communications and data sharing, is frequently
cited as a national priority \cite{pcast2023}.  Second, the rules for
operating in the US national airspace are typically relaxed during
natural disasters and relief efforts, so this is a suitable setting
for testing new technologies. Finally, this setting is an excellent
microcosm for the sorts of general challenges faced by other,
non-emergency applications.

One theme uniting this manuscript is \emph{continuity} in the sense
considered by topology.\footnote{For an introductory textbook see
  \cite{mendelson2012introduction}.}  The systems we examine must
function under harsh operating conditions that limit their
performance---for example, wireless communication is less reliable
during severe weather. To design a system whose behavior and
performance is predictable---this is clearly a prerequisite for
safety---it must flexible enough to perform reliably under a wide
range of adverse conditions. In other words, the behavior of a safe
system should in some sense be a \emph{continuous} function of its
inputs and environment. Achieving this sort of robust design is
challenging because distributed systems designers must navigate
delicate tradeoffs between competing objectives. At a high level,
these tradeoffs stem from an inherent tension between designing a
system that handles user requests quickly and one that prioritizes
maintaining a strong level of global consistency between system
components.

The key insight the reader should take away from this document is that
achieving system-wide safety is not solely a matter of improving
communications hardware and physical infrastructure. It is also in
large part a computer science and software problem concerned with the
high-level applications that execute on top of the communications
infrastructure. These software systems must be engineered to maintain
a common operating picture between distributed users while making
efficient use of networking and compute resources in a dynamic and
even adversarial environment.

\subsection{Summaries of the sections}
\label{ssec:summaries-of-the-sections}

Section \ref{sec:disaster-response} opens with a practical overview of
disaster response and some of the computing challenges encountered in
this setting. Real-world examples from disaster response scenarios are
presented that demonstrate the role of distributed systems principles
in achieving system-wide safety.

Section \ref{sec:background} summarizes fundamental concepts in
distributed systems, culminating in a illustrative result: the ``CAP''
theorem, which we present for both the linearizable and sequential
consistency models (Theorems \ref{thm:cap} and
\ref{thm:cap-sequential}, respectively). CAP is considered a
``negative'' result because it proves that something cannot be
done. Namely, a distributed system cannot guarantee that it will
maintain strong consistency and remain available to users at all
times, even if the network fails. The practical takeaway of the CAP
theorem is that agents in emergency response environments will always
act with incomplete information about the global system, a key
motivation for Section \ref{sec:continuous-consistency}. While the CAP
theorem is often presented as a kind of unfortunate prohibition, it
merely highlights a general kind of tradeoff. Furthermore, real-world
systems often exhibit a kind of ``locality'' that works around some of
the limitations implied by the CAP theorem.

% Section \ref{sec:networking} examines networking considerations. Our
% vision of future emergency communication networks integrates
% concepts from delay/disruption-tolerant networks (DTN) and mobile
% ad-hoc networks (MANET) to provide digital communications that are
% robust to a turbulent operational environment. We also examine the
% state of software-defined networking (SDN). SDN is an emerging field
% that puts networking protocols on the same footing as ordinary
% computer programs. In theory, this should furnish computer
% networking with all the benefits of modern software engineering,
% such as reprogrammable hardware, rapid iteration, version control,
% and especially formal verification.

Section \ref{sec:continuous-consistency} describes a hypothetical
application suitable for networks with frequent disruptions: a data
replication service built on the theory of ``conits'' (short for
``consistency unit'') investigated by Yu and Vahdat
\cite{2002tact}. This framework provides a \emph{continuous}
consistency model that balances the competing objects of consistency
and availability in a quantifiable and controllable way. The idea is
that applications can tolerate some level of inconsistency between
replicas of a data item as long as it can enforce an upper bound on
the allowable divergence between them. A conit-based framework would
allow applications to define units of replicated data whose
consistency is of interest, enforce policies bounding inconsistency
between replicas of these items, and change these policies
dynamically. This sort of service can provide the sort of strict
guarantees required of safety-related use cases while tolerating the
adverse environments and real-world limitations of the environment.

We conclude in Section \ref{sec:conclusion} by recapping some of the
main themes in this document and highlighting areas for further
investigation. Ultimately, building distributed systems for any
environment involves design decisions tuned to the particulars of that
environment. We expect that many of these decisions cannot be made a
priori---they will ultimately be informed by a combination of
simulation and testing in the field.

\begin{comment}
Section \ref{sec:data-fusion} concerns data fusion. Now and in the
future, agents in disaster scenarios will make decisions informed by
many different kinds of information. Efficient integration,
processing, filtering, and dissemination of this information will be
necessary to avoid ``swimming in sensors and drowning in data''
\cite{2010:magnuson}.  This task is especially challenging because
agents will often work with incomplete or out of date information, and
different sources of the same data may be contradictory, e.g. first
responders may receive contradictory reports about whether a structure
is occupied. One promising trend in this space, which we briefly
introduce in this section, is the development of sheaf theory as a
natural mathematical model for data fusion
\cite{2017robinsonCanonical}. Sheaf theory provides a rigorous
framework for discussing how heterogeneous sources of noisy data can
be integrated into a coherent picture, and can formally measure how
well this task has been achieved.

We conclude in Section \ref{sec:conclusion} by recapping some of the
main themes in this document and highlighting areas where design
decisions at various levels must be made to build a system that is
tuned to the exact conditions we can expect from real-world
scenarios. Such decisions might be informed by a combination of
simulation and experimentation in the field.
\end{comment}

\section{Coordination Challenges in Disaster Response}
\label{sec:disaster-response}
This section explores key aspects of disaster response, particularly
firefighting, that shape the focus of this document. We highlight how
real-world environments create fundamental challenges that require
solutions based on distributed computing principles. Even with the
best communications technologies, core issues arise when distributed
agents need to coordinate their actions across wide areas.

Disaster response settings, like wildfire suppression or hurricane
relief, are marked by systemic communications challenges. A 2023
report by the President’s Council of Advisors on Science and
Technology (PCAST) highlights the need to address ``the
vulnerabilities and shortfalls in wildland firefighter communications,
connectivity, and technology interoperability'' as its top
recommendation for wildland firefighting modernization
\cite{pcast2023}. Many of these vulnerabilities and shortfalls stem
from factors inherent to disaster response: remote locations,
difficult terrain, damaged infrastructure, harsh weather, and limited
power, to name a few.

Field agents often face high message loss, distorted signals, and
unpredictable delays in communication. A cautious approach suggests
preparing for the worst performance at critical times---network
failures often coincide with the sorts of conditions that demand
urgent, reliable contact. Disasters often damage and degrade the
communications infrastructure, which is accompanied by a sudden surge
in user demand that can overwhelm a network completely. This was
starkly evident in the immediate aftermath of the September
$11^\textrm{th}$ attacks, when sudden user demand and severed trunk
cables crippled New York public and private communication networks,
including dedicated networks for first responders
\cite{2011:Reardon}. These failures later became the impetus for the
creation of FirstNet \cite{2021:firstnet, 2021:firstnet2}, a national
public safety broadband network (NPSBN).

Unreliable networks make coordinating distributed agents a significant
challenge. Coherent decision-making and coordinated action require
consistency, meaning agreement on the data shared between agents. We
define consistency more precisely in Section \ref{sec:background}, but
the idea is clear: it is critical for everyone to agree which
firetrucks should respond to which areas, where helicopters should
land, which tasks should be prioritized, or which radio frequencies
are in use. Achieving stronger standards for consistency requires
sending more information in a shorter time frame, which places a
heavier strain on the network. When a communications link is slow,
system components may have to pause and wait before agreement can be
reached, diminishing the efficacy of the system. To avoid waiting in
such scenarios, standards for consistency may have to be relaxed,
meaning distributed agents have less agreement, which comes with its
own challenges. In sum, there is an inherent tension between
consistency and latency (delay) of action.

\subsection{Communication and User Safety}
\label{ssec:communication-and-safety}
We turn our attention to the implications of the consistency/latency
tradeoff from a user safety perspective. Operational safety depends on
agents quickly gathering and responding to information about their
environment. This information is relayed through communication
networks, so poor communication becomes a safety problem. When
communication falters, agents face a difficult choice: either wait for
more information before acting, or acting now with incomplete %TODO should be act now
knowledge. Both inaction and uninformed action carry risks. This
dilemma is closely related to a fundamental computer science principle
known as the safety/liveness tradeoff.

\begin{figure}
  \centering
  \includegraphics[scale=0.4]{images/dc10.jpg}
  \caption{A DC-10 airtanker, rated for 9,400 gallons, drops retardant
    above Greer, Arizona. Image source: Kari Greer/US Forest Service.}\label{fig:airtanker}
\end{figure}
% TODO: How to cite picture?
% https://www.flickr.com/photos/apachesitgreavesnf/5837741382
% Also appears at https://www.nifc.gov/resources/aircraft/airtankers

\begin{figure}
  \centering
  \includegraphics[scale=0.15]{images/forestfire-videox-scaled.jpg}
  \caption{Screenshot of a firefighter using ATAK, where the left
    panel shows a map and the right is a video stream from an air
    vehicle. Image source: Andreas ``AJ'' Johansson}\label{fig:atak}
\end{figure}
% TODO: How to cite picture?
%https://www.civtak.org/2020/08/04/tak-used-in-ca-firefighting-w-aircraft-video/

For example, consider the use of firefighting airtankers, particularly
Very Large Airtankers (VLATs), which can carry over 8,000 gallons of
water or fire retardant \cite{2019:airtankerops}
(Figure \ref{fig:airtanker}). The largest VLATs can drop more than
20,000 gallons---about 170,000 pounds' worth---in a single pass. In
the U.S., these drops are typically made from just 250 feet above the
tree canopy \cite{2019:airtankerops}, and sometimes lower in
practice. This sort of maneuver can easily crush a ground vehicle
\cite{2019:stickney}. In 2018, a firefighter was killed, and three
others were injured, when an unexpectedly powerful drop from a Boeing
747-400 Supertanker knocked down an 87-foot Douglas Fir tree
\cite{2018:calfire}.

Improving firefighter communications can be expected to lead to better
safety outcomes. One such improvement is the through the use of
applications like ATAK---the Android Team Awareness Kit, developed by
the U.S. military in 2010 and later released in a civilian
version. Wildland firefighters are increasingly using ATAK, extended
with aftermarket plugins, on ordinary cell phones to coordinate their
activities in the field (Figure \ref{fig:atak}). A key application of
this tool could be tracking the real-time GPS locations of
firefighters for safety monitoring.

Given the risks of VLAT drops, a seemingly reasonable safety measure
might be to disallow drops unless a VLAT's computers have up-to-date
information about the location of ground personnel. Unfortunately,
system-wide safety is not so easily achieved, as the proposed measure
is precisely the sort of thing subject to the safety/liveness
tradeoff. Here, it is important to recognize a linguistic nuance: in
the context of distributed systems, ``safety'' refers to a specific
type of system property; the concept in not inherently related to the
safety of people. A \emph{safety} property is a prohibition that stops
a system from taking an action that might be ``bad'' in some way. Here
is an exemplary safety property for the example above:
\begin{quote}
  $\textbf{P}_\textrm{safe}$: Ground agents are known to be at least
  100 feet outside the drop zone, and this information is current to
  within 30 seconds, or airtankers will not perform a drop.
\end{quote}
By contrast, a \emph{liveness} property demands some kind of action
from a system, usually one that is ``good'' in some way. A
characteristic of liveness properties is that they place an upper
bound on the allowable delay of something. An exemplary liveness
property for our scenario might be the following:
\begin{quote}
  $\textbf{P}_\textrm{live}$: A VLAT on the ground will take off and
  perform a drop within 20 minutes of receiving a request from an
  incident commander. \footnote{The Chief of Flight Operations for Cal
    Fire cited 20 minutes as an upper bound on the response time for
    aerial firefighting units within designated responsibility areas
    in an interview with PBS \cite{2021:aerialfirefighting}.}
\end{quote} Note that $\textbf{P}_\textrm{live}$
is a liveness property, not a safety property in the narrow technical
sense, but it impacts human safety: it might be critical for VLATs to
perform drops quickly if a wildfire is threatening the safety of
ground personnel.

Safety and liveness are frequently dual mandates that cannot be
guaranteed simultaneously. Such is the case in our example: though
$\textbf{P}_\textrm{safe}$ and $\textbf{P}_\textrm{live}$ are both
desirable, certain situations will force policymakers to prefer one
over the other. Consider the fact that the wildland firefighting
environment is frequently GPS-denied: heavy smoke, multipath effects,
and so on can easily prevent a consumer-grade cellphone from obtaining
reliable GPS coordinates. Additionally, factors like a damaged radio
tower or environmental obstructions like a tall mountain can prevent
communications between the air and ground. Such conditions would
prevent a VLAT's computers from knowing the locations of ground
agents, which immediately presents a dilemma: should the crew proceed
without knowing the locations of ground personnel, maintaining
$\textbf{P}_\textrm{live}$ at the cost of $\textbf{P}_\textrm{safe}$,
or should it be cautious and wait for more information, maintaining
$\textbf{P}_\textrm{safe}$ at the cost of $\textbf{P}_\textrm{live}$?
There is no simple answer, with either choice presenting a downside
with respect to the broader goal of system-wide safety.

Besides the safety/liveness tradeoff, the previous example exhibits
two other themes important in distributed systems, both of which will
be explored further in this document. The first is the
\emph{epistomological} nature---concerned with what information is
\emph{known} by \emph{whom}---of reasoning about distributed
systems. This aspect is reflected in wording of
$\textbf{P}_\textrm{safe}$ in VLAT example: Ground agents are known
(by the VLAT's computers) to be outside of a dangerous area. This
situation requires a deeper and more sophisticated analysis than one
simply considering what is true, but not necessarily
known. Mathematically, the logic of distributed agents is not the
ordinary propositional logic but the modal logic S5, which extends
propositional logic with additional axioms governing
knowledge.\footnote{The application of S5 to reason about distributed
  systems is the topic of \cite{kshemkalyani_singhal_2008}, Chapter
  8.} Distributing knowledge requires communication between agents
over a period of time over the network, which is not instantaneous and
reliable, and it is from these imperfections that the safety/liveness
tradeoff arises.

The second aspect exhibited above, albeit negatively, is that of
\emph{continuity}. A continuous system can flexibly adopt to its
environment, but a discontinuous system is rigid and may exhibit
suddenly different behavior in response to only small changes in the
environment. The properties $\mathbf{P}_\textrm{safe}$ and
$\mathbf{P}_\textrm{live}$ exhibit a stark lack of continuity because
they are inflexible, all-or-nothing propositions. Suppose that agents
are known to be $500$ feet outside the drop zone, but the information
is only current to within 31 seconds---this extra second technically
violates $\mathbf{P}_\textrm{safe}$, though it should be inferrable
that the ground agents are well away from danger. In particular, this
example highlights that system-wide safety is more of a quantitative
concept than a Boolean (true-or-false) one. A distributed system in a
network-challenged environment should exhibit smoothly varying
properties in response to its inputs, and ideally allow ``tuning'' the
system's properties for the particulars of its environment at any
moment. The technical aspects of this theme are the focus of Section
\ref{sec:continuous-consistency}.

\subsection{Communication Patterns in the Field}
\label{ssec:communication-patterns}
We now consider some of the communication patterns that occur in
wildland firefighting. Readers may be surprised to learn that the
state of the art is somewhat primitive, largely due to the sparse
permanent communications infrastructure that exists in this
setting. This makes wildfires an interesting and generalizable example
for other kinds of civil disaster environments where the network is
unreliable.

One important concept to draw attention to is a kind of ``geospatial
locality of reference'' that system designers should consider. By
this, we mean the concomitance of two observations which, while not
guaranteed rules, are approximately true in many circumstances. The
first observation states that nearby agents have aligned interests:
\begin{quote}
  $\textbf{O}_1$: Agents with the most urgent need to coordinate their
  actions will usually be located closer together and require similar
  kinds of information.
\end{quote}
The second observation states that nearby agents have more reliable
communications:
\begin{quote}
  $\textbf{O}_2$: Agents that are located closer together generally
  enjoy more reliable communications between them than agents that are
  far apart. Conversely, information that travels long distances tends to
  be delayed or degrade in quality.
\end{quote}

These related observations are what is meant by simply the
``locality'' principle. Locality is a crucial factor to analyze
because, as presented in Section \ref{sec:background}, there are major
theoretical and practical limits to how well agents can coordinate
\emph{globally}, meaning with all agents knowing and agreeing on
everything. To the extent the system exhibits locality, coordination
can be achieved using more efficient short-range communication than
less efficient long-range communication. Here, ``efficient'' should be
read broadly, measured with respect to things like battery life,
message delay, reliability, cost-effectiveness, equipment weight, and
so on. This raises the question of how to most efficiently utilize
network resources to achieve adequate levels of consistency. This
question is revisited in Section \ref{sec:continuous-consistency}.

\subsubsection{Communication on the Ground}
\label{sssec:ground-communication}
In the field, communication between firefighters and other agents is
often facilitated by handheld (analog) land-mobile radios, which are
inherently limited in their battery life, bandwidth, effective range,
and ability to work around environmental factors like foliage and
smoke.

As an alternative to using a radio, it is common for wildland
firefighters in the field simply to shout commands and notifications
to nearby personnel. This exhibits the locality principle: a
substantial amount of communication occurs directly between nearby
firefighters working on closely related tasks that can communicate
without network infrastructure. In a future environment where agents
might be equipped with body-worn sensors and or even some form of
heads-up display (HUD), this sort of low-range local communication
might be facilitated by relatively inexpensive, low-power technologies
such as Bluetooth, without the need for more sophisticated (and heavy)
equipment.

Communication over a long distance requires infrastructural support,
such as the use of cell towers and repeater stations. Typically,
disaster response environments have scarce permanent infrastructure:
in a wildland fire setting, perhaps a few repeaters mounted to a
nearby watch tower. Ad-hoc infrastructure, such as Cells On Wheels
(COWs) or Cells on Light Trucks (COLTs)---i.e. portable cellular
towers---can sometimes be deployed on an as-needed basis if the
location allows for it. Similar kinds of equipment can also be mounted
to backpacks and carried into the field by specially-trained users. A
common issue is making sure that all equipment is properly configured,
for instance that radios are listening on the correct
frequencies. Configuration is especially critical when different
agencies and groups need to interoperate---another problem highlighted
during the September $11^\textrm{th}$ attacks.

\begin{figure}[t]
  \centering
  \includegraphics[scale=0.085]{images/ironside.jpg}
  \caption{The Ironside Mountain lookout and radio repeater station,
    shown here with protective foil on August $10^\textrm{th}$, 2015
    during the 2015 River Complex fire. This particular fire burned
    77,077 acres over 77 days.}
  \label{fig:ironside}
\end{figure}
% TODO: How to cite picture?
%https://web.archive.org/web/20150923190323/http://inciweb.nwcg.gov/incident/photograph/4431/44/45122/

Use of centralized infrastructure comes with the potential for
widespread failure when the infrastructure breaks down. For example,
in California, the Ironside Mountain lookout/repeater station (seen in
Figure \ref{fig:ironside}) was destroyed during the 2021 Monument
Fire, which burned approximately 223,124 acres over 88 days
\cite{2021:monumentfire}. The Ironside Mountain station had strategic
importance, being located on a tall ridge. According to a video blog
from a volunteer firefighter involved in the incident, its loss
prevented communication between operators on different sides of the
ridge, in networking parlance creating a \emph{partition} that lasted
until crews could ascend the ridge to deploy a temporary station:
\begin{quote}
  ``When {[}the Ironside Mountain lookout station{]} burned down the
  radio repeater went with it. And so communications were lost across
  the fire\ldots{} one side of the fire couldn't talk to the other
  side\ldots.  So it was kind of a critical job to get that road
  cleared so that the radio crews could go back up there and set up a
  temporary radio tower.'' \cite{2022:mechfire}% See also https://web.archive.org/web/20220809061927/https://www.youtube.com/watch?v=4F2dDKMgAME
\end{quote}
A scenario where communication between two groups is completely
severed is exactly the sort of thing considered by the CAP theorem in
Section \ref{sec:background}.

\paragraph{Ground vehicles}
Large numbers of ground vehicles---sometimes on the order of 100
during a major response---are involved in wildfire
suppression. Besides various types of firetrucks, bulldozers and
similar vehicles are commonly used to control the landscape and
perimeter of the fire. An advantage of vehicles is that they can carry
heavier and higher-power communications equipment than a human. For
instance, a vehicle could be equipped with a BGAN or VSAT satellite
terminal to maintain a high-bandwidth connection back to a central
location. Additionally equipping the vehicle with something like a
Wi-Fi or cellular base station using the satellite connection as
backhaul would let the vehicle act as a bridge between agents in the
field and central coordinators such as incident commanders or 911
dispatchers.

\subsubsection{Communication in the Air}
Wildland firefighting increasingly involves the use of helicopters and
fixed wing aircraft. Civil aviation has traditionally employed simpler
communication patterns than this use case demands. For instance,
aircraft equipped with Automatic Dependent Surveillance-Broadcast
(ADS-B) monitor their location using GPS and periodically broadcast
this information to air traffic controllers and nearby aircraft. This
sort of scheme has worked well in traditional applications, where
pilots typically only monitor the general locations of a few nearby
aircraft. The locality principle is exhibited here, too: aircraft have
the highest need to coordinate when they are physically close and
therefore in range of each other's ADS-B broadcasts.

In our setting, a large number or aircraft, easily on the order of 10 or
more, may need to operate in a small area, near complex terrain,
during adverse conditions, often at low altitude. In other words, the
demands are many and the margins for error are small. This sort of use
case calls for more sophisticated coordination schemes between
airborne and ground-based elements than solutions like ADS-B provide
by themselves.

As aircraft generally have better line-of-site to ground crews than
ground crews have to each other, firefighters sometimes relay messages
to air-based units over the radio, which in turn is relayed back down
to other ground units. The locality principle comes into play for this
sort of message relaying scheme, but in the negative direction:
relaying allows knowledge to travel farther but requires more resources and effort,
and the extended reach comes at the cost of introducing delays and
possible degradation of message quality, as in the classic game of
``telephone.'' Hence, this mode of communication has generally been reserved for
more critical information.

The Communications Program of the Civil Air Patrol (a civilian
auxiliary of the U.S. Air Force) is sometimes deployed to provide
communications for firefighters on the ground using airplane-mounted
radio repeaters. Air-based repeaters are better than the ``telephone''
scheme in the previous paragraph as they do not require as much human
intervention to receive and re-transmit information. That is, this
form of relaying is more \emph{transparent}. In this future, this sort
of service could be provided autonomously by portable infrastructure
futuremounted to unmanned aerial vehicles (UAVs), which might perform
additional functions such as tracking the fire perimeter.

In future environments, we envision resilient networks formed from
heterogeneous collections of smaller networks, incorporating various
communication technologies such as digital radios, Wi-Fi, 4G/LTE, and
satellite communications. Communications in the field may incorporate
aspects of mesh networks and mobile ad-hoc networks (MANETs). The
infrastructure will be comprised of both permanent infrastructure and
temporary equipment carried into the field by responders, vehicles on
the ground or in the air. Given the environmental challenges, we
assume that two agents will often only have intermittent end-to-end
connectivity, if any. Facilitating communication through such a
dynamic and chaotic mobile network calls for a disruption-tolerant
networking (DTN) architecture, which provides a custody transfer and
store-carry-forward model that is resilient to disruption
\cite{2021:intro-dtn}. The exact form of such a network remains an
open question for future investigation.

\subsection{Data Collection and Processing}
\label{ssec:data-collection}
Perhaps the most universally acknowledged expectation for future
disaster response environments is a heavy reliance on data
gathered from both humans and sensors. Besides improvements to
communications that facilitate information sharing, we expect advances
in machine intelligence to greatly influence how this data is
handled.

Agents in disaster response environments will be both producers and
consumers of data, and this data will need to processed by humans and
machines in ways that agents can readily make sense of to support
their decision-making. Just some of the possible sources and types of
pertinent data are as follows:
\begin{itemize}
\item Free-form communication, especially real-time or recorded voice messages
  broadcast to many agents at once, which may need to be processed by
  machines to extract the most pertinent information into a more
  actionable format
\item The exact or estimated location of victims, firefighters,
  vehicles, hazards, and so on displayed on applications like ATAK
\item Medical information gathered from victims, perhaps stored in and
  collected from electronic triage tags \cite{2009:triagetag}
\item Data about current and predicted fire behavior gathered from
  systems like the Fire Integrated Real-time Intelligence System
  (FIRIS) or NASA's Fire Information for Resource Management System
  (FIRMS)
\item Weather data from the National Weather Service
\item Topographic information about the terrain, highlighting for
  instance the location of rivers and roads that could form a fire
  control line
\item Planned escape routes, rendezvous points, safety zones, and
  landing zones
\item Availability and dispatching of assets, e.g.~ambulances,
  airtankers, or crews on standby, such as the prototype application
  considered by Monares et al. \cite{2011:monares}
\end{itemize}
In a perfect environment, such information would be shared with all
necessary agents in whole and instantly. In reality, agents will be
presented with information that is sometimes incomplete, out of date,
or contradictory---all problems that are further exacerbated by an
unreliable network. A competing concern is that the information
presented will be \emph{overcomplete}, filled with petty details that
distract agents from their important tasks.

In some ways, future systems for disaster response will bear
resemblence to future systems for warfighting, such as the conceptual
\emph{Internet of Battle Things} (IoBT) \cite{2016:iobt}. To quote
from that paper, agents ``under extreme cognitive and physical
stress'' will be subject to a highly dynamic and dangerous
environment. Various kinds of technology will assist humans by
providing data to support sensemaking, but a contraindicating concern
will be flooding agents with a ``massive, complex, confusing, and
potentially deceptive ocean of information.'' To avoid ``swimming in
sensors and drowning in data'' \cite{2010:magnuson}:
\begin{quote}
``Humans seek well-formed, reasonably-sized, essential information
  that is highly relevant to their cognitive needs, such as effective
  indications and warnings that pertain to their current situation and
  mission.'' \cite{2016:iobt}
\end{quote}

We propose that researchers in field of Human-Computer Interaction
(HCI) should take up the question of how public safety agents under
stress can process and respond to the flood of information they may
face as public safety systems become more complex. HCI has been
defined as follows:
\begin{quote}
  ``A subfield within computer science concerned with the study of the
  interaction between people (users) and computers and the design,
  evaluation and implementation of user interfaces for computer
  systems that are receptive to the user's needs and habits.'' \cite{2009:hci-definition}
\end{quote}
HCI research offers insights into how interfaces should be structured
to avoid cognitive overload and facilitate intuitive control without
distracting users. As emergency control becomes increasingly
data-driven, particularly in centralized hubs like dispatch centers
that aggregate diverse streams of information, the challenge of
ensuring that users can interact effectively with these systems will
become increasingly important.

\subsubsection{Adversarial Behavior}
One feature of the Internet of Battle Things worth highlighting is
``the adversarial nature of the environment.'' This feature is common
also to disaster environments, whether resulting incidentally from
``fog of war'' effects or deliberately caused by malicious actors
seeking to exploit a civil disaster. Section
\ref{ssec:communication-patterns} cited a real-world example of a
critical communications station destroyed by wildfire, perhaps
comparable to an attack by enemy forces.

There is a growing trend in disaster response to rely on
``crowdsourced'' information, where public safety officials process
reports from the general public over non-traditional channels like
social media. However, a significant vulnerability of crowdsourcing
information is the potential for confusing of contradictory reports,
which can resemble intentional deception. Rumors frequently plague
disaster relief environments, which are quite susceptible to
misinformation. For instance, during Hurricane Harvey in 2017, there
were unconfirmed rumors of shots being fired at volunteer rescuers
\cite{2017:cajun-navy-rumors}. Tracing reports back to their source is
often difficult. Even fully malicious activity like
``swatting''---placing a fake 911 call to cause a large police
response---is often observed in public safety. Whether misinformation
is spread with malicious intent or through well-meaning confusion, the
proliferation of false information in this chaotic environment can
have adversarial effects. This should be anticipated as part of a
careful approach to modernizing systems in this space.

\subsubsection{Allocation of Network Resources}
Communications in disaster response environments might even be less
reliable than in the battlefield (setting aside offensive behavior
like signal jamming), requiring a greater emphasis on the preservation
of scarce network resources. For instance, a group of volunteer
firefighters would have fewer resources than a tactical military unit,
relying on commercial off-the-shelf (COTS) equipment rather than best
in class hardware like sophisticated handheld satellite links. High
bandwidth channels will often be in short supply, while adverse
conditions like inclement weather are assumed.

Given the heavy reliance on data and the scarcity of reliable
communication channels, we expect a complex interaction between the
high-level needs of distributed applications (e.g. an application for
sharing real time weather data) and low-level concerns about network
resources. This is because only the applications have enough
information to determine which data is the most important and must be
shared with whom first, while only the network-level protocols have
enough information and control to make prudent use of scarce network
availability.

There is a widely accepted wisdom in computing---the end-to-end
principle \cite{1984:end-to-end}---which suggests roughly that
applications should not make assumptions about the network, and that
the network should be relatively agnostic to high-level application
logic. However, in natural disaster environments where resources are
scarce and reliable communication is critical, these subsystems may
need to be more tightly integrated in how they influence each other to
achieve the best performance. This approach would not contravene the
end-to-end principle, but would involve carefully considering its
application in this relatively extreme context.

Consider, say 5 to 10 years in the future, a centralized data fusion
application that running in an edge data center.\footnote{An
  \emph{edge} data center is one located closer to a network's edge,
  nearer to users, to provide low-latency communications for
  time-sensitive applications. Edge centers support applications that
  require significant amounts of information processing---enough that
  the application must be hosted in a datacenter, where compute
  resources can be scaled dynamically, rather than colocated with
  users where resources are limited.} This application could detect
critical events like a fire crossing a control line (a phenomenon
called \emph{slopover}) and alert ground responders. It might also
warn responders who have strayed too far from an escape route or
safety zone. These are high-priority notifications, so it would be
worthwhile to allocate scarce network resources to convey them to the
relevant parties in real-time.

On the other hand, while it may be beneficial for each firefighter to
have real-time information about the GPS location of every other
firefighter, this may not always be critical. If transmitting this
data strains the network, then perhaps only the general location of
other crews or nearby teams should be sent. If the network is
extremely constrained, communication may be restricted to only
information strictly relevant to preserving life to ensure this is
delivered swiftly and reliably. Thus, network allocation is a dynamic
calculation influenced both by the criticality of the information
(which is determined by the application logic) and the availability of
network resources at a particular location. Advanced systems should
provide mechanisms like Quality of Service (QoS) values to allow
prioritizing certain communication. Such mechanisms can be
incorporated into a control loop where applications generate feedback
that drives the decision-making process in lower-level parts of the
network. However, traditional QoS mechanisms may not be enough---even
the routing protocol of the network may need to be more specialized to
higher-level applications than in traditional environments.

%TODO: Mention DTN again


\section{Introduction to Distributed Systems}
\label{sec:background}
In this section, we distill two core topics in the theory of
distributed systems: causality and timekeeping, along with shared
memory consistency.  Our discussion is primarily informed by the
manuscripts of Coulouris et al.  \cite{coulouris2005distributed} and
Kshemkalyani and Singhal \cite{kshemkalyani_singhal_2008}. We focus on
building applications relevant to the scenarios described in Section
\ref{sec:disaster-response}, aiming to highlight obstacles and
strategies for developing distributed systems that can endure the
delays and disruptions inherent to these communication-challenged
environments. Readers wishing for a condensed summary of the main
highlights may skip to Section \label{ssec:background-summary}.

At its core, a distributed system is a network of independent entities
working together to solve problems too complex for any one part to
tackle alone. From a bird’s-eye view, the systems we envision are
intricate and complex, made up of diverse, interconnected elements:
field agents like firefighters, their handheld devices, airborne and
ground vehicles loaded with communication and computing tools, swarms
of sensors and IoT devices, and so on. These decentralized components
operate alongside more centralized hubs: data fusion centers, incident
commanders, public safety answering points (PSAPs), and emergency
operations centers (EOCs). We imagine these components being woven
together by a patchwork of communication technologies ranging from
analog and digital radios to Bluetooth, Wi-Fi, LTE, 5G, satellite
links, and ad-hoc mesh networks like Meshtastic \citationneeded and
DECT-2020 NR \citationneeded. Together, the systems form a dynamic
mosaic of elements cooperating to save lives and protect proprty.

Given the unpredictable nature of the environment and the locality
principle outlined in Section \ref{sec:disaster-response},
communication between edge components---such as field operators---and
centralized hubs is often inconsistent, sometimes available only
intermittently. As a result, information flow is subject to
appreciable delays compared to the timescale of critical events like a
fire shifting direction or a dangerous condition being detected. In
other words, the computing landscape is unmistakably
\emph{distributed}. Singhal and Shivaratri \cite{10.5555/562065}
define a distributed computing system as:
\begin{quote}
  ``A collection of computers that do not share common
  memory or a common physical clock, that communicate by message
  passing over a communication network, and where each computer has
  its own memory and runs its own operating system.''
\end{quote}
This stands in contrast to a centralized computing environment, where
processes can seamlessly share data through common memory, and memory
access times are considered negligible.

For our use cases, message-passing latencies are not only significant
but unpredictable and difficult to control. As a result, we can safely
assume that some parts of the wider system will not have
instantaneous, complete knowledge of every new piece of
information. Only a few components, if any, will be able to maintain a
global systemwide awareness. While deploying additional network
resources in the field—such as COWs (Cells on Wheels) and COLTs (Cells
on Light Trucks)—can help, the inherently distributed nature of the
environment cannot be fully overcome or abstracted away. This reality
must be embedded in the design of the software and networking
architecture itself. Typically, this manifests in a shared
`middleware' layer to coordinate the numerous moving parts, ensuring
they function as a unified system.

The fragmented flow of information presents several challenges for
system designers. Foundationally, one of the primary computer science
problems is that unpredictable latencies make it difficult for
components to maintain a common understanding about the global
sequence of events. For similar reasons, it becomes challenging for
processes to synchronize and agree on shared values, such as the
current number of firetrucks available for dispatch. The remainder of
this section will delve into these issues with more technical depth,
while later sections will examine how some of these challenges can be
reflected in the middleware design.

\subsection{Physical Synchrony}
\label{ssec:physical-synchrony}
A lot of challenges in distributed computing could be
straightforwardly overcome if we assume that all participants have
instantaneous access to a common time base, i.e.  synchronized
clocks. Let us explore why fine-grained synchronization is not a
tenable assumption, at least not for all purposes.

Physical clocks, especially consumer-grade ones, suffer from
\emph{drift}, which is to say they do not all run at the same
rate. Experienced IT administrators will testify that clocks can also
be prone to misconfiguration. An incorrect date, time, timezone, or
daylight saving time policy setting is a common source of IT issues,
typically causing time-based security mechanisms like TLS
authentication \citationneeded to misbehave. Consider also that
devices may spend a long time sitting unpowered in storage without
maintaining an always-on clock. For these sorts of reasons, we would
not want to rest the integrity of a safety-related system on the
assumption that a numerous and diverse assortment of devices have
precisely synchronized internal clocks.

Clock drift can be corrected for using, for instance, signals from GPS
satellites, but as mentioned in Section \ref{sec:disaster-response},
civil disaster environments are frequently GPS-denied: factors like
mountainous terrain, heavy smoke, and subterranean operations can lead
to errors or block signals entirely. Protocols like the Network Time
Protocol (NTP) \cite{rfc1119} work to bring clocks into
synchronization with respect to an authoritative source. Over a
moderately challenged connection, NTP typically achieves synchrony to
within values on the order of 100ms \citationneeded, but it is not
clear the level of synchrony that can be expected from NTP in the
sorts of use cases we have in mind. A field device initialized without
internet access may have no idea what the time is, but it must still
operate.

For our use case, what seems most important about time is that
\emph{the future cannot influence the past}
\cite{1989mattern}. Fortunately, this sort of invariant can be
enforced with mechanisms that do not rely on measuring real
time. Below, we explain how so-called logical clocks can be used to
measure and enforce a key relation between events, this being their
\emph{causal precedence}.


\subsection{Message Passing and Causality}
\label{ssec:message-passing}
We model a distributed system abstractly as a fixed set
$\mathcal{P} = \{P_1, P_2, \ldots P_n\}$ of \emph{processes} which
undergo atomic (indivisible) state changes known as
\emph{events}. Events are divided into three types: internal events,
representing state changes inside a single process, and send and
receive events corresponding to messages passed between processes. It
may be helpful to imagine processes as radios and messages as audio
broadcasts, though the framework we are using is abstract and applies
to any kind of communication technology. To draw out the core issues
surrounding messaging, the diagrams in this section do not depict any
internal events.

Throughout the section, processes and networks are opaque blackboxes,
which concentrates our attention on the ramifications of unpredictable
network latencies. We implicitly assume the reliable asynchronous
network model: when a message is sent between processes, it certainly
arrives at some point in the future, but we cannot say anything about
when or in what order compared to other messages. At times, we
consider the possibility that a message may never arrive. The choice
depends on which networking technology (or which layer of the OSI
networking model \citationneeded) is under consideration.

Figure \ref{fig:message-latencies} illustrates a series of time
diagrams for messages exchanged between three processes: $P_1$, $P_2$,
and $P_3$. The $x$-axis represents the flow of real time from left to
right, which each process represented by a worldline depicting the
events occurring within that process. Each message, $m$, originates
from a send event $\msend{}$, marking the moment the message is
dispatched across the network by its source process. The delivery of
the message corresponds to a receive event, $\mrecv{}$. For now we
assume messages have a single receiver. We write subscripts on
messages to distinguish them for clarity, but these are not inherent
to the messages themselves.

\begin{figure}[p]
  \setlength\belowcaptionskip{5ex}
  \begin{subfigure}{1\textwidth}
    \centering
    \input{images/pgf/mpEx1.pgf}
    \caption{$P_1$ has a somewhat lower-latency connection to $P_2$ than to $P_3$}
    \label{fig:message-latencies-a}
  \end{subfigure}
  \begin{subfigure}{1\textwidth}
    \centering \input{images/pgf/mpEx2.pgf}
    \caption{$P_1$ has a much lower-latency connection to $P_2$ than to $P_3$}
    \label{fig:message-latencies-b}
  \end{subfigure}
  \caption{Message-passing time diagram examples}
  \label{fig:message-latencies}
\end{figure}

In these diagrams, arrows connect corresponding send and receive
events, with their diagonal slant representing the latencies
experienced as messages traverse the network. Because messages arrive
with varying delays, they might arrive in a different order than they
were sent in. In Figure \ref{fig:message-latencies-a}, for example,
$P_1$ sends messages $m_2$ and $m_4$ sequentially, but $m_4$ arrives
before $m_2$, which might occur if $P_1$ and $P_3$ are separated by a
high-latency communication link. In Figure
\ref{fig:message-latencies-b}, $m_1$ is the first message sent but the
last to be delivered, potentially indicating a deteriorating link
between $P_1$ and $P_3$, perhaps due to increased distance or
inclement weather.

For many applications, it is critical to maintain a natural ordering
of events known as \emph{causal precedence}, or Lamport's ``happens
before'' relation \cite{1978:lamportclocks}. To formalize this, we
first consider the intuitive way to order events within a
\emph{single} process:
\begin{definition}
  For two events $e$ and $e'$ occurring in process $P_i$, we
  write $e <_{P_i} e'$ if $e$ occurs before $e'$ in $P_i$'s
  worldline.
\end{definition}
The previous definition is local to one process and unambiguous, as we
assume events within a process occur at discrete, non-overlapping
points in time. To extend this to a system-wide definition of causal
precedence, we relate corresponding send and receive events, then take
the transitive closure of the relation.

\begin{definition}[Causal precedence]
  \label{def:causalprecedence}
  We define a binary relation $\to$ on the set of events as follows:
  \[e \to e' \iff
  \begin{cases}
    e <_{P_i} e' \textrm{ for some process $P_i$}
    \textbf{ or} \\
    e = \msend{} \textrm{ and } e' =\mrecv{}
    \textbf{ or} \\
    \textrm{there is some } e'' \textrm{ such that } e \to e'' \textrm{ and } e'' \to e'
  \end{cases}
  \]
  If $e \to e'$, we say $e$ has \emph{causal precedence over} $e'$ or
  \emph{happens before} $e'$.
\end{definition}
Visually, $e \to e'$ holds when one can put a finger on $e$ in the
diagram and trace a ``path of causality'' to $e'$ by following
worldlines or arrows. We use the notation $e \not \to e'$ to mean
$e \to e'$ does not hold. Note that $e \not \to e'$ does not imply
$e' \to e$.

Incidentally, ``causal precedence'' and ``happens before'' can be
misnomers, as $e \to e'$ only conveys the possibility that information
from $e$ could have influenced $e'$. If information from event $e$
might have influenced $e'$, then it is crucial that applications avoid
situations where, from the user's point of view, it appears that $e'$
happened before $e$. For example, this proscription means an
application cannot let an ``answer'' appear before the underlying
``question''. This is what is meant by not letting the future affect
the past.

\begin{example}
  Figure \ref{fig:causal-precedence} illustrates the causal precedence
  relation corresponding to the time diagrams in Figure
  \ref{fig:message-latencies}. For readability we suppress redundant
  transitive arrows. The visual difference between Figures
  \ref{fig:message-latencies-b} and \ref{fig:message-co-b} reflects
  the fact that causal order only captures a logical relationship
  between events, but does not reflect their absolute time or within
  which process they occurred.
\end{example}

Mathematically, causal precedence is an irreflexive partial order:
\emph{irreflexive} because $e \not \to e$ (an event does not precede
itself), and \emph{partial} because any two events $e$ and $e'$ may
satisfy neither $e \to e'$ nor $e' \to e$. Events $e$ and $e'$ that
are not related by causality are said to be \emph{logically
  synchronous}, denoted $\sync{e}{e'}$. Note that logical
synchronicity is not usually transitive, meaning it is possible to
have $\sync{e}{e'}$ and $\sync{e'}{e''}$ but not $\sync{e}{e''}$. For
example,
\begin{itemize}
\item In Figure \ref{fig:message-co-a}, $\sync{\mrecv{1}}{\mrecv{2}}$
  and $\sync{\mrecv{2}}{\msend{4}}$, but $\mrecv{1} \to \msend{4}$.
\item In Figure \ref{fig:message-co-b}, $\msend{1}$ is logically synchronous
with every event except $\mrecv{1}$, but those other events are
totally ordered by causality and not synchronous with each
other.
\end{itemize}
Relations like $\sync{}{}$ that are reflexive and symmetric but not
necessarily transitive are sometimes called \emph{compatibility
  relations}.

\begin{figure}
  \begingroup
  \setlength\belowcaptionskip{4ex}
  \begin{subfigure}{1\textwidth}
    \centering
    \input{images/pgf/mpEx1CO.pgf}
    \caption{Causal precedence among the events in Figure \ref{fig:message-latencies-a}}
    \label{fig:message-co-a}
  \end{subfigure}
  \endgroup
  \begin{subfigure}{1\textwidth}
    \centering
    \input{images/pgf/mpEx2CO.pgf}
    \caption{Causal precedence among the events in Figure \ref{fig:message-latencies-b}}
    \label{fig:message-co-b}
  \end{subfigure}
  \caption{Causal precedence relations for Figure \ref{fig:message-latencies} (transitive arrows not shown)}
  \label{fig:causal-precedence}
\end{figure}

\subsection{Virtual Clocks}
\label{ssec:timestamps}
Distributed applications systematically track causality by employing
\emph{logical} clocks, which measure the logical flow of time by
timestamping events with (possibly sets of) non-negative integers that
are advanced according to certain rules. The three major variants are
scalar, vector, and matrix clocks, which form a kind of
spectrum. Scalar clocks are simple but provide coarse-grained
information, while vector and matrix clocks track increasingly more
precise information at the cost of greater administrative overheads.

All processes timestamp their events using their local clocks. For
each event $e$, let $C(e)$ denote the timestamp attached to that
event. The fundamental property we want to satisfy is that if $e$
causally precedes $e'$, it should receive a lesser timestamp. This is
called the clock consistency condition, commonly just called the clock condition.

\begin{definition}
  A system of timestamps satisfies the \emph{clock consistency
  condition} if the following monotonicity property holds:
  \[ \textrm{For all events $e$ and $e'$, } e \to e' \implies C(e) < C(e') \label{eq:mp}\tag{CC} \]
\end{definition}

This notation states that if one event causally precedes another, then
the earlier one receives a lesser timestamp. Somewhat subtly, the
clock condition does \emph{not} imply that we can decide if events are
causally related by comparing timestamps. It may be helpful to express
\eqref{eq:mp} in terms of the following logically equivalent
condition.
\[ \textrm{For all events $e$ and $e'$, }C(e) \leq C(e') \implies e'
  \not\to e \label{eq:mp-conv}\tag{CC$'$} \]

If this condition is true, we can be sure that a particular sequence
of events $e_1, e_2, e_3\ldots$ does \emph{not} violate causal
precedence (i.e., does not list any event $e$ before another event
that could have influenced $e$) by checking that
$C(e_{i}) \leq C(e_{i+1})$ for all $i$. We emphasize that this does
not give us a definite way to tell whether two events are in fact
causally related.

For some applications it is important to determine conclusively
whether events are causally related. In this case, one is led to
consider the following stronger requirement from a system of logical
timestamps.
\begin{definition}
  An event-timestamping mechanism satisfies the \emph{strong} clock   condition if the following property holds.
  \[ \textrm{For all events $e$ and $e'$, } e \to e' \iff C(e) < C(e') \label{eq:sc}\tag{SC} \]
  Note that $\iff$ is notation for ``if and only if,'' i.e. logical equivalence.
\end{definition}
Below we see one logical clock protocol that satisfies the weaker
clock condition, and two that satisfy the stronger condition.

\subsubsection{Scalar clocks}
\label{sssec:scalar-clocks}
\begin{figure}
  \setlength\belowcaptionskip{5ex}

  \begin{subfigure}{1\textwidth}
    \centering \input{images/pgf/mpEx1Sc.pgf}
    \caption{Figure \ref{fig:message-latencies-a} redepicted with scalar clocks}
    \label{fig:message-latencies-scalar-a}
  \end{subfigure}

  \begin{subfigure}{1\textwidth}
    \centering \input{images/pgf/mpEx2Sc.pgf}
    \caption{Figure \ref{fig:message-latencies-b} redepicted with scalar clocks}
    \label{fig:message-latencies-scalar-b}
  \end{subfigure}

  \caption{Scalar clock examples}
  \label{fig:message-latencies-scalar}
\end{figure}

Lamport's scalar clocks \cite{1978:lamportclocks} require each
process $P_i$ to maintain a single non-negative scalar value $C_i$,
initialized to $0$. The clock follows two simple update rules:
\begin{enumerate}
\item[\textbf{R1}:] Before a message is sent or an internal event occurs, $P_i$
  increments its clock:
  \[C_i := C_i + 1.\]
  The new value serves as the event's timestamp and, for messages, is ``piggybacked''
  as part of its metadata.
\item[\textbf{R2}:] When $P_i$ receives a message with timestamp $C$, it
  updates $C_i$ as such:
  \[C_i := \max(C, C_i) + 1.\]
  The value is the receive event's timestamp.
\end{enumerate}

\begin{example}
  Figure \ref{fig:message-latencies-scalar} depicts the same events in
  Figure \ref{fig:message-latencies} with scalar timestamps (shown in
  parentheses) assigned to each event. Piggybacked timestamps are
  shown as labels on the message arrows.
\end{example}

Scalar clocks satisfy the clock condition \eqref{eq:mp}. This can be
observed by tracing the path of causality between related events and
seeing that the clock is incremented at each step. However, they do
not satisfy \eqref{eq:sc}.  While $e$ having a lesser timestamp than
$e'$ rules out $e' \to e$, it does not imply $e \to e'$. For instance,
in Figure \ref{fig:message-latencies-scalar-b}, $\msend{1}$ has a
globally minimal timestamp value of $1$, but it does not causally
precede all events with timestamps greater than $1$, or indeed any
event except $\mrecv{1}$.

\subsubsection{Vector clocks}
\label{sssec:vector-clocks}
The strong clock condition \eqref{eq:sc} cannot hold either using
scalar clocks or even synchronized physical clocks. This is because
both mechanisms assign timestamps whose values form a total order---meaning any
two distinct timestamps $C_1, C_2$ satisfy either $C_1 < C_2$ or $C_2 < C_1$.

A clock protocol that uses a total order cannot enforce the strong
clock condition. For logically synchronous events $\sync{e}{e'}$,
neither $e \to e'$ nor $e' \to e$ holds, so strong consistency and the
total order property would require $C(e) = C(e')$. This is an
impossible requirement because logical concurrency is not
transitive. For instance, recall that in Figure
\ref{fig:message-latencies-b}, $\msend{1}$ is logically synchronous
with every event except $\mrecv{1}$. This would require assigning them
all the same timestamp, which contradicts the fact they are not
synchronous with each other. The solution is to let timestamps from a
partial order, where distinct timestamps $C_1$ and $C_2$ do not have
to satisfy either $C_1 < C_2$ or $C_1 > C_2$.

\begin{figure}
  \setlength\belowcaptionskip{5ex}

  \begin{subfigure}{1\textwidth}
    \centering
    \input{images/pgf/mpEx1Vec.pgf}
    \caption{Figure \ref{fig:message-latencies-a} redepicted with vector clocks}
    \label{fig:message-latencies-vector-a}
  \end{subfigure}

  \vspace{4ex}

  \begin{subfigure}{1\textwidth}
    \centering \input{images/pgf/mpEx2Vec.pgf}
    \caption{Figure \ref{fig:message-latencies-b} redepicted with vector clocks}
    \label{fig:message-latencies-vector-b}
  \end{subfigure}

  \caption{Vector clock examples}
  \label{fig:message-latencies-vector}
\end{figure}
\afterpage{\clearpage}

Vector clocks store one scalar value for each process in the system,
which forms a partial order when vectors are compared
component-wise. If the system is comprised of $N$ processes, $P_i$
maintains a vector $\vt_i[1 \ldots N]$ of non-negative integers, with
all values initialized to $0$. The $i^\textrm{th}$ component, or
$\vt_i[i]$, is called $P_i$'s local time, while the remaining
components are used to estimate other processes' local times. Vector
clocks have two update rules:
\begin{enumerate}
\item[\textbf{R1}:] Before an internal event occurs or a new message is sent, $P_i$
  increments its local time according to the rule:
  \[\vt_i[i] := \vt_i[i] + 1.\]
  The (entire) updated $\vt_i$ is the event's timestamp and is piggybacked with outgoing messages.
\item[\textbf{R2}:] When $P_i$ receives a message with timestamp
  $\vt$, $\vt_i$ is updated according to
  \[\vt_i[x] := \max(\vt[x], \vt_i[x]) \quad \textrm{for all $x = 1\ldots N$}.\]
  After this, $P_i$ increments its local time:
  \[ \vt_i[i] := \vt_i[i] + 1.\]
  The final vector is the timestamp of the receive event.
\end{enumerate}
These rules are more intuitively understood by demonstration.

\begin{example}
  Figure \ref{fig:message-latencies-vector} depicts the same events as
  Figure \ref{fig:message-latencies-scalar} with vector timestamps.
\end{example}

For all $j \neq i$, $\vt_i[j]$ represents $P_i$'s \emph{estimate} of
$P_j$'s local time, or $\vt_j[j]$. This estimate is always a lower
bound, since $P_j$'s local time may advance without $P_i$'s knowledge,
but $P_i$ never updates $\vt_i[j]$ ahead of $P_j$'s actual local
time. $P_i$ learns about updates to $P_j$'s local time through
piggybacked timestamp vectors, which allow $P_i$ to learn about
$P_j$'s time without communicating directly with $P_j$.

Vector timestamps are compared component-wise. This forms a partial
order because one vector may be greater than another in some
components but less than it in others.

\begin{definition}[Vector comparison]
  Let $v, w$ be two vector clocks. We define the following relations:
  \begin{align*}
             v = w &\iff \forall i, v[i] = w[i] \\
  v \preccurlyeq w &\iff \forall i, v[i] \leq w[i] \\
         v \prec w &\iff v \preccurlyeq w \textrm{ and } \exists i, v[i] < w[i] \\
            \syncts{v}{w} &\iff \textrm{ neither } v \prec w \textrm{ nor } v \succ w
  \end{align*}
  That is, $v \prec w$ if all of $w$'s components are at least as
  great as $v$'s, and at least one of its components is strictly
  greater. When two non-equal vectors are compared, and neither is
  greater than the other, we write $\syncts{v}{w}$ and say the vectors
  are \emph{incomparable}.
\end{definition}

 is justified by the fact that vector
clocks satisfy \ref{eq:sc}, so these otherwise distinct notions will
coincide.

\begin{lemma}
  Vector clocks satisfy the strong clock consistency condition. That
  is, where $C(e)$ is the vector timestamp of an event, then
  \[ e \to e' \iff C(e) \prec C(e'). \]
  From this it follows that for non-equal events $e$ and $e'$ we have
  \[\sync{e}{e'} \iff {C(e) \texttt{\#}\,C(e')}. \]% This isn't typesetting right
\end{lemma}

For reasons of space we omit a proof of the preceding lemma, though
the reader may find it enlighting to formalize the details.

\subsubsection{Matrix clocks}
\label{sssec:matrix-clocks}
If a vector clock stores both a local time and a lower bound estimate
of every other process's local time, then a matrix clock stores a
local vector clock and a lower bound estimate of every other process's
vector clock. Each process $P_i$ stores an $N\times{}N$ matrix
$\vt_i$, initialized to all zeros, with the following
interpretation. The $i^{\textrm{th}}$ row from the top, $\vt_i[i, -]$,
stores $P_i$'s vector time. All other rows $\vt_i[j,-]$ store $P_i$'s
estimate of $P_j$'s vector time. Matrix timestamps are piggybacked
with messages, and the receiver uses the sender's vector clock to
update their own vector clock as usual, and takes the pointwise
maximum of all other rows.

\begin{enumerate}
\item[\textbf{R1}:] Before a new message is sent, $\vt_i[i]$ is updated according to the rule
  \[\vt_i[i,i] := \vt_i[i,i] + 1.\]
  The entire updated matrix $\vt_i$ is piggybacked with the message.
\item[\textbf{R2}:] When a message is received from $P_j$ with a piggybacked timestamp $\vt$,
  $\vt_i$ is updated according to two cases
  \begin{enumerate}
  \item Update the row $\vt_i[i, -]$ according to
    \[\vt_i[i, k] := \max(\vt_i[i,k], \vt[j, k]) \quad \textrm{for all $k = 1\ldots N$}.\]
  \item Update all other rows $\vt_i[j', -]$ according to
    \[\vt_i[j', k] := \max(\vt_i[j',k], \vt[j', k]) \quad \textrm{for all $k = 1\ldots N$}.\]
  \end{enumerate}
  After this, $P_i$ advances its own local time according to the rule
  \[ \vt_i[i,i] := \vt_i[i,i] + 1.\]
  This new matrix is the timestamp attached to the receive event.
\end{enumerate}

\begin{figure}[p]
  \begingroup
  \setlength\belowcaptionskip{4ex}
  \begin{subfigure}{1\textwidth}
    \input{images/pgf/mpEx1Mat.pgf}%
    \caption{Matrix clock timestamps for the events in Figure \ref{fig:message-latencies-a}}
    \label{fig:message-latencies-matrix-a}
  \end{subfigure}
  \begin{subfigure}{1\textwidth}
    \input{images/pgf/mpEx2Mat.pgf}%
    \caption{Matrix clock timestamps for the events in Figure \ref{fig:message-latencies-b}}
    \label{fig:message-latencies-matrix-b}
  \end{subfigure}
  \caption{Figure \ref{fig:message-latencies} depicted with matrix clocks}
  \label{fig:message-latencies-matrix}
  \endgroup
\end{figure}

\begin{example}
  Figure \ref{fig:message-latencies-matrix} depicts the same events as
  Figures \ref{fig:message-latencies-scalar} and
  \ref{fig:message-latencies-vector} with matrix timestamps. By
  comparison to Figure \ref{fig:message-latencies-vector}, observe
  that that rows of the form $\vt_i[i,-]$ act like ordinary vector
  clocks.
\end{example}

In Section \ref{sec:background}, we mentioned the epistemic nature of
reasoning about distributed systems: a process can only make decisions
based on what it \emph{knows}, which is usually a strict subset of all
(system-wide) truths. In many cases, it is important to take into
account a kind of second-order knowledge: what does a process know
about what other processes know? One major utility of vector and
matrix clocks is that they can be be used to track which facts known
to one process are also known to another processes. When this is of
interest, it is often because a process wants to compute which facts
are known to \emph{all} other processes. In Section
\ref{sec:continuous-consistency} we will see an example of how this
works in practice in the context of replicating a database over a
disruption-heavy network.

\begin{comment}
Suppose a group of processes collaborate to replicate updates to a
shared data structure. We imagine that all updates are submitted by
users to some process, say $P_O$ ($O$ for ``origin''), whereupon it is
timestamped with $P_O$'s local time (value $\vt_O[O,O]$) and with the
value $O$ to indicate which process originally accepted the update
from a user. As an invariant, we ensure that no process $P_j$ updates
its estimate $\vt_j[j, O]$ of $O$'s local time to a value $t$ until
$P_j$ has been informed about all updates submitted to $P_O$ with
timestamps less than or equal to $t$. With these assumptions, any
process $P_i$ can use the value $\vt_i[j, O]$ as a lower bound
estimate of which updates originating from $P_O$ have already been
seen by $P_j$. In particular, once we have the
condition
\[ \textrm{for all $j$}, \vt_i[j, O] \geq t
\]
for some logical time $t$, $P_i$ can be sure that all other processes
have seen all updates with timestamps less than or equal to $t$
originating at $P_O$. Using this principle, matrix clocks have been
used to discard obsolete information during database replication
\cite{1987:sarinlynch}. In Section \ref{sec:continuous-consistency} we
will see an example of how this sort of scheme can be applied in
practice.
\end{comment}

\subsection{Message Ordering}
Coulouris et al. \cite{coulouris2005distributed} aptly summarized why
it is a problem for unpredictable network latencies to cause messages
to arrive in a different order than they were sent in.
\begin{quote}
  ``This lack of an ordering guarantee is not satisfactory for many
  applications. For example, in a nuclear power plant it may be
  important that events signifying threats to safety conditions and
  events signifying actions by control units are observed in the same
  order by all processes in the system.''
\end{quote}
In this section, we explore different paradigms for message ordering
in distributed systems. As with clocks and timestamps, the choice of
which ordering guarantee to use depends on the needs of the
application. We later generalize the discussion by admitting messages
sent to multiple recipients at once, such as in a group chat
application, where ensuring predictable message ordering is critical.

When ordering is important, applications do not show messages to the
user immediately when they come in from the network---the network can
deliver messages in unexpected and undesirable orders, after all. The
\emph{arrival} time of a message is when it is received from the
network, but instead of acting on it right away, an application may
buffer an arrived message while waiting for other messages (such as
ones with an earlier causal precedence) to ``catch up.'' When a
message is ready to be presented to the user, it is
\emph{delivered}. By waiting to deliver some messages, we can ensure
the stream of messages in order of their delivery satisfies particular
guarantees.

\subsubsection{FIFO ordering}
A modest requirement is the \emph{first-in, first-out} (FIFO)
condition, which stipulates that on any logical communication link
between two processes in the system, messages arrive in the order they
were sent. The restrictive phrase here is ``any logical communication
link''---by definition there is one link for any \emph{pair} of
processes. Hence, FIFO does not impose any conditions on messages
unless they are from the same sender and to the same recipient.

\begin{definition}[FIFO delivery]
  \label{def:fifo}
  The FIFO ordering guarantee is defined by the following condition. Let
  $P_i$ and $P_j$ be any two processes and $m_1$ and $m_2$ be two
  messages sent from $P_i$ to $P_j$. Then
  $\msend{1} \to \msend{2} \implies \mrecv{1} \to \mrecv{2}$.
\end{definition}

The Internet Protocol (IPv4 or IPv6) by itself does not provide FIFO
semantics. In the OSI model, FIFO ordering is often provided by the
transport layer, in practice usually in the form of the transmission
control protocol or TCP. (The other classic internet transport, the
user datagram protocol or UDP, does not provide any ordering or
reliability guarantees). Applications built on top of TCP or a similar
transport can take therefore take FIFO for granted. Providing FIFO can
be as simple as marking messages sent from $P$ to $P'$ with
consecutive numbers $(1,2,3\ldots)$. If message $1$ arrives and then
$3$ arrives, $P'$ infers that $2$ is lagging behind, delivering $1$ to
the user immediately but withholding delivery of $3$ until after $2$
is received and delivered.

The guarantees provided by FIFO are minimal because they only apply on
a per-link basis: every link requires its own numbering scheme, so
message numbers cannot be meaningfully compared across different
links. To compare messages globally requires something like causal
order, below.

\begin{figure}[p]
  \setlength\abovecaptionskip{0ex}
  \setlength\belowcaptionskip{4ex}
  \begin{subfigure}[t]{0.475\textwidth}
    \centering
    \input{images/pgf/ordEx1.pgf}
    \caption{A non-FIFO execution}
    \label{fig:ordex-non-fifo}
  \end{subfigure}
  \begin{subfigure}[t]{0.475\textwidth}
  \centering
  \input{images/pgf/ordEx2.pgf}
  \caption{A CO (therefore FIFO) execution}
  \label{fig:ordex-co-1}
\end{subfigure}
\begin{subfigure}[b]{0.475\textwidth}
  \centering
  \input{images/pgf/ordEx3.pgf}
  \caption{A CO execution}
  \label{fig:ordex-co-2}
\end{subfigure}
\begin{subfigure}[b]{0.475\textwidth}
  \centering
  \input{images/pgf/ordEx6.pgf}
  \caption{A CO execution}
  \label{fig:ordex-co-3}
\end{subfigure}
\begin{subfigure}[b]{0.475\textwidth}
  \centering
  \input{images/pgf/ordEx5.pgf}
  \caption{A FIFO and non-CO execution}
  \label{fig:ordex-non-co-1}
\end{subfigure}\hfill
\begin{subfigure}[b]{0.475\textwidth}
  \centering
  \input{images/pgf/ordEx4.pgf}
  \caption{A FIFO and non-CO execution}
  \label{fig:ordex-non-co-2}
\end{subfigure}
\caption{Message ordering examples}
\label{fig:message-ordering}
\end{figure}

\subsubsection{Causal ordering}
Causal order is an order guarantee consistent with causal precedence
of events. A network provides causally ordered (CO) delivery if it
satisfies the following property.
\begin{definition}[CO delivery]
  \label{def:causalorder}
  For any process $P_\mathrm{dest}$ in the system, if we consider all
  messages $m$ and $n$ sent to $P_\mathrm{dest}$ (possibly by
  different senders), if $\msend{} \to n^\textrm{send}$ then
  $\mrecv{} \to n^\textrm{recv}$. That is, each destination receives
  messages in an order consistent with causality between their send
  events.
\end{definition}
Unlike FIFO, the CO condition enforces a partial order among messages
with (in general) different senders. In mathematical terms, if we for
each process $P_{\mathrm{dest}}$, the function mapping send events to
corresponding receive events at $P_{\mathrm{dest}}$ must be monotonic
with respect to causal precedence.

\begin{example}
  Figure \ref{fig:message-ordering} demonstrates different message
  ordering conditions. We make a few observations for emphasis.

  \begin{itemize}
    \tightlist
  \item \ref{fig:ordex-non-fifo} violates FIFO because messages $m_1$
    and $m_2$ are both sent from $P_1$ to $P_2$, but the arrive in the wrong order.
  \item \ref{fig:ordex-co-1} satisfies CO and therefore FIFO. Messages
    $m_1$ and $m_2$ arrive in the opposite order but they are sent to
    different destinations.
  \item \ref{fig:ordex-non-co-1} violates CO because the send event of
    $m_1$ happens before that of $m_3$ via the chain
    $\msend{1} \to \msend{2} \to \mrecv{2} \to \msend{3}$ but
    $\mrecv{3} \to \mrecv{1}$.
  \item \ref{fig:ordex-non-co-2} violates CO because it is equivalent to
    \ref{fig:ordex-non-co-1} with the roles of $P_2$ and $P_3$ swapped.
  \end{itemize}
\end{example}

\subsubsection{Multicasting and Broadcasting}
We now extend the above definitions to the group communication setting
by allowing messages to have multiple recipients. For simplicity, we
suppose messages are broadcast to all other recipients, though the
definitions can easily generalize to ``multicast'' scenarios where
messages are sent to a subset of recipients.

One way to implement broadcasting is to sending distinct network
messages which, for present purposes, we would treat as a single
unit. Alternatively, we can lean on the network itself for assistance,
sending a single message specially marked as a broadcast, relying on
lower-level protocols in the network to distribute a copy to each
recipient. Regardless of implementation, the challenge and importance
of ensuring consistent message ordering across an entire group is a
paramount concern.

The FIFO and CO broadcast conditions are adapted from Definitions
\ref{def:fifo} and \ref{def:causalorder}. Additionally, we introduce
the notion of total ordering (TO) below.

\begin{definition}[FIFO broadcast]
  \label{def:fifo-bcast}
  A broadcast primitive satisfies the FIFO semantics if it satisfies
  the following condition. For any process $P_i$, if $P_i$ broadcasts
  $m_1$ before $m_2$, then all recipients receive $m_1$ before $m_2$.
\end{definition}

\begin{definition}[CO broadcast]
  \label{def:causalorder-bcast}
  A broadcast primitive satisfies CO semantics if for any broadcasts
  $m$ and $n$, if $\msend{} \to n^{\textrm{send}}$, then all
  destinations deliver $\mrecv{}$ before $n^{\textrm{recv}}$.
\end{definition}
In the above definition, the happens before relation is defined just
as in the unicast (non-broadcast) setting by following a path of
causality along worldlines and message arrows.

\begin{figure}[h]
  \centering \input{images/pgf/mpEx3.pgf}
  \caption{Broadcast example that satisfies FIFO but violates CO}
  \label{fig:broadcast-fifo-1}
\end{figure}

\begin{example}
  In Figure \ref{fig:broadcast-fifo-1}, causal order is violated. Imagine the following conversation:
  \begin{itemize}
    \tightlist
  \item [$P_1$]: ``I need an ambulance at location A.''
  \item [$P_2$]: ``Understood, the last ambulance has been dispatched.''
  \end{itemize}
  However, $P_3$ receives $P_2$'s response before $P_1$'s request, resulting in this conflicting view:
  \begin{itemize}
    \tightlist
  \item [$P_2$]: ``Understood, the last ambulance has been dispatched.''
  \item [$P_1$]: ``I need an ambulance at location A.''
  \end{itemize}
  From $P_3$'s perspective, $P_1$ appears to be requesting resources
  that are no longer available. The sort of conflict can lead to
  confusion, with requests being duplicated or going
  unanswered. Tracking causal order is crucial to avoid such resource
  misallocations.
\end{example}

A total order broadcast ensures that all recipients receive the
messages in the same order. This order is not required to satisfy any
particular constraints except that all recipients agree on it. Such a
model is appropriate when it is more important that everyone agrees on
a common order of events but the order itself is not necessarily
critical.

\begin{definition}[TO broadcast]
  \label{def:totalorderbroadcast} For any processes $P$ and $P'$ and
  messages $m$ and $m'$ that arrive at \emph{both} destinations, $m$
  arrives before $m'$ at both processes or $m'$ arrives before $m$ at
  both processes.
\end{definition}

\begin{figure}[p]
  \setlength\belowcaptionskip{5ex}
  \begin{subfigure}{1\textwidth}
    \centering
    \input{images/pgf/bcastEx1.pgf}
    \caption{Broadcast example that satisfies FIFO but violates CO and TO}
    \label{fig:bcast-order-examples-1}
  \end{subfigure}
  \begin{subfigure}{1\textwidth}
    \centering
    \input{images/pgf/bcastEx2.pgf}
    \caption{Broadcast example that satisfies CO but violates TO}
    \label{fig:bcast-order-examples-2}
  \end{subfigure}
  \begin{subfigure}{1\textwidth}
    \centering
    \input{images/pgf/bcastEx3.pgf}
    \caption{Broadcast example that satisfies TO but violates FIFO}
    \label{fig:bcast-order-examples-3}
  \end{subfigure}
  \caption{Multicast ordering examples}
  \label{fig:bcast-ordering-examples}
\end{figure}

\begin{example}
Figure \ref{fig:bcast-ordering-examples} depicts examples of broadcast
message orders.
\begin{itemize}
  \tightlist
\item Figure \ref{fig:bcast-order-examples-1} trivially satisfies FIFO
  because no process sends more than one multicast, but causality is
  violated because $\msend{1} \to \mrecv{1,4} \to \msend{2}$, but
  $P_2$ receives $m_2$ before $m_1$. Total order is also violated
  because $P_2$ and $P_3$ receive the messages in opposite orders.
\item Figure \ref{fig:bcast-order-examples-2} violates TO for the same
  reason above, but satisfies causality because the two send events
  have no causal relation.
\item Figure \ref{fig:bcast-order-examples-3}
  satisfies TO, but $m_2$ arrives at both processes before $m_1$ so
  FIFO is violated.
\end{itemize}
\end{example}

\paragraph{Self-delivered messages}
In some contexts one considers broadcast primitives that include the
original sender among the recipients of a message. For simplicity, the
examples in this section have not shown this sort of self-delivery,
but it is useful in many cases. A typical use case is that
participants are using a total order broadcast to maintain local
replicas of a state machine that can be advanced by any participating
process by announcing updates. An example of this usage is presented
in Section \ref{sec:continuous-consistency}, where the goal is to
maintain distributed replicas of a database over a disruption-heavy
network, which can improve the performance of system applications.

%When a site wishes to update the shared data, it
%announces its intention using a totally ordered (say) broadcast that
%includes the sender in the list of recipients. To ensure consistency,
%we only actually modify our replica of the database when we hear about
%updates, including our own, over the totally ordered broadcast
%channel. In this manner, we can ensure all sites participating in the
%database replication reflect all updates in the same order, thus
%maintaining consistency.


\subsection{Shared Memory}
\label{ssec:shared-memory}
Desiging a distributed application using direct message passing can be
challenging due to the complexity of managing the low-level details
surrounding message ordering and reliability. A more abstract
approach, the \emph{distributed shared memory} (DSM) framework,
simplifies this task by allowing programmers to think in terms of
reading and writing to memory locations instead of sending messages
over a network.

The defining feature of DSM is that it allows all processes to
interact as if they had access to a single, unified proof of shared
memory---just like processes running on a single computer---despite
being spread across different, physically separated computers.  This
seamless experience (at least \emph{mostly} seamless---more on that in
a bit) is facilitated by a middleware layer inside the process called
the \emph{memory manager}, which handles all read and write requests
submitted by an application. In the background, not directly visible
to the application, the memory manager coordinates with other
instances over the network to maintain the illusion of a shared
state---what first responders would call a ``common operating
picture.''

We mention that DSM is only ``mostly'' seamless because, like all nice
things in distributed systems, there is a catch: usually, the memory
manager does not necessarily return the most up-to-date values of
memory locations. Indeed, we shall see that different processes can
write conflicting values to one memory location at the same time, it
is not clear a priori what it means for a value to be ``up-to-date''
in the first place. For developers, understanding the semantics of the
virtual memory layer---what consistency guarantees are provided by the
memory manager---is crucial to build applications that function
correctly while providing reliable performance. As introduced in
Section \ref{sec:disaster-response}, the design space is generally
marked by a tradeoff between stronger consistency guarantees and
faster performance.

\begin{figure}
    \centering
    \input{images/pgf/smEx1NoEdges.pgf}
    \caption{Time diagram for memory operations}
    \label{fig:dsm-example-1}
\end{figure}

\begin{figure}
  \centering
  \input{images/pgf/smEx1DAG.pgf}
  \caption{External order relation among operations in Figure \ref{fig:dsm-example-1}}
  \label{fig:dsm-example-1DAG}
\end{figure}

Figure \ref{fig:dsm-example-1} depicts an exemplary time diagram for the
shared memory abstraction, similar to those for message passing. Two
kinds of operations are shown: \emph{reads} and \emph{writes}. A read
operation, $\mathsf{R}(x)$, retrieves the value stored at (virtual)
location $x$, which returns some value $v$. When we want to indicate
the value returned by the read, we write $\memreadVal{x}{v}$. A write
operation, $\memwrite{x}{v}$, indicates writing value $v$ to memory
location $x$, which in code might be written as something like
$x := v$.

An operation does not happen instantly, but has a \emph{duration}. An
arbitrary read or write operation, $\Op$, spans from the moment of
time the operation is invoked by the application ($\memstart{\Op}$),
to when it finished ($\memstop{\Op}$), returning either the read value
or an acknowledgment of the write request. During this span, the
memory management layer is usually coordinating in the background with
other processes over the network, say by looking up the current value
of a memory location, but this is not shown in the diagrams. The
entire sequence of requests across all processes forms what we call a
\emph{history}. If $H$ is a history, we write $H|_P$ to mean just the
sequence of operations happen on process $P$, called the \emph{local
  history} of $P$.

Because they have duration, memory operations on different
processes---including ones that access the same virtual memory
locations---can occur simultaneously. A fundamental relation among
operations is their \emph{external order}, the partial order that
orders non-overlapping events by their physical times, but does not
assign an ordering to events whose executions overlap in physical
time.
\begin{definition}[External order]
  Let $H$ be a history. An operation $\Op^1$ \emph{externally
    precedes} operation $\Op^2$ if
  $\memstop{\Op^1} < \memstart{\Op^2}$. This induces an irreflexive
  partial order on $H$ called external order.
\end{definition}
The definition states that one operation externally precedes another
if it stops before the other is invoked. Note that we are comparing
events in terms of real, physical time: external order is the partial
order that would be expected by an outside observer who can see
operations executing globally in real time.

Figure \ref{fig:dsm-example-1DAG} shows the external order among the
operations in Figure \ref{fig:dsm-example-1} in the form of a directed
acyclic graph (DAG).


\begin{definition}[Physical concurrency]
  Consider two operations $\Op^1$ and $\Op^2$. If neither externally
  precedes the other, in another words if there is some moment in time
  during which both operations are executing, the operations are said
  to be \emph{physically concurrent}, written $\concurrent{\Op^1}{\Op^2}$.
\end{definition}

\subsection{Semantics and Consistency}
In a sequential application running on a single computer, it is clear
how read and write requests should be interpreted. A read request
$\memread{x}$ should return the most recent value $v$ that was written
to $x$ by a write $\memwrite{x}{v}$ (or some default value if no such
write exists, but we will not consider such examples). This is
unambiguous because we assume that in a single process, operations do
not overlap in time, so there is always a sense of which one happened
first. In a transactional database, this property can be achieved
using standard \emph{serialization} mechanisms.

\begin{example}
  Consider the following history of operations running inside a single process.
  Note that this diagram does not indicate what values are returned by the read operations.
  \[\input{images/pgf/dsm_example_1.pgf}\]
  Since there is no ambiguity in the order of events, it is clear
  which values \emph{should} be returned. Read requests, underline
  below with their results shown, should return the value of most
  recent write operation to each location.
  \[ \memwrite{x}{0} \to \memwrite{y}{5} \to \underline{\memreadVal{x}{0}} \to \memwrite{x}{3} \to \underline{\memreadVal{y}{5}} \to \underline{\memreadVal{x}{3}}. \]
\end{example}

In a distributed system, operations on different processes can run
concurrently, so there is no obvious way to arrange events into a
total order that all processes can agree on. Consequently, the notion
of ``most recent'' operation is ambiguous, so it does not even make
sense to say that read requests always return the most recent written
value.

\begin{figure}[h]
  \input{images/pgf/dsm_example_2.pgf}
  \caption{A history featuring concurrent writes to the same location}
  \label{fig:dsm-example-2}
\end{figure}

%\begin{example}
%  \label{exmpl:concurrentupdates}
%\end{example}

Consider Figure \ref{fig:dsm-example-2}. First, there are three operations that write to the value $x$.  Two of
these writes are executed at overlapping moments in physical time,
making it unclear whether $\memwrite{x}{3}$ or $\memwrite{x}{5}$
should be considered as happening first. For a period of time, $P_1$
and $P_2$ are both writing to location $x$ at the same time. Second,
these operations are immediately followed by several read requests,
whose return values are not shown in the diagram. A memory model
answers the question, ``Which values might be returned by each of
these read requests.'' Such a model would have to answer several
questions, like the following ones:
\begin{enumerate}
\item Do the read operations on $P_1$ and $P_2$ have to return the same value?
\item Can the second read operation at $P_2$ return a different value
  than the first one?
\item Is it ever possible for any of the $\memread{x}$ operations to
  return the value 4?
\end{enumerate}

It is possible to consider different ways of answering these
questions. The strictest widely used memory model, called
\emph{linearizability} and formally defined in the next section, would
require that all read operations return the same value, which must be
either $3$ or $5$. We shall see that this model is in fact too strict
for our use case, so programmers must face the possibility for less
rigidly prescribed behavior from the memory manager. An application
designed for one memory model may misbehave if executed in an
environment that implements a different one, while on the other hand,
a stricter memory model may result in poor application
performance. Thus, choosing a memory model requires balancing the
needs and expectations of the application against its performance
characteristics, including its usage patterns and networking
environment.

To resolve the ambiguity caused by overlapping memory operations, one
might attempt to assign timestamps to them, and use this to define
some global total order based on when the operations occur. However,
this would require sufficiently fine-grained timestamps from
synchronized clocks, which is usually an infeasible assumption (see
Section \ref{ssec:physical-synchrony}). %In that section, we were lead
% to define a global partial order, causal precedence, partly in
% reaction to the fact that we cannot use physical timestamps to
% arrange all systemwide events into a total order.

\subsection{Strong Consistency Models}
\label{ssec:strong-consistency}
This section considers the two major memory models usually said
to provide ``strong'' consistency: linearizability and sequential
consistency. Both models require that the global history aligns with
(read: returns values consistent with) some total sequential arrangement of the operations, with processes maintaining a shared understanding of which operations occur
in which order. Where the models differ is in how they constrain which
sequential arrangements are allowed.


\subsubsection{Linearizability}
\label{sssec:linearizability}

\emph{Linearizability}, a sort of gold standard for memory
consistency, can be concisely defined as a system that acts like
``each operation applied by concurrent processes takes effect
instantaneously at some point between its invocation and response.''
\cite{10.1145/78969.78972} The same condition is known by other names
like atomic consistency, strict consistency, and external
consistency. It means almost the same thing as strict serializability,
except the latter terminology is used to discuss transactional
databases and implies other database-related guarantees.

More formally, a linearizable history is defined by three features.
\begin{definition}[Linearizable history]
  \label{def:linearizable}
  A \emph{linearizable history} is one satisfying the following three rules.
\begin{enumerate}
  \tightlist
\item \textbf{Global Agreement on Order}: All processes behave as if
  they are observing a single, global, consistent sequence of
  operations.
\item \textbf{Correct Responses}: Responses are correct, meaning a read request
  \(\memreadVal{x}{a}\) returns the value of the most recent write request
  \(\memwrite{x}{a}\) to \(x\) in the aforementioned global order.
\item \textbf{Consistent with External Order}: The global sequence of
  events is consistent with external order: if $\memstop{\Op^1} < \memstart{\Op^2}$,
  the global order must include $\Op^1$ before $\Op^2$.
\end{enumerate}
\end{definition}

The previous definition is concerned with individual executions of an
application. When only linearizable executions are permitted by the
memory manager, the entire system is said to be linearizable.

\begin{definition}[Linearizable system]
  A DSM application is linearizable if all possible executions of the
  application are linearizable according to Definition \ref{def:linearizable}.
\end{definition}

Consider Figure \ref{fig:dsm-example-2} again. If the distributed
system is using a linearizable memory manager, then the requirement of
global agreement requires $P_1$ and $P_2$ to agree on the logical
order of write events. The requirement of consistency with external
order means that $\memwrite{x}{4}$ must be ordered before
$\memwrite{x}{3}$ and $\memwrite{x}{5}$, but the latter operations can
be logically sequenced in any order, as long as $P_1$ and $P_2$
agree. They must return responses that are consistent with this common
operating picture, which constrains the system to one of two
possibilities: all the read operations return $3$, or they all return
$5$. These possibilities are illustrated in Figure
\ref{fig:dsm-example-2-linearizations}.

A visually intuitive way of approaching linearizability is by defining
it in terms of \emph{linearization points.}
\begin{definition}
  A \emph{linearization point} $t \in [\memstart{\Op}, \memstop{\Op}]$
  for an operation $\Op$ is a time between the event's invocation and
  response at which time the operation appears to take effect in whole
  and instantaneously. A \emph{linearization} of a history is an
  assignment of linearization points consistent with the values
  returned by the operations.
\end{definition}
The subfigures in \ref{fig:dsm-example-2-linearizations} each depict a
consistent choice of linearization points in yellow.

\begin{figure}
  \begin{subfigure}{1\textwidth}
    \setlength\belowcaptionskip{4ex}
    \centering
    \input{images/pgf/dsm_example_2_linear_1.pgf}
    \caption{A linearization where the read operations return 3}
    \label{fig:dsm-example-2-linearizations-1}
  \end{subfigure}
  \begin{subfigure}{1\textwidth}
    \input{images/pgf/dsm_example_2_linear_2.pgf}
        \caption{A linearization where the read operations return 5}
    \label{fig:dsm-example-2-linearizations-b}
  \end{subfigure}
  \caption{Two possible linearizations of Figure \ref{fig:dsm-example-2} with linearization points shown in yellow}
  \label{fig:dsm-example-2-linearizations}
\end{figure}

\subsubsection{Sequential consistency}
\label{sequential-consistency}

Linearizability offers very strong guarantees related to real-time
constraints. There are many scenarios where this level of rigor is
unnecessary. A more relaxed model, sequential consistency, provides
comparably strong guarantees but does not impose the same constraints
with respect to external order. First, we define a \emph{sequential
  history} as any way of arranging read and write requests into a
linear sequence.

\begin{definition}[serial]
  \label{def:sequential-history}
  Let $H$ be a history of memory operations. A \emph{sequential
    history} of $H$ is any choice of total order among the events in
  $H$.
\end{definition}

Next we define \emph{program order}. This is a partial order among
operations that imposes constraints on operations in the same
process. Recall that because operations in a single process do not
overlap, this definition is unambiguous.

\begin{definition}[Program order]
  An operation $\Op^1$ precedes another operation $\Op^2$ in program
  order if the events occur in the same process and
  $\memstop{\Op^1} < \memstart{\Op^2}$.
\end{definition}

The difference between program order and external order is that
program order does not impose any constraints on operations that do
not occur in the same process.

A sequentially consistent system guarantees that any history is
consistent with some serial that respects \emph{program
  order}. Otherwise, the definition follows the same structure as that
for linearizability.

%\begin{definition}[Legal serial]
%Let $H$ be a history of invocation/response events occurring on a set
%of processes $\{P_i\}_{i = 1 \ldots N}$. A \emph{serial}
%of $H$ is any choice of total order among the events in $H$---that is,
%a rearrangement of the operations as to ensure none of them overlap in
%time. If each $P_i$ issues
%$r_i$-many requests for some non-negative integer $r_i$, observe there
%are a total of
%\[
%\frac{\left(\sum_{i = 1}^N r_i\right)!}{\prod_{i = 1}^N r_i!}
%\]
%possible sequential histories.
%\end{definition}

\begin{definition}[Sequentially consistent execution]
  \label{def:sequentially-consistent-execution}
  A \emph{sequentially consistent execution} is one satisfying the following three rules.
\begin{enumerate}
  \tightlist
\item \textbf{Global Agreement on Order}: All processes behave as if
  they are observing a single, global, consistent sequence of
  operations.
\item \textbf{Correct Responses}: Responses are correct, meaning a read request
  \(\memreadVal{x}{a}\) returns the value of the most recent write request
  \(\memwrite{x}{a}\) to \(x\) in the aforementioned global order.
\item \textbf{Consistent with Program Order}: The global sequence of
  events is consistent with program order: if
  $\memstop{\Op^1} < \memstart{\Op^2}$ and $\Op^1$ and $\Op^2$ run in
  the same process, the global order must include $\Op^1$ before
  $\Op^2$.
\end{enumerate}
\end{definition}

Since external order imposes more constraints than program order, a
linearizable system is always sequentially consistent. Hence, the
executions in Figure \ref{fig:dsm-example-2-linearizations} are
sequentially consistent.
\begin{lemma}
  \label{lem:linearsequential}
  A linearizable execution is sequentially consistent.
\end{lemma}

The converse of Lemma \ref{lem:linearsequential} does not hold. The
executions in Figure \ref{fig:dsm-example-2-linearizations} are
sequentially consistent but not linearizable---as noted earlier, the
only two linearizable executions require all three read operations to
return the same value for $x$.

\begin{figure}
  \begin{subfigure}{1\textwidth}
    \setlength\belowcaptionskip{4ex}
    \centering
    \input{images/pgf/smEx4S1.pgf}
    \caption{Sequentially consistent example where $P_1$ and $P_2$ read different values}
    \label{fig:dsm-example-2-sequential-1}
  \end{subfigure}
  \begin{subfigure}{1\textwidth}
    \input{images/pgf/smEx4S2.pgf}
    \caption{Sequentially consistent example where $P_2$ observes $x$ with a value of $4$}
    \label{fig:dsm-example-2-sequential-2}
  \end{subfigure}
  \caption{Sequentially consistent but nonlinearizable executions of Figure \ref{fig:dsm-example-2}}
  \label{fig:dsm-example-2-sequential}
\end{figure}

Visually, we can think of sequential consistency as organizing
operations like beads on a string. The worldline of each process is
like its own ``string,'' while the operations occurring in that
process are like ``beads''. Beads on the same string can move forward
or backward in time, but they cannot overtake each other; this
corresponds to respecting the program order of operations. However,
beads on different strings---representing operations from different
processes---are free to slide past each other. Sequential consistency
constrains the application to be consistent with some total order of
operations that can be produced by rearranging them in this fashion.

\begin{example}
  Figure \ref{fig:dsm-example-2-sequential} depicts two
  nonlinearizable, sequentially consistent executions of the same
  operations depicted in Figure \ref{fig:dsm-example-2}. In Figure
  \ref{fig:dsm-example-2-sequential-1}, by ``sliding'' all the
  operations in $P_2$ along their worldline so that they occur after
  the operations in $P_1$, we arrive at a sequentially consistent
  history where $P_1$ and $P_2$ read different values for $x$. By a
  similar sliding, we can also produce a sequentially consistent
  history in Figure \ref{fig:dsm-example-2-sequential-2} where the
  second read operation in $P_2$ returns $4$ after the first one reads
  $5$, despite the fact that in \emph{real time}, $\memwrite{x}{4}$
  occurs \emph{before} $\memwrite{x}{5}$.
\end{example}


It may seem strange to allow executions, like those in Figure
\ref{fig:dsm-example-2-sequential}, that seem to defy the real time
order of operations. However, sequential consistency is a very
intuitive property for programmers to reason about. Consider this: A
programmer designs an two-process distributed application that
executes the operations shown in Figure
\ref{fig:dsm-example-2}. Before the application is actually executed,
we do not know the exact moment in time that each operation will run,
since the processes might run at different speeds (they run on
distinct computers, after all). If the processes and operations run at
certain speeds, then the executions shown in
\ref{fig:dsm-example-2-sequential-1} and
\ref{fig:dsm-example-2-sequential-2} are possible. Thus,
sequentially consistent systems are always consistent with some way
the system could have executed in real time.


% Processes in a sequentially consistent system are required to agree
% on a total order of events, presenting the illusion of a shared
% database from an application programmer's point of view. However,
% this order need not be given by external order. Instead, the only
% requirement is that serial must agree with process
% order, i.e.~the events from each process must occur in the same
% order as in they do in the process.  This is nearly the definition
% of linearizability, except that external order has been replaced
% with merely program order. We immediately get the following lemma.


\begin{comment}
  It may seem strange to consider executions such as the one shown in
  REF in which operations appear to take effect at different times for
  different processes, or at times that do not agree with external
  order. The reader should remember that in the background, these
  processes would be engaged in message-passing over the network and
  are therefore subject to all the complexities previous discussed in
  Section \ref{sec:message-passing}, including delayed and
  out-of-order messages. Looser requirements by the memory model
  impose fewer constraints on the message passing layer requiring less
  coordination, and allowing for greater performance.
\end{comment}

\subsection{The CAP Theorem}
Real-world systems rarely function as a perfectly coherent,
integrated, cohesive system. The gap between idealized system behavior
and real-world behavior stems from a well-understood and fundamental
tradeoff between coherence and performance. The more ``coherence'' we
demand from the system, the more processes have to communicate over
the network, whose unpredictable delays impose overheads that degrade
performance. Conversely, the more we demand immediate answers from our
system, the less time a process has to communicate with other
processes, so the system as a whole does not seem as coherent and
unified to end users.

This tradeoff is made fully stark by considering the possibility that
the network suffers from a partition, which prevents some processes
from communicating with others.

\begin{definition}[Network partition] A \emph{network partition} is a
span of time where some nodes are unable to communicate with another
set of nodes on the network.
\end{definition}

In 1999, Fox and Brewer \cite{1999foxbrewer} articulated a formal
tradeoff between three desirable properties of distributed systems:
consistency, availability, and an ability to function during network
partitions. This observation was formalized and rigorously proven by
Gilbert and Lynch \cite{2002gilbertlynchCAP} in 2002. Despite its
prominence at the heart of distributed systems, and the fact that its
proof is fairly straightforward, the CAP theorem is sometimes
misunderstood, so it is worth clarifying its key terms.

\begin{description}
\item[Consistency] Gilbert and Lynch define consistency as
  linearizability.
\item[Availability] A CAP-available system responds to every client
  request (a read or write operation) in a finite time.
\item[Partition tolerance] A partition-tolerant system continues to
  function in the face of arbitrary partitions inthe network.
\end{description}

In the last case, the possibility is allowed that a partition never
recovers. This could happen if a critical communications cable is
permanently severed, for instance.

The CAP theorem is the simple observation that a distributed system
cannot guarantee all three properties simultaneously. A system that
operates during network partitions cannot ensure both linearizability
and availability. We give only the informal sketch here, leaving the
interested reader to consult the more formal analysis by Gilbert and
Lynch. The main assumption of the proof is that a process's behavior
cannot be affected by messages that are sent to it but never received.

\begin{theorem}[The CAP Theorem]
  \label{thm:cap}
  If indefinite network partitions are possible, then a distributed
  system cannot guarantee both linearizability and
  eventual availability.
\end{theorem}
\begin{proof}
  Consider again the execution in Figure \ref{fig:dsm-example-2}. We have seen
  that there are only two possible linearizations of this history, and
  both of them require all reads to return the same value, either $3$
  or $5$. Now suppose the network suffers from a partition so that
  $P_1$ and $P_2$ cannot communicate. There are two possibilities:
  \begin{enumerate}
  \item The processes could proceed despite the lack of
    communication. In this case, because processes do not otherwise
    affect each other, $P_1$ could not see the $\memwrite{x}{5}$
    operation and its read must return the value $3$. Likewise, $P_2$
    does not see $\memwrite{x}{3}$ and its reads return $5$.
  \item The processes might detect that the network is unavailable and
    refuse to respond to read requests. This ensures the processes do
    not return inconsistent answers. However, there is no guarantee
    that all requests will eventually be handled, since the partition
    might never recover.
  \end{enumerate}

  Thus, we cannot have both linearizable consistency and
  availability. More precisely, to ensure both of these properties, we
  would have to assume the network never suffers from partitions, but
  this is unrealistic. Section \ref{sec:disaster-response} cited a
  real-world example where a wildfire caused a communications
  partition.
\end{proof}


The proof above assumes that the standard of consistency is
linearizability. This raises the question of whether the weaker notion
of sequential consistency can be used to avoid the ramifications of
the CAP theorem. Unfortunately the answer is negative: sequential
consistency is also CAP-unavailabile.

\begin{figure}
  \input{images/pgf/dsmCAPex2.pgf}
  \caption{An execution that cannot maintain sequential consistency during a network partition}
  \label{fig:dsm-cap-example-2}
\end{figure}

\begin{figure}
  \setlength\belowcaptionskip{5ex}
  \begin{subfigure}{1\textwidth}
    \centering
    \input{images/pgf/dsmCAPex2_serial1.pgf}
    \caption{A serial order where $\memread{y}$ precedes $\memwrite{y}{1}$ which forces $\memwrite{x}{1}$ to precede $\memreadVal{x}{0}$}
    \label{fig:dsm-cap-example-2-serial1}
  \end{subfigure}
  \begin{subfigure}{1\textwidth}
    \centering
    \input{images/pgf/dsmCAPex2_serial2.pgf}
    \caption{A serial order where $\memread{x}$ precedes $\memwrite{x}{1}$ which forces $\memwrite{y}{1}$ to precede $\memreadVal{y}{0}$}
    \label{fig:dsm-cap-example-2-serial2}
  \end{subfigure}
  \caption{Possible two serializations of the execution in Figure \ref{fig:dsm-cap-example-2}}
  \label{fig:dsm-cap-example-2-serial}
\end{figure}

\begin{lemma}[CAP for sequential consistency]
  \label{thm:cap-sequential}
  An eventually-available system cannot provide sequential consistency
  in the presense of network partitions.
\end{lemma}
\begin{proof}
  We assume that all uninitialized variables start with the value
  $0$. Consider two processes, $P_1$ and $P_2$, which perform the
  following sequences of operations:
  \begin{itemize}
  \item $P_1$ writes $y := 1$, then reads the value of $x$.
  \item $P_2$ writes $x := 1$, then reads the value of $y$.
  \end{itemize}
  Suppose $P_1$ and $P_2$ are separated by a partition. Since $P_1$
  cannot hear any updates from $P_2$, and its behavior is not affected
  by messages it does not receive, then $P_1$'s behavior will be same
  as in case where $P_2$ does not write to $x$. Therefore, $P_1$ will
  read $x$ as having value 0. Likewise $P_2$ will read $y$ with the
  value $0$. See Figure \ref{fig:dsm-cap-example-2}.

  If the system maintains sequential consistency, the read operations
  must return values that are consistency with a global serialization
  of the history. There are two possibilities for this history:
  \begin{enumerate}
  \item It could be that $\memreadVal{y}{0}$ precedes
    $\memwrite{y}{1}$. In this case, $\memwrite{x}{1}$ would have to
    precede $\memreadVal{x}{0}$. See Figure
    \ref{fig:dsm-cap-example-2-serial1}.
  \item It could be that $\memreadVal{x}{0}$ precedes
    $\memwrite{x}{1}$. In this case, $\memwrite{y}{1}$ would have to
    precede $\memreadVal{y}{0}$. See Figure
    \ref{fig:dsm-cap-example-2-serial2}.
  \end{enumerate}
  However, both possibilities above are incorrect. In either case, the
  second read operation returns an incorrect value.  Thus, during a
  partition, the processes cannot return values from read operations
  in a way that is consistent with some global serial order. Hence, a
  sequentially consistent system is not CAP-available.
\end{proof}

\subsubsection{Consequences of CAP}
\label{interpretation-of-the-cap-theorem}
While the CAP theorem is theoretically simple, its implications are
more nuanced than they may appear \cite{2012CAP12Years}. A common
oversimplification is that the CAP theorem is represents a ``choose 2
of 3'' scenario: a system designer can choose at most two of
consistency, availability, and partition resilience. In fact, real
systems may balance weaker forms of all three properties. The CAP
theorem only rules out the combination of all three properties when
each of them is defined in an idealized, rigid way.

In practice, applications often settle for weaker levels of
consistency than linearizability or sequential consistency. We shall
see an example in the next section. Resilience to network partitions
typically requires coping with intermittent, rather than indefinite,
communications failures. Finally, availability is best measured in
terms of actual response time as experienced by the user, and not the
mere assurance that a request will ``eventually'' be handled. Thus,
each of these dimensions is actually quantitative in nature, rather
than an all-or-nothing proposition.

The locality principle is also highly relevant when considering
implications of the CAP theorem for a real system: the closer agents
are located, the more reliable their communications will be (in
general), and the more applications can provide consistency and
availability for operations that only require coordinating with nearby
agents. At short time scales, operations that only require local
coordination are common.

\subsection{Causal Consistency}
\label{ssec:causal-consistency}
\emph{Causal} consistency\citationneeded is a weaker memory model than
sequential consistency. Whereas sequential consistency requires the
system as a whole to behave as if all write operations take place in
some total order (which must also respect program order), causal
consistency allows different processes to behave as if they witnessed
past write operations take effect in different orders. Only write
operations related by \emph{causally precedence} are required to take
effect in a common order across all processes: ``reads respect the
order of causally related writes.'' \citationneeded

We have not defined what causal precedence means for memory
operations. The notion is similar in its motivation to causal
precedence in message-passing framework (Definition
\ref{def:causalprecedence}) and the idea of causal broadcast
\citationneeded..., but the definition of causally related memory
operations is not as simple as that of causal precedence among
messages. In message passing, a receive event is always associated
with a unique send event, but multiple processes can write the same
value to the same memory location, and for a later operation that
reads this value, it is not clear which write ``caused'' it. For this
purpose we define a \emph{writes-into} order.

\begin{definition}[Writes-into order]
  Let $H$ be a history of memory operations. We are treating $H$ as a
  \emph{multiset}, meaning for example that two operations of the form
  $\memwrite{x}{v}$ are considered distinct if they happen at
  different times or inside different processes. A ``writes into''
  order $\writesinto$ is any binary relation among the operations
  in $H$ that satisfies the following conditions:
  \begin{itemize}
  \item All pairs of operations related by $\writesinto$ are of
    the form $\memwrite{x}{v} \writesinto \memreadVal{x}{v}$ for
    memory some location $x$ and value $v$.
  \item For each operation of the form $\memreadVal{x}{v}$, there is
    exactly one write operation in $H$ that
    $\memwrite{x}{v} \writesinto \memreadVal{x}{v}$
  \end{itemize}
\end{definition}
\citationneeded give a slightly more complex definition allowing
operations that can read uninitialized memory locations---those
returning a default value because there is no prior write to that
location. For simplicity we assume each memory location is written to
before it is read.

\begin{definition}[Causality order on memory operations]
  \label{def:memorycausalprecedence}
  For a given writes-into order $\writesinto$ on
  $H$, the associated \emph{causality order}
  $\causalityorder$ is the transitive closure of the union of
  $\writesinto$ and program order. That is, if $\Op \causalityorder
  \Op'$, then one of the following holds:
  \begin{itemize}
  \item $\Op \programorder{i} \Op'$ for $P_i$
  \item $\Op \writesinto \Op'$
  \item There is some $\Op''$ such that $\Op \causalityorder \Op''$ and $\Op'' \causalityorder \Op'$
  \end{itemize}
  By \emph{fiat}, we also require that $\causalityorder$ must not be cyclic, meaning their are no ``causality loops'' like $\Op \causalityorder \Op' \causalityorder \Op$.
  If $\Op \causalityorder \Op'$, we say $\Op$ causally precedes $\Op'$.
\end{definition}

We can now define causally consistent executions of memory
operations. For a history $H$ and each process $P_i$, let $A_i$ be
union of $H|_P$ and the set of all writes in $H$. In other words,
$A_i$ is the local history of $P_i$ plus any write operation on any
other process.

\begin{figure}[p]
  \begin{subfigure}{1\textwidth}
    \centering
    \input{images/pgf/smEx4S3.pgf}
    \caption{}
    \label{fig:smEx4L2}
  \end{subfigure}
  \begin{subfigure}{1\textwidth}
    \input{images/pgf/smEx4S4.pgf}
    \caption{}
    \label{fig:smEx4S2}
  \end{subfigure}
  \caption{Two non-sequentially-consistent executions of Figure \ref{fig:dsm-example-2}}
  \label{fig:smEx4-alt}
\end{figure}

\begin{definition}[Causal consistency]
  \label{def:causalconsistency}
  An execution is causally consistent if each $P_i$ behaves as if it
  observes some serialization of $A_i$ consistent with $\causalityorder$.
\end{definition}

Notably, Definition \ref{def:causalconsistency} does not require that
all processes behave as if they are observing the \emph{same}
serialization.

\begin{figure}
  \begin{subfigure}{1\textwidth}
    \centering
    \input{images/pgf/smEx4S3.pgf}
    \caption{}
    \label{fig:smEx1L1}
  \end{subfigure}

  \begin{subfigure}{1\textwidth}
    \input{images/pgf/smEx4S4.pgf}
    \caption{}
    \label{fig:smEx5}
  \end{subfigure}

  \caption{PLACEHOLDER: Causally consistent and inconsistent executions}
  \label{fig:smCausal}
\end{figure}
\clearpage

Causal consistency not subject to the limits of the CAP theorem.

\begin{lemma}[Causal consistency is CAP-available]
  \label{thm:cap-causal}
  Causal consistency can be enforced during network partitions. That
  is, causal consistency is not subject to the CAP thereom.
\end{lemma}
\begin{proof}
  Consider processes that execute read and write operations purely
  locally. That is, they never send messages to other processes and
  they only read to their own writes. In this situation, the causal
  relation generated by Definition \ref{def:memorycausalprecedence} is
  essentially unique: operations at $P_i$ are causally preceded by the
  previous operations at $P_i$, and no others. In the absence of a
  non-trivial causal order, causal consistency imposes no constraints
  and the execution is vacuously consistent.
\end{proof}

Unfortunately, the proof of the prior lemma has more to do the
weakness of the promises made causal consistency. The example allows
different processes to deviate arbitrary far from consistency, in the
sense that no processes need to agree on any of the updates applied to
the database. The effect is that causal consistency is too weak to
apply any kind of bound on divergence, which suggests it is not strong
enough for the kinds of safetly-related applications we have in mind.

\subsection{Section Summary}
\label{ssec:background-summary}
This discussion has explored the key challenges involved in building
distributed systems that connect geographically dispersed components
over unpredictable networks. The variability in message delays,
particularly in the context of broadcasts sent to multiple recipients,
can result in messages that arrive in different orders.  This
situation can lead to chaos if a message-ordering discipline is not
imposed.

To mitigate the effects described above, distributed systems must
track the causal precedence relation between events. Because physical
clocks are not generally reliable enough for this purpose, especially
at fine time scales, logical clocks---scalar, vector, and matrix
clocks---are typically used, each with a different tradeoff in terms
of precision of the information tracked and the administrative and
messaging overhead. If groups can change dynamically, as in our use
cases, then additional group membership protocols are needed to ensure
that all processes know which other processes are participating in the
system at any moment.

Programmers may find it easier to frame distributed applications in
terms of reading and writing from a shared pool of virtual memory,
rather than sending messages, by employing the distributed shared
memory or DSM abstraction. However, the fact that many processes can
access the same virtual memory locations at the same time makes it
challenging to maintain systemwide coherence. Strong consistency
models like linearizability and sequential consistency provide the
illusion of a single, unified source of truth, but the CAP theorem
makes it virtually impossible to guarantee these properties over
chaotic, disruption-prone networks. Weaker models like causal
consistency are not subject to the same limitations, but they do not
enforce any limits bounding how far apart data replicas can
diverge. This renders weaker models potentially unsuitable for
safety-critical applications like we aim to build.

In summary, there is no free lunch in distributed systems. Designing
resilient distributed systems for emergency response scenarios
requires a careful balancing act between competing properties that is
carefully calibrated to the use case and the operational environment.

\section{Continuous Consistency}
\label{sec:continuous-consistency}
In this section, we present Golding's Timestamped Anti-Entropy (TSAE)
protocol. TSAE is a \emph{weak consistency group communication}
mechanism, a concept whose meaning will become clearer as we progress
through the section. It is important to note that the version of TSAE
presented here is not fully specified---certain implementation details
can be altered depending on the specific needs of the higher-level
distributed application that utilizes TSAE as its group communication
layer.

Here, we imagine TSAE as one part of a middleware used to coordinate
first responders throughout a wide area (possibly statewide, or even
distributed throughout a US region) responding to major events like
natural disasters and massive wildfires. Section
\ref{sec:disaster-response} discusses exemplary application features
like monitoring firefighter's GPS locations, tracking the current fire
boundaries, sharing weather data, and providing mechanisms to dispatch
resources.

TSAE allows replicating the underlying data managed by the application
throughout a distributed system. That is, it facilitates storing
multiple copies of the same data in different places. For some
applications, this might involve storing a complete set of data on
each user's cell phone. For more data-intensive applications running
on centralized servers, it might be that multiple emergency operating
centers or incident command systems store a copy of the data.

Replication supports \emph{scalability} because application services
can be provided by any processes that maintains a replica of the data,
distributing the load amongst a set of processes rather than a
centralized server. For the same reason, replication supports
resiliency, as there is not a single point of failure. But perhaps
above all, replication improves \emph{locality}, first discussed in
Section \ref{ssec:communication-patterns}. Users of the application
can receive service by contacting the nearest replica over a
low-latency, high-reliability communications link.

% TSAE is a middleware-level protocol that provides reliable group
% messaging over a lower-level network. Depending on the
% implementation details, which are determined by the needs of the
% higher-level application, TSAE can provide any of the message
% ordering guarantees discussed in Section \ref{}. For instance, TSAE
% can be used to implement a totally ordered group broadcast service
% that communicates by passing unicast messages over a TCP/IP network,
% where the TCP/IP component only guarantees FIFO ordering by itself.


% The point of replicating is to improve locality. See Golding
% dissertation Section 1.2

The reliability of TSAE means every message is eventually delivered to
every functioning group member, assuming the underlying network does
not suffer from permanent partitions. A canonical use of TSAE is to
replicate some kind of database at multiple sites. As TSAE only
provides a messaging service, this usage scenario does not dictate
whether to prefer availability or consistency during a partition. We
return to this discussion in Section \ref{ref} which unites the
preceding and present material.

The basic idea of TSAE is simple if we assume principals have
approximately synchronized clocks. All principals maintain a write log
containing messages sent by themselves are other principals. All
entries are tagged with the principal identifier and a timestamp. As
an invariant, if a write log contains a message $m$ originating from
$P$, then it also contains every message originating at $P$ sent
earlier than $m$ (i.e. having a lesser timestamp). Hence, the write
log at any moment is completely determined by knowing the gre


\subsubsection{Assumptions and Data Structures}

\paragraph{Assumptions}

\[
  \textbf{A1}: \textrm{The set of group members is static and known to every member.}
\]
In particular, we assume member knows their own identity. This
statement is necessary because in a dynamic group environment,
mechanisms are required to assign unique identifiers to principals and
inform them of this assignment. For instance, in a typical
TCP/IP-based network, a central DHCP service assigns IP addresses to
computers when they added to the network, ensuring there are no
conflicts.

TSAE can be augmented with a service that provides dynamic group
membership, ensuring all members are informed of changes to the
membership list. See Golding.

\[
  \textbf{A2}: \textrm{Members have approximately synchronized physical clocks}
\]

The clock resolution must be fine-grained enough for each principal to
assign unique timestamps to all important events (e.g. receiving a
message) occurring in that process. We assume clocks are pairwise
synchronized to within some fixed $\delta$, which requires some
mechanism like NTP to correct for clock drift. Messages originating
from processes with fast clocks may experience a $\delta$-length
latency after they are received before they are delivered to the
application.

The assumption of clock synchronization can be weakened. Section 5.4.4
of Golding \citationneeded discusses a form of matrix-based
acknowledgement vectors. This requires no synchronization but adds
storage and protocol overhead. For the applications we have in mind
(database replication over a heavily disrupted ad-hoc network), we
expect sufficiently fine clock synchronization is indeed realistic in
many cases, provided timekeeping mechanisms like GPS signals or NTP
are available often enough to correct for drift.

\paragraph{Message Log}

Messages are added to the message log when they are received. The
message log is linear, and typically the order will differ among
principals. (Nonetheless the message \emph{delivery} can enforce
things like a total order.)

\begin{itemize}
\item When a message originates at process $P$, $P$ tags it with $(P, \currenttime{P})$
\end{itemize}

Messages are always transmitted in batches. For instance, if $A$
updates $B$, $A$ will send $B$ the entire set of messages sent by $C$
that $A$ estimates $B$ has not heard. If this batch includes a message
with timestamp $t$, then it also includes every other message sent by
$C$ with timestamp less than $t$. Below, this corresponds to the
\emph{coverage property}.

\subsubsection{Clock vectors}

\subsubsection{Invariants}

\subsection{The Algorithm}

Each process $P_i$ has access to an approximately synchronized physical clock
The data structures maintain by each process:

\begin{itemize}
\item The \emph{message log}, a queue of timestamped messages received by the host
\item The \emph{summary} vector, containing a \emph{physical} clock value for each host
\item The \emph{acknowledgement} vector,\ an additional set of $n$ clock values
\end{itemize}

The summary vector is much like the vector clocks of Section
\ref{sssec:vector-clocks}, except they store physical timestamps
rather than logical ones. That is to say, whenever $P$ timestamps some
event it reads the current physical time. As usual, summary vectors
will be piggybacked used by the receiver to update their vector
pointwise.

The message log contains a queue of the messages seen by $P$ sent by
any process.

%\textbf{Invariant}: If the message log of $A$ contains a message with
%timestamp $(B, t)$, then it also contains every message whose
%timestamp is $(B, t')$ where $t' < t$.


\begin{description}
\item[Coverage Property] $A$ has seen all from $B$ whose timestamps are less
  than the maximum timestamp of any message $A$ received from $B$
  \[ \WLat{A}{B} \overset{covg.}{=} \{w \in \WLat{B}{B} | w.t \leq \summary{A}{B} \} \]
\item[Acknowledgement Property] $A$'s commit line is less than or equal to the minimum value in $A$'s summary vector. It's ackowledgement entry for $B$ is a lower bound estimate of $B$'s commit line.
  \begin{align*}
    \ack{A}{A} &\overset{ack}{\leq} \min_{i \in \AllProc} \left(\summary{A}{i}\right) \\
    \ack{A}{B} &\overset{ack}{\leq} \ack{B}{B}
  \end{align*}
\end{description}

As a corollary, all messages in $\WLat{A}{A}$ (i.e. all the ones
originating at $A$) have a timestamp less than
$\summary{A}{A}$. Whenever the TSAE protocol needs to read the value
of $\summaryVec{A}$, it first updates $\summary{A}{A}$ to
$\currenttime{A}$. The key property of $\summary{A}{A}$ is that it
always greater than the maximum timestamp in $\WLat{A}{A}$. It is also
less than $\currenttime{A}$---otherwise a newly message accepted would
be timestamped with $\currenttime{A}$ and violate the previous
inequality.

For all intents and purposes $\ack{A}{A}$ is the minimum value
timestamp in the summary vector of $A$. For implementors, it would
suffice that $\ack{A}{A}$ is always less than this value to avoid
constantly iterating over the summary vector to compute the minimum,
as long as $\ack{A}{A}$ is periodically updated.

\newpage
\begin{landscape}
  \begin{figure}%For some reason this empty figure adds vertical whitespace that makes the next figure positioned similarly to the ones that follow it.
  \end{figure}
  \begin{figure}[h]
    \centering
    \includegraphics[width=1.4\textwidth]{images/tsae/TSAE1.png}
    \caption{TSAE at time $t=1$. $A$ has originated a single message with timestamp $t=1$, but $B$ and $C$ have not received it.}
    \label{fig:tsae1}
  \end{figure}
  \begin{figure}[h]
    \centering
    \includegraphics[width=1.4\textwidth]{images/tsae/TSAE2.png}
    \caption{TSAE at time $t=2$. $B$ and $C$ have both accepted messages with timestamp $t=2$.}
    \label{fig:tsae2}
  \end{figure}
  \begin{figure}[h]
    \centering
    \includegraphics[width=1.4\textwidth]{images/tsae/TSAE3.png}
    \caption{TSAE at time $t=3$. $B$ has accepted another message with timestamp $t = 3$. At this moment we assume $A$ and $B$ initiate an anti-entropy session and exchange summary vectors $\langle 3,0,0\rangle$ and $\langle 0,3,0 \rangle$, which they use to determine which messages have not been seen by the other, shown in shaded boxes.}
    \label{fig:tsae3}
  \end{figure}
  \begin{figure}[h]
    \centering
    \includegraphics[width=1.4\textwidth]{images/tsae/TSAE4.png}
    \caption{TSAE at time $t=4$. $A$ and $B$ have finished their TSAE session. Concurrently, $B$ accepted a new message with timestamp $t=4$. Neither $A$ nor $B$ can update their commit line past $0$, because they both contain $\summary{}{C} = 0$, indicating they have not seen any messages from $C$. At this moment we assume $A$ and $C$ initiate a TSAE session and decide to exchange the shaded messages.}
    \label{fig:tsae4}
  \end{figure}
  \begin{figure}[h]
    \centering
    \includegraphics[width=1.4\textwidth]{images/tsae/TSAE5.png}
    \caption{TSAE at time $t=5$. $A$ and $C$ have finished their TSAE session. While $A$ was sending messages to $C$ it also accepted a new message with timestamp $t=5$. $A$ and $C$ can both update their commit lines to $3$, because they have seen all messages with timestamps less than or equal to $3$. However, their purge lines remain at $0$, because their acknowlegment vectors satisfy $\ack{\!}{B} = 0$, indicating their (accurate) estimate of $B$'s commit line. In particular, it would be unsafe for them to purge the message with stamp $(C, 2)$ because then $B$ would never receive it. At this moment we assume $B$ and $C$ initiate a TSAE session and decide to exchange the shaded messages.}
    \label{fig:tsae5}
  \end{figure}
  \begin{figure}[h]
    \centering
    \includegraphics[width=1.4\textwidth]{images/tsae/TSAE6.png}
    \caption{TSAE at time $t=6$. $A$ accepted a new message. $B$ and $C$ have finished their TSAE session. $B$ and $C$ can both update their commit lines to $4$, because they know they have seen all messages with timestamp less than $4$. However, their purge lines remain at $3$, because their acknowlegment vectors satisfy $\ack{}{A} = 3$, and in particular they cannot purge the message $(B, 4)$ because $A$ has not seen it. $A$ does not purge any messages, because it underestimates $B$'s commit line as $\ack{A}{B} = 0$, so $A$ believes it's possible that $B$ has not received $(C, 2)$.}
    \label{fig:tsae6}
  \end{figure}

\begin{comment}
  \begin{figure}[h]
    \centering
  \begin{subfigure}[b]{0.5\textwidth}
    \centering
    \includegraphics[width=0.9\textwidth]{images/tsae/Process A Message Log.png}
    \caption{Cellular network topology}
    \label{fig:centralized}
  \end{subfigure}
  \begin{subfigure}[b]{0.5\textwidth}
    \centering
    \includegraphics[width=0.9\textwidth]{images/tsae/Process B Message Log.png}
    \caption{Ad-hoc network topology}
    \label{fig:decentralized}
  \end{subfigure}
  \begin{subfigure}[b]{0.5\textwidth}
    \centering
    \includegraphics[width=0.9\textwidth]{images/tsae/Process B Message Log.png}
    \caption{Ad-hoc network topology}
    \label{fig:decentralized}
  \end{subfigure}
  \caption{States of processes $A$ and $B$ before an anti-entropy session}
  \label{fig:nettopology}
\end{figure}
\begin{figure}[h]
  \centering
  \begin{subfigure}[b]{0.5\textwidth}
    \centering
    \includegraphics[width=\textwidth]{images/tsae/Process A Message Log.png}
    \caption{Cellular network topology}
    \label{fig:centralized}
  \end{subfigure}
  \begin{subfigure}[b]{0.5\textwidth}
    \centering
    \includegraphics[width=1\textwidth]{images/tsae/Process B Message Log.png}
    \caption{Ad-hoc network topology}
    \label{fig:decentralized}
  \end{subfigure}
  \begin{subfigure}[b]{0.5\textwidth}
    \centering
    \includegraphics[width=1\textwidth]{images/tsae/Process B Message Log.png}
    \caption{Ad-hoc network topology}
    \label{fig:decentralized}
  \end{subfigure}
  \caption{States of processes $A$ and $B$ before an anti-entropy session}
  \label{fig:nettopology}
\end{figure}
\newpage
\begin{figure}[h]
  \centering
  \begin{subfigure}[b]{0.5\textwidth}
    \centering
    \includegraphics[width=\textwidth]{images/tsae/Process A Message Log.png}
    \caption{Cellular network topology}
    \label{fig:centralized}
  \end{subfigure}
  \begin{subfigure}[b]{0.5\textwidth}
    \centering
    \includegraphics[width=1\textwidth]{images/tsae/Process B Message Log.png}
    \caption{Ad-hoc network topology}
    \label{fig:decentralized}
  \end{subfigure}
  \begin{subfigure}[b]{0.5\textwidth}
    \centering
    \includegraphics[width=1\textwidth]{images/tsae/Process B Message Log.png}
    \caption{Ad-hoc network topology}
    \label{fig:decentralized}
  \end{subfigure}
  \caption{States of processes $A$ and $B$ before an anti-entropy session}
  \label{fig:nettopology}
\end{figure}
\end{comment}
\end{landscape}
\clearpage

\subsubsection{Message delivery}

\subsubsection{Message purging}

\subsection{Properties of TSAE}

\begin{definition}
  We provide a couple shorthand definitions
\begin{description}
  \item[Global writes] The set of all writes submitted to any replica
    \[ \W \equiv \bigcup_{i \in \AllProc} \WL{A}\]
  \item[Unseen Writes] The set of global writes not seen by a particular replica
    \[\Unseen{A} \equiv \W \setminus \WL{A} \]
  \item[Estimated Unseen Writes] $A$'s estimate of writes unseen by $B$
    \[\UnseenEst{A}{B} \equiv \{w \in \WL{A} | w.t > \ack{A}{B} \} \]
  \item[Commit line] The commit line at $A$ is $A$'s own
    acknowledgement timestamp and provides a lower bound estimate of
    messages seen by $A$
    \[\commitline{A} \equiv \ack{A}{A}\]
  \item[Global commit line] The global commit line is the minimum
    value of any process's ackowledgement timestamp and provides an
    upper bound of messages seen by anyone
    \[ \globalcommit \equiv \min_{i \in \AllProc} \left( \commitline{A} \right) \]
  \item[Purge line] $A$'s purge line is $A$'s upper bound estimate of the global commit line
    \[ \knowncommit{A} \equiv \min_{i \in \AllProc} \left( \ack{A}{i} \right) \]
  \end{description}
\end{definition}


%    \item[Acknowledgement Property] $A$'s acknowledgment entry for $B$ is a lower bound estimate of $B$'s commit line
%      \[\ack{A}{B} \leq \ack{B}{B} \equiv \commitline{B}\]

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{corollary}
  \label{cor:commitline}
  $A$ has seen all writes with timestamps less than or equal to its commit line. That is,
  \[w.t \leq \commitline{A} \implies w.t \in \WL{A}.\]
\end{corollary}
\begin{proof}
  Consider a message $w$ originating at $B$ with timestamp $w.t \leq \commitline{A}$.
  Then \[w.t \leq  \commitline{A} \equiv \ack{A}{A} \overset{ack}{\leq} \min_{i \in \AllProc}\left(\summary{A}{i}\right) \leq \summary{A}{B}.\]
  By the coverage property, $w.t \in \WLat{A}{B}$.
\end{proof}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

In particular, a message with a timestamp less than or equal to $\globalcommit$ has been seen by everyone.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{corollary}
  \label{cor:purgeline}
  $A$'s purgeline is a lower bound of the global commit line.
\end{corollary}
\begin{proof}
  First we observe that $A$'s purgeline is a lower bound of every process's commit line
  \[\knowncommit{A} \equiv \min_{i \in \AllProc} \left( \ack{A}{i} \right) \leq \ack{A}{B} \overset{ack}{\leq} \commitline{B}
  \]
  But the global commit line is the minimum (greatest lower bound) of all commit lines, so we have
  \[ \knowncommit{A} \leq \globalcommit. \]
\end{proof}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{corollary}[Log purging]
  \label{cor:purge}
  $A$ can safely discard from its write log all messages whose timestamp is less than or equal to $\knowncommit{A}$.
\end{corollary}
\begin{proof}
  We have $w.t \leq \knowncommit{A} \leq \commitline{B} \implies w \in \WL{B}$ by combining Corollaries \ref{cor:pugreline} and \ref{cor:commitline}.
  Consequently, $w$ has been seen by every other process and will not be exchanged during any future anti-entropy sessions.
\end{proof}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{corollary}
  \label{cor:unseenest}
  $A$'s estimate of writes unseen by $B$ form an over-approximation of $B$'s true unseen writes.
\end{corollary}
\begin{proof}
  Consider a message $w$ sent by $C$ (where possibly $C = A$ or $C = B$). We must show
  $w \in \Unseen{B}$ implies $w \in \UnseenEst{A}{B}$.  If $w \in \left(\W \setminus \WL{B}\right)$ then we have
  \[ \summary{B}{C} < w.t\]
  (Otherwise $w \in \WLat{B}{C} \subset \WL{B}$ by the coverage property.)
  We conclude by the following inequality.
  \[\ack{A}{B} \overset{ack}{\leq} \ack{B}{B} \overset{ack}{\leq} \min_{i \in \AllProc} \left(\summary{B}{i}\right) < \summary{B}{C} < w.t\]
\end{proof}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

The contrapositive of Corollary \ref{cor:unseenest} is that $A$'s
write log contains a message with timestamp $w.t \leq \ack{A}{B}$,
then $A$ knows $B$ has already received this message.

\subsection{Conit Definitions}


\newcommand{\UnseenWeight}[1]{\textsf{Unseen}_{#1}}
\newcommand{\UnseenWeightEst}[2]{\textsf{EstUnseen}_{#1}{\left(#2\right)}}


\begin{definition}
  We provide a couple shorthand definitions
\begin{description}
  \item[Total Weight] $\textsf{NumWeight}(W) = \sum_{w \in W} \textsf{numweight}{\left(w\right)}$
  \item[Unseen Numerical Weight] $\textsf{UnseenNumWeight}_{A} = \textsf{NumWeight}\left(\Unseen{A}\right)$
  \item[CorrectHistory] $\WL{A}|_{\mathcal{F}} \cap \WL{\mathsf{ideal}}|_{\mathcal{F}}$
  \item[IncorrectHistory] $\WL{A} - \left(WL{A}|_{\mathcal{F}} \cap \WL{\mathsf{ideal}}|_{\mathcal{F}}\right)$
  \item[Unseen Order Weight] $\textsf{OrdWeight}\left(\Unseen{A}\right)$
  \item[Estimated Unseen Positive Weight] $\textsf{NumWeight}\left(\{w \in \Unseen{A} | w.\textsf{numweight} \geq 0 \}\right)$
  \item[Estimated Unseen Negative Weight] $\textsf{NumWeight}\left(\{w \in \Unseen{A} | w.\textsf{numweight} \leq 0 \}\right)$
  \end{description}
\end{definition}




Strong (i.e. atomic or sequential) consistency, as defined in Section
\ref{ssec:strong-consistency}, is an all-or-nothing proposition: an
application provides strong consistency or it does not. Because of
considerations such as the CAP theorem, this sort of guarantee is an
unrealistic requirement for our setting. Causal consistency (Section
\ref{ssec:causal-consistency}) and other kinds of weaker assumptions
are tenable, but by themselves they provide no upper bound on the
divergence between replicas of data items. Because of the
safety-related nature of our system, we would like to do better than
this.

This section summarizes the key ideas behind \emph{continuous}
consistency, particularly the conit (``consistency unit'') model
proposed by Yu and Vahdat
\cite{2000tact,2000tactalgorithms,10.5555/1251229.1251250,DBLP:conf/icdcs/YuV01,2002tact},
designed to strike a balance between generality and
practicality. Continuous consistency is motivated by the observation
that for many real-world applications, it is evidently acceptable to
work with data that is only consistent to within some error bound
$\epsilon \geq 0$. It is not immediately obvious what this means
formally, but loosely speaking one hopes to measure the difference
between the user-observed value of a database operation and the value
that would have been returned if we were maintaining strong
consistency. By allowing replicas to diverge by some amount, the
thought goes, we sacrifice strong consistency but gain additional
flexibility in choosing how often and with which other replicas we
synchronize our data. In terms of the CAP theorem, by increasing
$\epsilon$ and thus relaxing our error bound, we trade consistency for
availability, but without abandoning all guarantees about the relative
correctness of values observed by the user. Thus, we shift from
consistency and availability as all-or-nothing conditions, as in CAP,
towards viewing them as measurable values that can be traded along a
\emph{spectrum} of possible configurations.

A quantitative definition of consistency seems specific to each
application, but ideally our consistency model can be defined and
implemented in an application-generic manner. Thus we have a few
requirements.
\begin{itemize}
  \tightlist
\item A mechanism to measure (in)consistency of data items
\item A mechanism for applications to set upper bounds on the allowed
  inconsistency of each item
\item A mechanism to efficiently enforce these inconsistency bounds
\end{itemize}

To implement a continuous consistency model that is not tied to one
application, we must first explain how inconsistency can be quantified
generically in the first place. Second, we must explain how a generic
protocols can be used to enforce.

\subsection{The Conit Model}
\label{ssec:conits}

We assume a fixed set $\mathcal{P} = \{P_1, P_2, \ldots P_n\}$ of
processes that coordinate to replicate a database $D$. Note that we
read ``database'' very loosely to mean any kind of data items that can
be updated in response to user requests---our ownly real requirement
is that it is possible to roll back (undo) updates if necessary. We
also make a couple simplifying assumptions about our system.
\begin{itemize}
  \tightlist
\item The database is replicated in whole at each site.
\item The group members are fixed in advance. Otherwise we would
  require a group-membership protocol to update the set of
  participants dynamically.
\end{itemize}

The application could be essentially anything. Relevant examples for
our environment could be a group chat application, a system for
requesting and dispatching resources, or a system for disseminating
information about weather and fire conditions. Clients can submit
requests to read or write (or update) values from the database at any
site. However, the application itself never updates the database
directly, but instead submits requests to a lower-level ``middleware''
that is logically situated below the application but above the network
transport layer (see Section \ref{ssec:shared-memory}). The role of
the middleware is to mediate between the application and the local
database.

When the application submits a database operation (read or write), the
middleware does not necessarily respond to the request
immediately. Instead, it may block (become unavailable) while it
coordinates with other processes to enforce the sort of consistency
guarantees defined below. During this time, the middleware can inform
other sites about new updates, or request that other sites report to
it any updates that could be relevant to the user's
request. Eventually, the middleware reads or writes from the local
copy of the database and returns a value back to the application, who
gives it to the user.

Separately from the local database itself, the middleware maintains a
\emph{write log} that stores a history of recent write requests it has
seen.

\subsubsection{Dimensions of consistency}
\label{measuring-consistency-on-conits}

We discuss the three consistency dimensions and how they can be
implemented. Real-time staleness and order error are both bounded by a
\emph{pull}-based approach: the originating replica of a database
operation may block while contacting other sites in order to request
information relevant to the request. Numerical error in contrast is
bounded by a \emph{push}-approach: A node may have to block during a
request in order to proactively inform other nodes about the update
before it can be applied.

\subsubsection{Real time staleness}
\label{sssec:real-time-consistency}
Take, say, an application for disseminating the most up-to-date
visualization of the location of a fire front. It may be acceptable if
this information appears 5 minutes out of date to a client, but
unacceptable if it is 30 minutes out of date. That is, we could
measure consistency with respect to \emph{time}. One should expect the
exact tolerance for \(\epsilon\) will be depend very much on the
client, among other things. For example, firefighters who are very
close to a fire have a lower tolerance for stale information than a
central client keeping only a birds-eye view of several fire fronts
simultaneously.

To enforce real time staleness, we assume that each site has loosely
synchronized physical clocks. Note that while we do not assume
physical clocks can be synchronized precisely enough for the kind of
causality tracking discussed in Section \ref{sec:background}, it may
be tenable to assume synchronization precise enough for the purposes
discussed in this section, depending on how small the allowed
real-time divergence values are. For example, we estimate that
synchronization within, say, 30 minutes is tenable using appropriate
synchronization mechanisms even in deeply challenging network
environments.

Each site maintains a vector of \emph{physical} timestamp values. The
\emph{staleness} of a conit is the physical time elapsed since the
last update not seen by this replica was submitted.

The update rule is simple. When the real time.

\newcommand{\vtphys}[2]{\mathrm{vt}_{#1}}
\begin{itemize}
\item First, check whether $t_{\mathrm{now}} - \vt{i}[j] < \delta$
  holds for each entry $j$ in the real time vector.
\item If $t_{\mathrm{now}} - \vt{i}[j] \geq \delta$, $P_i$ sends a
  request to $P_j$ to pull any updates seen by $P_j$ but not by $P_i$.
\item After receiving the updates, $P_i$ reads the conit's value and
  returns it to the user.
\end{itemize}

\subsubsection{Order consistency}
\label{order-consistency}
When the number of tentative (uncommitted) writes is high, TACT
executes a write commitment algorithm. This is a \emph{pull-based}
approach which pulls information from other processes in order to
advance \(P_i\)'s vector clock, raising the watermark and hence
allowing \(P_i\) to commit some of its writes.

\subsubsection{Numerical consistency}
\label{numerical-consistency}

We describe split-weight AE. Yu and Vahdat also describe two other
schemes for bounding numerical error. One, compound AE, bounds absolute
error trading space for communication overhead. In their simulations,
they found minimal benefits to this tradeoff in general. It is possible
that for specific applications the savings are worth it. They also
consider a scheme, Relative NE, which bounds the relative error.


\paragraph{Dynamic bounds}
Because real-time staleness and order error are bounded by pull
approach, it is straightforward to allow the user to dynamically
change the error bounds at each site. However, numerical error is
bounded by a push approach that requires every process to be aware of
all other processes's bounds and proactively cooperation to ensure
this invariant is maintained. Therefore, dynamically tuning numerical
error bounds requires a consensus mechanism that allows informing
other processes of any updates to these bounds. The application cannot
guarantee the new consistency bound will be enforced until it knows
that all other processes have seen the newly updated bounds.

\subsubsection{Variations and Additional Features}
One can imagine various ways that the conit model can be augmented
with additional capabilities.

\paragraph{Dynamic conits}
Mechanism for conits to be created. Because the application (whose
source code is of course is fixed) must specify the weight of each
update to each conit, this requires that the weight of an update can
be calculated. Donkervliet's master's thesis \citationneeded explored
dynamic conits in the context of massive multiplayer online games,
particularly Minecraft.

\paragraph{Dynamic network tuning}
We expect a rich interplay between the network protocol and a
conit-based replication protocol. We previously mentioned an example
where a UAV or a message ferry could be deployed dynamically to
provide greater throughput in a particular geographical area. Such a
resource could be dispatched if the application signals to a network
controller that it is struggling to enforce conit bounds in a timely
manner.

Network packets, or DTN bundles, could be specially marked as
containing database updates alongside any metadata (such as the weight
of an update to various conits) that could be used by the network for
quality-of-service purposes. Such usage may run contrary to the
conventional wisdom that networking protocols should be agnostic to
the actual content of a message, e.g. routers should be concerned only
with the data in IP packet headers but not the data contained in the
packet. This sort of atypical usage is potentially justified in our
setting because of a heightened requirement to optimize the user of
very scarce networking resources, even at the cost of blurring the
line between the network and application layers. We conjecture that
SDN would be particularly suitable because it is easier to modify or
customize software-defined networking protocols, so that custom
hardware is not required even for extremely specialized networking
needs.

\subsection{Old material}

The definition of \(\epsilon\) evidently requires a more or less
application-specific notion of divergence between replicas of a shared
data object.

Now suppose many disaster-response agencies coordinate with to update
and propagate information about the availability of resources. A client
may want to lookup the number of vehicles of a certain type that are
available to be dispatched within a certain geographic range. We may
stipulate that the value read by a client should always be \(4\) of the
actual number, i.e.~we could measure inconsistency with respect to some
numerical value.

In the last example, the reader may wonder we should tolerate a client
to read a value that is incorrect by 4, when clearly it is better to be
incorrect by 0. Intuitively, the practical benefit of tolerating weaker
values is to tolerate a greater level of imperfection in network
communications. For example, suppose Alice and Bob are individually
authorized to dispatch vehicles from a shared pool. In the event that
they cannot share a message.

Or, would could ask that the the value is a conservative estimate,
possibly lower but not higher than the actual amount. In these examples,
we measure inconsistency in terms of a numerical value.

As a third example,

By varying \(\epsilon\), one can imagine consistency as a continuous
spectrum. In light of the CAP theorem, we should likewise expect that
applications with weaker consistency requirements (high \(\epsilon\))
should provide higher availability, all other things being equal.

Yu and Vahdat explored the CAP tradeoff from this perspective in a
series of papers \cite{2000tact,2000tactalgorithms,10.5555/1251229.1251250,DBLP:conf/icdcs/YuV01,2002tact}
propose a theory of \emph{conits}, a logical unit of data subject to
their three metrics for measuring consistency. By controlling the
threshold of acceptable inconsistency of each conit as a continuous
quantity, applications can exercise precise control the tradeoff between
consistency and performance, trading one for the other in a gradual
fashion.

They built a prototype toolkit called TACT, which allows applications to
specify precisely their desired levels of consistency for each conit. An
interesting aspect of this work is that consistency can be tuned
\emph{dynamically}. This is desirable because one does not know a priori
how much consistency or availability is acceptable.

The biggest question one must answer is the competing goals of
generality and practicality. Generality means providing a general notion
of measuring \(\epsilon\), while practicality means enforcing
consistency in a way that can exploit weakened consistency requirements
to offer better overall performance.

\begin{itemize}
\item
  The tradeoff of CAP is a continuous spectrum between linearizability
  and high-availability. More importantly, it can be tuned in real time.
\item
  TACT captures neither CAP-consistency (i.e.~neither atomic nor
  sequential consistency) nor CAP-availability (read and write requests
  may be delayed indefinitely if the system is unable to enforce
  consistency requirements because of network issues).
\end{itemize}

\begin{comment}
\hypertarget{causal-consistency-1}{%
  \subsection{Causal consistency}\label{causal-consistency-1}}

Causal consistency is that each clients is consistent with a total order
that contains the happened-before relation. It does not put a bound on
divergence between replicas. Violations of causal consistency can
present clients with deeply counterintuitive behavior.

\begin{itemize}
  \tightlist
\item
  In a group messaing application, Alice posts a message and Bob
  replies. On Charlie's device, Bob's reply appears before Alice's
  original message.
\item
  Alice sees a deposit for \$100 made to her bank account and, because
  of this, decides to withdraw \$50. When she refreshes the page, the
  deposit is gone and her account is overdrawn by \(50\). A little while
  later, she refreshes the page and the deposit reappears, but a penalty
  has been assessed for overdrawing her account.
\end{itemize}

In these scenarios, one agent takes an action \emph{in response to} an
event, but other processes observe these causally-related events taking
place in the opposite order. In the first example, Charlie is able to
observe a response to a message he does not see, which does not make
sense to him. In the second example, Alice's observation at one instance
causes her to take an action, but at a later point the cause for her
actions appears to have occurred after her response to it. Both of these
scenarios already violate atomic and sequential consistency because
those models enforce a system-wide total order of events. Happily, they
are also ruled out by causally consistent systems. The advantage of the
causal consistency model is that it rules out this behavior without
sacrificing system availability, as shown below.

Causal consistency enforces a global total order on events that are
\emph{causally related}. Here, causal relationships are estimated very
conservatively: two events are potentially causally if there is some way
that the outcome of one could have influenced another.

\begin{figure}
  \center
  \includegraphics[scale=0.4]{images/causal1.png}
  \caption{A causally consistent, non-sequentially-consistent execution}
\end{figure}

\begin{lemma}
  Sequential consistency implies causal consistency.
\end{lemma}
\begin{proof}
  This is immediate from the definitions. Sequential consistency
  requires all processes to observe the same total order of events,
  where this total order must respect program order. Causal consistency
  only requires processes to agree on events that are potentially
  causally related. Program order is a subset of causal order, so any
  sequential executions also respects causal order.
\end{proof}

However, causal consistency is not nearly as strong as sequential
consistency, as processes do not need to agree on the order of events
with no causal relation between them. This weakness is evident in the
fact that the CAP theorem does not rule out highly available systems
that maintain causal consistency even during network partitions.

\begin{lemma}
  A causally consistent system need not be unavailabile during partitions.
\end{lemma}
\begin{proof}

  Suppose $P_1$ and $P_2$ maintain replicas of a key-value store, as
  before, and suppose they are separated by a partition. The strategy is
  simple: each process immediately handles read requests by reading from
  its local replica, and handles write requests by applying the update
  to its local replica. It is easy to see this leads to causally
  consistent histories. Intuitively, the fact that no information flows
  between the processes also means the events of each process are not
  related by causality, so causality is not violated.  \end{proof}

Note that in this scenario, a client's requests are always routed to the
same processor. If a client's requests can be routed to any node, causal
consistency cannot be maintained without losing availability. One
sometimes says that causal consistency is ``sticky available'' because
clients must stick to the same processor during partitions.

The fact that causal consistency can be maintained during partitions
suggests it is too weak. Indeed, there are no guarantees about the
difference in values for \(x\) and \(y\) across the two replicas.
\end{comment}

%\subsection{TACT system model}
%\label{tact-system-model}

As in Section \ref{sec:background}, we assume a distributed set of
processes collaborate to maintain local replicas of a shared data object
such as a database. Processes accept read and write requests from
clients to update items, and they communicate with each other to ensure
to ensure that all replicas remain consistent.

However, access to the data store is mediated by a middleware library,
which sits between the local copy of the replica and the client. At a
high level, TACT will allow an operation to take place if it does not
violate user-specific consistency bounds. If allowing an operation to
proceed would violate consistency constraints, the operation blocks
until TACT synchronizes with one or more other remote replicas. The
operation remains blocked until TACT ensures that executing it would
not violate consistency requirements.

\[\textrm{Consistency} = \langle \textrm{Numerical error, \textrm{Order error}, \textrm{Staleness}} \rangle.\]

Processes forward accesses to TACT, which handles commiting them to the
store. TACT may not immediately process the request---instead it may
need to coordinate with other processes to enforce consistency. When
write requests are processed (i.e.~when a response is sent to the
originating client), they are only commited in a \emph{tenative} state.
Tentative writes eventually become fully committed at some point in the
future, but when they are commited, they may be reordered. After
fullying committing, writes are in a total order known to all processes.

\begin{figure}[h]
  \center
  \includegraphics[scale=0.4]{images/TACT Logs.png}
  \caption{Snapshot of two local replicas using TACT}
  \label{fig:tact_logs}
\end{figure}

A write access \(W\) can separately quantify its \emph{numerical weight}
and \emph{order weight} on conit \(F\). Application programmers have
multiple forms of control:

Consistency is enforced by the application by setting bounds on the
consistency of read accesses. The TACT framework then enforces these
consistency levels.


\section{Conclusion and Summary}
\label{sec:conclusion}



\section*{Bibliography}\label{bibliography}
\addcontentsline{toc}{section}{Bibliography}

\bibliographystyle{abbrv}
\bibliography{bibliography}
\end{document}

%%% Local Variables:
%%% mode: LaTeX
%%% TeX-master: t
%%% End:


\begin{figure}[p]
  \begin{subfigure}[a]{1\textwidth} \center
    \includegraphics[scale=0.4]{images/linear1.png} \caption{A
      linearizable execution. Any choice of linearization works here.}
    \label{fig:linear_example11} \end{subfigure}
  \begin{subfigure}[b]{1\textwidth} \center
    \includegraphics[scale=0.4]{images/nonlinear0.png} \caption{A
      non-linearizable execution. The request to read $y$ returns a
      stale value. } \label{fig:linear_example12} \end{subfigure}
  \caption{A linearizable and non-linearizable execution.}
  \label{fig:linear_example1} \end{figure}

\begin{figure}[p]
  \begin{subfigure}[a]{1\textwidth}
    \center
    \includegraphics[scale=0.4]{images/linearTemplate.png}
    \caption{An execution with read responses left unspecified.}
    \label{fig:nonlinear}
  \end{subfigure}
  \begin{subfigure}[b]{1\textwidth}
    \center
    \includegraphics[scale=0.4]{images/linear3.png}
    \caption{A linearizable execution for which both reads return $1$.}
  \end{subfigure}
  \begin{subfigure}[c]{1\textwidth}
    \center
    \includegraphics[scale=0.4]{images/linear2.png}
    \caption{A linearizable execution for which both reads return $2$.}
  \end{subfigure}
  \caption{Two linearizable executions of the same underlying events that return different responses. Possible linearization points are shown in red.}
  \label{fig:linearization}
\end{figure}

\begin{figure}[p]
  \begin{subfigure}[a]{1\textwidth}
    \center
    \includegraphics[scale=0.4]{images/nonlinear1.png}
    \caption{A nonlinearizable execution with the read access returning disagreeing values. We will see later (Figure \ref{fig:sequential}) that this execution is still sequentially consistent. }
    \label{fig:nonlinear1}
  \end{subfigure}
  \begin{subfigure}[b]{1\textwidth}
    \center
    \includegraphics[scale=0.4]{images/nonlinear2.png}
    \caption{Another nonlinearizable execution with read access values swapped. This execution is not sequentially consistent.}
    \label{fig:nonlinear2}
  \end{subfigure}
  \caption{Two non-linearizable executions of the same events shown in Figure \ref{fig:linearization}.}
  \label{fig:nonlinearizable}
\end{figure}

\begin{figure}
  \begin{subfigure}[a]{1\textwidth}
    \center
    \includegraphics[scale=0.4]{images/sequential1.png}
    \caption{A non-linearizable, sequentially consistent execution.}
    \label{fig:sequential1}
  \end{subfigure}
  \begin{subfigure}[b]{1\textwidth}
    \center
    \includegraphics[scale=0.4]{images/sequential2.png}
    \caption{An equivalent interleaving of \ref{fig:sequential1}.}
    \label{fig:interleaving1}
  \end{subfigure}
  \caption{A sequentially consistent execution and a possible interleaving.}
  \label{fig:sequential}
\end{figure}

\begin{figure}
  \begin{subfigure}[a]{1\textwidth}
    \center
    \includegraphics[scale=0.4]{images/nonsequential1.png}
    \caption{A non-sequentially consistent execution.}
    \label{fig:nonsequential1}
  \end{subfigure}
  \begin{subfigure}[b]{1\textwidth}
    \center
    \includegraphics[scale=0.4]{images/nonsequential_x.png}
    \caption{The sequentially consistent history of $x$.}
    \label{fig:sequentialx}
  \end{subfigure}
  \begin{subfigure}[b]{1\textwidth}
    \center
    \includegraphics[scale=0.4]{images/nonsequential_y.png}
    \caption{The sequentially consistent history of $y$.}
    \label{fig:sequentialy}
  \end{subfigure}
  \caption{A non-sequentially consistent execution with sequentially-consistent executions at each variable.}
  \label{fig:nonsequential}
\end{figure}

%%% Local Variables:
%%% mode: LaTeX
%%% TeX-master: t
%%% End:
