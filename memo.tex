% !TeX document-id = {beb7ced9-b3cd-42b2-b16a-3ed3c633a1d9}
\documentclass[]             % options: RDPonly, coveronly, nocover
{NASA}                       %   plus standard article class options
%\DeclareRobustCommand{\mmodels}{\mathrel{|}\joinrel\Relbar}

\usepackage[utf8]{inputenc}
\usepackage{csquotes}
\usepackage{setspace}
\usepackage{hyperref}
\usepackage{amsmath, amssymb, amscd, amsthm, amsfonts}
\usepackage{mathtools}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{amsthm}
\usepackage[english]{babel}
\usepackage{stmaryrd}
\usepackage{proof}
\usepackage{tikz-cd}
\tikzcdset{scale cd/.style={every label/.append style={scale=#1},
    cells={nodes={scale=#1}}}}
% Added for subfigures
\usepackage{caption}
\usepackage{subcaption}
\usepackage{afterpage}
\usepackage{comment}

\newtheorem{theorem}{Theorem}[section]
\newtheorem{corollary}{Corollary}[theorem]
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{example}{Example}
\theoremstyle{definition}
\newtheorem{definition}{Definition}[section]
\include{macros.tex}

% Globally redefine pgfpicture to use \Large fonts
\let\origpgfpicture=\pgfpicture
\def\pgfpicture{\origpgfpicture\small}

% Try loading this package to prevent so much hyphenation
% as recommended by https://stackoverflow.com/questions/1609837/latex-breaking-up-too-many-words
\usepackage{microtype}

\title{A Survey of Distributed Systems Challenges for Wildland
  Firefighting and Disaster Response}

\author{Lawrence Dunn and Alwyn E. Goodloe}

\AuthorAffiliation{Lawrence Dunn \\ Department of Computer and Information
  Science \\ University of Pennsylvania \\ Philadelphia, PA \\ Alwyn Goodloe\\                                          % for cover page
  NASA Langley Research Center, Hampton, Virginia
}
\NasaCenter{Langley Research Center\\Hampton, Virginia 23681-2199}
\Type{TM}                    % TM, TP, CR, CP, SP, TT
\SubjectCategory{64}         % two digit number
\LNumber{XXXXX}              % Langley L-number
\Number{XXXXXX}              % Report number
\Month{12}                   % two digit number
\Year{2022}                  % four digit number
\SubjectTerms{Distributed Systems, Formal Methods, Logic, }     % 4-5 comma separated words
\Pages{46}                   % all the pages from the front to back covers
\DatesCovered{}              % 10/2000--9/2002
\ContractNumber{}            % NAS1-12345
\GrantNumber{}               % NAG1-1234
\ProgramElementNumber{}
\ProjectNumber{}             % NCC1-123
\TaskNumber{}                % Task 123
\WorkUnitNumber{}            % 123-45-67-89
\SupplementaryNotes{}
\Acknowledgment{The work was conducted during a summer internship at the NASA Langley Research Center in the Safety-Critical Avionics Systems Branch focusing on distributed computing  issues arising in the Safety Demonstrator challenge in the NASA Aeronautics System Wide Safety (SWS) program.}

%Added for Pandoc
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}


\abstract{The System Wide Safety (SWS) program has been investigating
  how crewed and uncrewed aircraft can safely operate in shared
  airspace. Enforcing safety requirements for distributed agents
  requires coordination by passing messages over a communication
  network. Unfortunately, the operational environment will not admit
  reliable high-bandwidth communication between all agents,
  introducing theoretical and practical obstructions to global
  consistency that make it more difficult to maintain safety-related
  invariants. Taking disaster response scenarios, particularly
  wildfire suppression, as a motivating use case, this self-contained
  memo discusses some of the distributed systems challenges involved
  in system-wide safety through a pragmatic lens. We survey topics
  ranging from consistency models and network architectures to data
  replication and data fusion, in each case focusing on the practical
  relevance of topics in the literature to the sorts of scenarios and
  challenges we expect from our use case.  }

\begin{document}
\newpage
\setcounter{tocdepth}{2}
\tableofcontents
\newpage

\section{Introduction}
\label{introduction}
Civil aviation has traditionally focused primarily on the efficient
and safe transportation of people and goods via the airspace. Despite
the inherent risks, the application of sound engineering practices and
conservative operating procedures has made flying the safest mode of
transport today. Now the desire not to compromise this safety makes it
difficult to integrate unmanned vehicles into the airspace, accomodate
emerging applications, and keep pace with unprecedented recent growth
in commercial aviation. To that end, the System Wide Safety (SWS)
project of the NASA Aeronautics' Airspace Operations and Safety
Program (AOSP) has been investigating technologies and methods by
which crewed and uncrewed aircraft may safely operate in shared
airspace.

This memo surveys topics in computing that are relevant to maintaining
system-wide safety across large, physically distributed data and
communication systems. It aims to be self-contained and accessible to
a technical audience without a deep background in distributed
systems. Our primary motivating use cases have been taken from civil
emergency response scenarios, especially wildfire suppression and
hurricane relief, primarily for three reasons. First, improved
technology for wildfire suppression, especially related to
communications and data sharing, is frequently cited as a national
priority \cite{pcast2023}.  Second, the rules for operating in the US
national airspace are typically relaxed during natural disasters and
relief efforts, so this is a suitable setting for testing new
technologies. Finally, this setting is an excellent microcosm for the
sorts of general challenges faced by other, non-emergency
applications.

If there is a central theme uniting the sections of this manuscript,
it is \emph{continuity} in the sense considered by
topology.\footnote{For a typical introductory textbook see
  \cite{mendelson2012introduction}.} The systems we consider will be
subject to harsh operating conditions that limit how well they can
perform---for example, wireless communication is typically less
reliable during inclement weather. To build a system that is
predictable (clearly a prerequisite for safety), one must ensure the
system is flexible enough to perform reasonably well under a wide
variety of adverse conditions. In other words, the behavior of a safe
system should in some sense be a \emph{continuous} function of its
inputs and environment. This sort of robust design is particularly
challenging because distributed systems designers are forced to make
delicate tradeoffs between competing objectives, most famously between
performance and consistency, the topic of Section
\ref{sec:background}.

\subsection{Summaries of the sections}
\label{summaries-of-the-sections}

Sections \ref{sec:disaster-response}--\ref{sec:desiderata} contain
background material on disaster response, distributed systems, and the
specifics of our use case. The critical takeaway of these sections is
that system-wide safety is, at least in part, a computer science
problem, indeed a software problem, and not ``just'' a matter of
engineering better hardware. Sections
\ref{sec:networking}--\ref{sec:data-fusion} survey particular topics
from the distributed systems literature, proceeding from lower-level
considerations to higher-level ones; these sections may be read
independently of each other. Below we summarize each section.

Section \ref{sec:disaster-response} starts with a pragmatic summary of
disaster response and some of the relevant computing challenges in
that setting. We aim to justify and explain the role of distributed
systems theory in system-wide safety by citing real examples
encountered in disaster response scenarios.

Section \ref{sec:background} is an introduction to distributed
systems, culminating in a illustrative result: the ``CAP'' theorem(s)
for the atomic and sequential consistency models (Theorems
\ref{thm:cap} and \ref{thm:cap-sequential}, respectively). CAP is
considered a ``negative'' result, meaning it proves something cannot
be done. The CAP theorem proves that strong consistency for a
distributed system makes systemwide network performance an upper bound
on the availability of a system to do useful work for clients, which
for our purposes is an unacceptable restriction. The practical
significance of CAP is that in emergency response environments, agents
will always act with incomplete information about the global system, a
key motivation for Section \ref{sec:continuous-consistency}.

Section \ref{sec:desiderata} refines our assumptions and identifies
desirable properties of systems for our use case. We use these points to
frame the discussion of systems and protocols in subsequent sections.

Section \ref{sec:networking} examines networking considerations. Our
vision of future emergency communication networks integrates concepts
from delay/disruption-tolerant networks (DTN) and mobile ad-hoc
networks (MANET) to provide digital communications that are robust to
a turbulent operational environment. We also examine the state of
software-defined networking (SDN). SDN is an emerging field that puts
networking protocols on the same footing as ordinary computer
programs. In theory, this should furnish computer networking with all
the benefits of modern software engineering, such as reprogrammable
hardware, rapid iteration, version control, and especially formal
verification.

Section \ref{sec:continuous-consistency} describes a hypothetical
application that might be used in a disruption-heavy network: a data
replication service built on Yu and Vahdat's theory of ``conits"
(short for ``consistency unit'') \cite{2002tact}. This framework
realizes a \emph{continuous} consistency model in the sense that, as
typically configured, it provides neither strong consistency nor
guaranteed high-availability, but rather a quantifiable and
controllable tradeoff between the two. The key idea is that many
applications can tolerate inconsistency among replicas of a data item
if an upper bound on the divergence between replicas is enforced. A
conit-based database replication framework would allow system
designers to define units of replicated data whose consistency is of
interest, enforce policies bounding inconsistency between replicas of
these items, and even dynamically tune these policies on the fly. We
believe that only a conit-based replication infrastructure can provide
the strict guarantees required for safety-related systems while also
tolerating the adverse environments and real-world limitations of the
systems we have in mind.

Section \ref{sec:data-fusion} concerns data fusion. Now and in the
future, agents in disaster scenarios will make decisions informed by
many different kinds of information. Efficient integration,
processing, filtering, and dissemination of this information will be
necessary to avoid ``swimming in sensors and drowning in data''
\cite{2010:magnuson}.  This task is especially challenging because
agents will often work with incomplete or out of date information, and
different sources of the same data may be contradictory, e.g. first
responders may receive contradictory reports about whether a structure
is occupied. One promising trend in this space, which we briefly
introduce in this section, is the development of sheaf theory as a
natural mathematical model for data fusion
\cite{2017robinsonCanonical}. Sheaf theory provides a rigorous
framework for discussing how heterogeneous sources of noisy data can
be integrated into a coherent picture, and can formally measure how
well this task has been achieved.

We conclude in Section \ref{sec:conclusion} by recapping some of the
main themes in this document and highlighting areas where design
decisions at various levels must be made to build a system that is
tuned to the exact conditions we can expect from real-world
scenarios. Such decisions might be informed by a combination of
simulation and experimentation in the field.

\section{Coordination Challenges in Disaster Response}
\label{sec:disaster-response}
This section explains aspects of disaster response, particularly
firefighting, that motivate the remainder of this document. We
describe how real-world environments give rise to foundational
challenges that must be addressed through the application of
distributed computing principles. Even using the best communications
equipment cannot avoid the fundamental computer scientific problems
raised when distributed agents must coordinate their actions over a
widespread area.

The operational environment of wildfire suppression, hurricane relief,
and other disaster settings is generally characterized by systemic
communications challenges. A 2023 report on wildland firefighting
modernization by the President’s Council of Advisors on Science and
Technology (PCAST) cites ``the vulnerabilities and shortfalls in
wildland firefighter communications, connectivity, and technology
interoperability'' in their first of five recommendations
\cite{pcast2023}. These shortfalls can be partly attributed to factors
that are simply inherent to disaster response: remote locations,
difficult terrain, damaged infrastructure, harsh weather, and limited
battery power, to name a few.

Agents in the field generally experience high packet loss, garbled
transmissions, and unpredictable latencies when passing information
over the communication network(s). A conservative view suggests
expecting the worst performance at the most inopportune times, because
the conditions that prompt urgent communication tend to correlate with
those that make communication difficult. It could be that the disaster
itself damages the communications infrastructure. A more basic
observation---essentially a tautology---is that a network is the most
congested, and therefore the least available, precisely when everyone
needs to use it. Both of these phenomena were famously exhibited in
the immediate aftermath of the September $11^\textrm{th}$ terrorist
attacks, when sudden user demand and severed trunk cables brought New
York public cell phone networks virtually to a halt
\cite{2011:Reardon}. Other networks along the East Coast, including
dedicated networks for first responders, experienced similar
effects. The communication failures of 9/11 eventually became the
impetus for the creation of a nationwide public safety broadband
network, FirstNet \cite{2021:firstnet, 2021:firstnet2}.

From a systems perspective, an unreliable network presents a challenge
for coordinating distributed agents. Coherent decision-making and
coordinated action require consistency, i.e.~agreement, among data
shared between agents. We shall make this somewhat vague notion more
precise in Section \ref{sec:background}, but the intuition is clear:
it is very important for everyone to agree which firetrucks should
respond to which scenes, which tasks should be prioritized, or which
radios and radio frequencies have been reserved for whom. A general
observation is that stronger standards for consistency are more
difficult to maintain than relaxed ones because they require
exchanging more information in less time, putting a heavier burden on
the network. When a communications link is slow, system components
that need to coordinate may have to pause and wait, diminishing the
efficacy of the system. When this delay is unacceptable, the most
realistic alternative is to relax the standards for coordination. In
the next section we demonstrate that such a tradeoff has implications
for system safety.

\subsection{Communication and Safety}
\label{communication-and-safety}
Operational safety requires agents to gather information about their
environment and react to it quickly and systematically. This
information is often transmitted over networks, so a poor
communications environment is a safety problem. Poor communication
forces agents to choose between delays in sending and receiving
information or acting with only limited knowledge, but note that
inaction and uninformed action are both problematic for their own
reasons. This turns out to be related to a computer science phenomenon
generally known as the safety/liveness tradeoff.

As a running example, we consider the use of firefighting airtankers,
the largest of which are the Very Large Airtankers (VLATs), defined as
those carrying more than 8,000 gallons of water or fire retardant
\cite{2019:airtankerops}. The largest VLATs can deposit more than
20,000 gallons, weighing about 170,000 pounds, in a single ``drop.''
In the United States today, a typical policy is to perform drops from
a mere 250 feet above the tree canopy \cite{2019:airtankerops}, though
the complexity of the maneuver means errant drops are sometimes
performed even lower than this, easily crushing a ground vehicle
\cite{2019:stickney}. A 2018 accident led to the death of one
firefighter and the injury of three others when an 87-foot Douglas Fir
tree was knocked down by an unexpectedly forceful drop from a Boeing
747-400 Supertanker \cite{2018:calfire}.

\begin{figure}[h]
  \label{fig:airtanker}
  \centering
  \includegraphics[scale=0.4]{images/dc10.jpg}
  \caption{A DC-10 airtanker, rated for 9,400 gallons, drops retardant above Greer, Arizona. \citationneeded.}
\end{figure}
% https://www.flickr.com/photos/apachesitgreavesnf/5837741382
% Also appears at https://www.nifc.gov/resources/aircraft/airtankers

Suppose that in the future, firefighters are equipped with GPS sensors
and digital transmitters that are integrated into an environment-wide
communication system. A reasonable policy would be to prohibit VLATs
from performing a drop if its computers do not have up-to-date
information about the location of firefighters on the
ground. Unfortunately, this information may be difficult or impossible
to share: perhaps heavy smoke, a damaged radio tower, or a tall ridge
prevents communications between the air and ground. In these
scenarios, rigid enforcement of the safety policy would prevent
airtankers from operating.

This scenario exemplifies a classic tradeoff between opposing goals:
system \emph{safety} and system \emph{availability} (or
\emph{liveness}), elaborated on in Section \ref{sec:background}. In
the distributed computing context, safety properties guarantee that a
system will not perform an action that violates a constraint. An
exemplary safety property could look like the following:
\begin{quote}
  \interlinepenalty=10000 % Punish pagebreaks inside this quote!
  $\textbf{P}_\textrm{safe}$: Ground agents are known to be at least
  100 feet outside the drop zone, and this information is current to
  within 30 seconds, or airtankers will not perform a drop.
\end{quote}
By contrast, liveness properties stipulate that the system will
certainly perform requested actions, typically within some time
bound. An exemplary liveness property might be the following:
\begin{quote}
  $\textbf{P}_\textrm{live}$: A VLAT on the ground will takeoff and
  perform a drop within 20 minutes \footnote{In an interview with
  PBS, the Chief of Flight Operations for Cal Fire cited 20
  \mbox{minutes} as the response time for aerial firefighting units
  within designated responsibility areas
  \cite{2021:aerialfirefighting}} of receiving a request from an
  incident commander.
\end{quote}

Safety and liveness are frequently dual mandates: safety, in the sense
used here, requires a system \textbf{never} to perform certain
actions, while liveness requires a system to \textbf{always} perform
certain actions. The tension between these ideals means the two often
cannot be guaranteed simultaneously. Such is the case in our example:
if firefighters are unable to broadcast their locations to the pilot,
then the pilot's actions are impeded to maintain
\(\textbf{P}_\textrm{safe}\) at the cost of
\(\textbf{P}_\textrm{live}\), allowing the fire to spread in the
meantime.\footnote{A slight linguistic idiosyncrasy exhibited here is
that liveness properties---not just ``safety'' properties---can also
be relevant to human safety. Thus, the narrow technical meaning of
safety properties for distributed systems does not capture the whole
meaning of System Wide Safety.}

Besides the safety/liveness tradeoff, the previous example exhibits
two other aspects of reasoning about distributed systems. We pause to
draw attention to them.

\paragraph{Epistemology}
Observe that the issue in the VLAT example does not simply disappear
if no ground personnel are actually within 100 feet of a drop
zone. That is, it is not simply a matter of whether a danger is
factually present. To guarantee \(\textbf{P}_\textrm{safe}\), an
airtanker's actions must be restricted when its computers do not
\emph{know} whether an action would violate
\(\textbf{P}_\textrm{safe}\)---knowledge of the fact, and not merely
the fact of it, is the crucial part. In philosophical terms, the logic
of distributed agents is inherently an \emph{epistemic} one, meaning
it must take into account not just what is true but what is known. The
need to share knowledge is what drives communication and puts a burden
on the network.

\paragraph{Discontinuity}
The properties $\mathbf{P}_\textrm{safe}$ and
$\mathbf{P}_\textrm{live}$ are inflexible, all-or-nothing
propositions. The complexity of the operational environment demands
considering more flexible kinds of properties. Suppose that agents are
known to be $500$ feet outside the drop zone, the extra margin meaning
they are well away from any danger, but the information is only
current to within 35 seconds. Clearly this is good enough information
to authorize a drop, but strictly speaking the 5-second difference is
a violation of $\mathbf{P}_\textrm{safe}$. When safety properties are
this rigid, the system's behavior becomes overly sensitive to the
particulars of the environment and therefore difficult to predict,
which is precisely the kind of \emph{discontinuity} that we aim to
prevent. Our goal is to build reliable systems that perform well in a
wide range of circumstances.

\subsection{Communication Patterns in the Field}
\label{communication-in-practice}

We now consider some of the communication patterns that occur in
wildland firefighting. The layman reader may be surprised to learn
that the state of the art is somewhat primitive, which is partly
attributable to the fact that very little permanent infrastructure
exists in this setting. This fact is also what makes wildfires an
interesting and generalizable example for other kinds of civil
disaster environments.

One trend we will draw attention to is a kind of ``geospatial locality
of reference'' principle that system designers should take into
account. By this, we mean the happy coincidence of two observations
which, if not exactly guaranteed rules, are at least approximately
true in many circumstances. The first observation is simple:
\begin{quote}
  $\textbf{O}_1$: Agents with the most urgent need to
  coordinate their actions will tend to be located closer to each
  other and require the same kinds of information.
\end{quote}
The second observation is as follows:
\begin{quote}
  $\textbf{O}_2$: Agents that are located closer together
  will tend to have more reliable communications between them than
  agents that are far apart. Conversely, information that must travel
  a long distance tends to be delayed or degrade in quality.
\end{quote}

We will refer to the concomitance of these two facts as simply the
``locality'' principle. The reason the locality principle is crucial
is that, as we see in Section \ref{sec:background}, there are major
theoretical and practical limits to how well \emph{all} agents in the
system can share \emph{all} information with each other, i.e. how well
a system can achieve global consistency. As luck would have it, in
many cases this will not be required: it will be often be enough for
\emph{some} agents to share \emph{some} information with each other, a
fact that raises opportunities to optimize scare network resources. Of
course, this does raise the question of how to decide which
information must be shared with whom, and how to use this knowledge to
best exploit the communication network. We will revisit this question,
without the pretense of answering it conclusively, throughout Sections
\ref{sec:networking}, \ref{sec:continuous-consistency} and
\ref{sec:data-fusion}. For now, we resume our examination of what
communication patterns look like today.

\paragraph{Communication on the ground}
In the field, communication between firefighters and other agents is
often facilitated by handheld (analog) land-mobile radios, which are
inherently limited in their battery life, bandwidth, effective range,
and ability to work around environmental factors like foliage and
smoke.

As an alternative to using a radio, it is common for wildland
firefighters in the field simply to shout commands and notifications
to nearby personnel. This is a clear manifestation of the locality
principle: a substantial amount of communication occurs directly
between nearby firefighters working on the same or closely related
tasks, and in some cases they are so nearby they can communicate
without any network infrastructure at all. In a future environment
where agents might be equipped with body-worn sensors and heads-up
displays (HUDs) \citationneeded, this sort of local communication
might be facilitated by simple low-power technologies such as
Bluetooth, without the need for more sophisticated (and heavy)
equipment.

Communication over a long distance requires infrastructural support,
such as the use of cell towers and repeater stations. Typically,
disaster response environments have scarce permanent infrastructure:
in a wildland fire setting, perhaps a few repeaters mounted to a
nearby watch tower. Ad-hoc infrastructure, such as a cell on wheels
(COW), can sometimes be deployed on an as-needed basis if the location
allows for it. An extremely common issue is making sure that all
equipment is properly configured, for instance that all radios are
listening on the correct frequencies, particularly when different
agencies and groups need to interoperate (another problem highlighted
during the September $11^\textrm{th}$ attacks).

\begin{figure}[t]
  \centering
  \includegraphics[scale=0.085]{images/ironside.jpg}
  \caption{The Ironside Mountain lookout and radio repeater station, destroyed in the 2021
    Monument fire, shown with protective foil on August
    $10^\textrm{th}$, 2015 during the 2015 River Complex fire. This
    particular fire burned 77,077 acres over 77 days. \citationneeded}
  \label{fig:ironside}
\end{figure}
%https://web.archive.org/web/20150923190323/http://inciweb.nwcg.gov/incident/photograph/4431/44/45122/

Use of centralized infrastructure comes with the potential for
widespread failure when the infrastructure breaks down. For example,
in California, the Ironside Mountain lookout/repeater station, seen in
Figure \ref{fig:ironside}, was destroyed during the 2021 Monument
Fire, which burned approximately 223,124 acres over 88 days
\cite{2021:monumentfire}. The Ironside Mountain station had strategic
importance, being located on a tall ridge. According to a video blog
from a volunteer firefighter involved in the incident \cite{2022:mechfire},
% See also https://web.archive.org/web/20220809061927/https://www.youtube.com/watch?v=4F2dDKMgAME
its loss prevented communication between operators on different sides
of the ridge, in networking parlance creating a \emph{partition} that
lasted until crews could ascend the ridge to deploy a temporary
station:
\begin{quote}
  ``When {[}the Ironside Mountain lookout station{]} burned down the
  radio repeater went with it. And so communications were lost across
  the fire\ldots{} one side of the fire couldn't talk to the other
  side\ldots.  So it was kind of a critical job to get that road
  cleared so that the radio crews could go back up there and set up a
  temporary radio tower.''
\end{quote}
A scenario where communication between two groups is completely
severed is exactly the sort of thing considered by the CAP theorem in
Section \ref{sec:background}.

\paragraph{Vehicles on the ground}
Large numbers of ground vehicles are involved in wildfire
suppression. A large wildfire response can involve up to 100
firetrucks distributed over a large geographical area. Bulldozers and
similar vehicles are also commonly used to control the landscape and
perimeter of the fire. An advantage of vehicles is that they can carry
heavier, which is to say better, communications equipment than a
human. For instance, a vehicle could be equipped with a satellite link
as well as a local wireless area network (WLAN) base station, serving
as a bridge between agents in the field and central coordinators
(e.g. incident commanders).

\paragraph{Communication in the air}
Wildland firefighting increasingly involves the use of helicopters and
fixed wing aircraft. Civil aviation has traditionally employed simpler
communication patterns than this use case demands. For instance,
aircraft equipped with Automatic Dependent Surveillance-Broadcast
(ADS-B) monitor their location using GPS and periodically broadcast
this information to air traffic controllers and nearby aircraft. This
sort of scheme has worked well in traditional applications, where
pilots typically only monitor the general locations of a few nearby
aircraft. The locality principle is exhibited here, too: aircraft have
the highest need to coordinate when they are physically close and
therefore in range of each other's ADS-B broadcasts.

In our setting, a large number or aircraft, easily a half dozen or
more, may need to operate in a small area, near complex terrain,
during adverse conditions, often at low altitude. In other words, the
demands are many and the margins for error are small. This sort of use
case calls for more sophisticated coordination schemes between
airborne and ground-based elements than solutions like ADS-B provide
by themselves.

\paragraph{Message relaying}
As aircraft generally have better line-of-site to ground crews than
ground crews have to each other, firefighters sometimes relay messages
to air-based units over the radio, which in turn is relayed back down
to other ground units. The locality principle comes into play for this
sort of message relaying scheme, but in the negative direction:
relaying allows knowledge to travel farther but requires more effort,
and the extended reach comes at the cost of introducing delays and
possible degradation of message quality, as in the classic game of
telephone. Hence, this mode of communication should be reserved for
more critical information.

The Communications Program of the Civil Air Patrol (a civilian
auxiliary of the U.S. Air Force) is sometimes deployed to provide
communications for firefighters on the ground using airplane-mounted
radio repeaters. Air-based repeaters are better than scheme in the
previous paragraph as they do not require a human to receive and then
manually re-transmit information. That is, this form of relaying is
\emph{transparent}. In this future, this sort of service could be
provided autonomously by base stations mounted to unmanned aerial
vehicles (UAVs), which might perform additional functions such as
tracking the fire perimeter.

In Section \ref{sec:networking} we imagine a resilient ad-hoc digital
network built from handheld and ground- and air-vehicle-mounted
devices, permanent base stations, portable temporary infrastructure,
and so on. In effect, this would be a high-tech modernization of the
sort of informal relay schemes operating today over traditional radio
channels.

%More generally, future systems should transparently facilitate
%exchanging information between agents in a decentralized fashion that
%is robust to the failure of any one component.

\subsection{Towards the Future}
\label{towards-the-future}
So far we have said a lot about the state of disaster response
today. A distributed system for this sort of challenge should be
designed for the kinds of environments and conditions expected in the
near- to medium-term future, so we briefly turn our attention to some
of our expectations for this topic.

Perhaps the most prominent expectation for future disaster response
events is a heavy reliance on \emph{data}. Besides improvements to
communications that facilitate information sharing, we expect advances
in machine intelligence to greatly influence how this data is
handled. Agents in disaster response environments will be both
producers and consumers of data, and this data will need to processed
by humans and machines in ways that agents can readily make sense of
to support their decision-making. Our background research indicated
many different kinds of data that could be valuable for
responders. Just some of the kinds of information and communication we
expect include the following:
\begin{itemize}
  \tightlist
\item
  Free-form communication, especially recorded voice messages
  broadcast to many agents at once, which may need to be processed by
  machines to extract the most pertinent information into a more
  actionable format
\item
  The exact or estimated location of victims, firefighters, vehicles,
  hazards, etc.
\item
  Medical information gathered from victims, perhaps stored in and
  collected from digital triage tags\citationneeded
\item
  Data about current and predicted fire or weather patterns
\item
  Topographic information about the terrain, highlighting for instance
  the location of rivers and roads that could form a fire control line
\item
  Planned escape routes, rendezvous points, safety zones, and landing
  zones
\item
  Availability and dispatching of assets, e.g.~ambulances, airtankers,
  or crews on standby\footnote{NOTE Need to mention Monares et al 2011}
\end{itemize}
In a perfect environment, such information would be shared with all
necessary agents in whole and instantly. In reality, agents will be
presented with information that is sometimes incomplete, out of date,
or contradictory---all problems that are further exacerbated by an
unreliable network. A competing concern is that the information
presented will be \emph{overcomplete}, filled with petty details that
distract agents from their important tasks.

In some ways, future systems for disaster response will bear
resemblence to future systems for warfighting, such as the conceptual
\emph{Internet of Battle Things} (IoBT) \cite{2016:iobt}. Chiefly,
agents ``under extreme cognitive and physical stress'' will be subject
to a highly dynamic and dangerous environment. Various kinds of
technology will assist humans by providing data to support
sensemaking, but a contraindicating concern will be flooding agents with a
``massive, complex, confusing, and potentially
deceptive\footnote{While deliberately adversarial network behavior
seems like less of a concern for disaster response agents than
warfighters, we conjecture that a similar ``fog of war'' may
lead to confusing or contradictory reports that share similarities
with intentionally deceptive behavior.} ocean of information.'' To
avoid ``swimming in sensors and drowning in data''
\cite{2010:magnuson}:
\begin{quote}
``Humans seek well-formed, reasonably-sized, essential information
  that is highly relevant to their cognitive needs, such as effective
  indications and warnings that pertain to their current situation and
  mission.'' \cite{2016:iobt}
\end{quote}
The most distinctive feature of the Internet of Battle Things, which
separates it from the everyday internet of things, is ``the
adversarial nature of the environment.'' To some extent this adversial
behavior is common also to disaster response. We previously cited a
real-world example of a critical communications station destroyed by
wildfire, perhaps not unlike an attack by enemy forces.

Lest we overstate the similarities between a battlefield environment
and civil disaster response, a distinctive feature of the latter is a
greater emphasis on the preservation of scarce network resources. More
so than a tactical military unit, a group of (say) volunteer
firefighters has to make do with off-the-shelf equipment rather than
purpose-built, best in class hardware like sophisticated satellite
links. Dedicated logistical support, and even things like allocated
radio frequencies, will likely be in shorter supply, while adverse
conditions like inclement weather will be almost guaranteed. Therefore
we expect a complex interaction between the high-level needs of
distributed applications and low-level concerns about network
resources. This is because only the applications have enough
information to determine which data is the most important and must be
shared with whom first, while only the network-level protocols have
enough information and control to make prudent use of scarce network
availability. In contravention to the common wisdom that applications
should be relatively blind to network considerations---or conversely
that network protocols ought to be transparent to applications---our
setting calls for mechanisms allowing the two layers to have some
influence over each other. This interaction between the network and
applications will be considered in more detail in Sections
\ref{sec:networking}, \ref{sec:continuous-consistency}, and
\ref{sec:data-fusion}.

To give an example, a central data fusion center may be used to detect
and alert responders to a fire that has accidentally moved beyond a
control line (known as \emph{slopover}), or it may warn firefighters
when they have strayed too far from an escape route or safety
zone. Such information would be of high importance, and it would be
worthwhile to expend network resources conveying this information to
the relevant parties. It might also be nice for firefighters to have
access to real-time information about the GPS location of every other
firefighter. However, if this strains the network, then perhaps the
exact location of teammates, but only the general location of other
crews, is called for. If the network is extremely constrained, perhaps
only information immediately relevant to preserving life should be
sent in order to ensure the network is able to deliver this
information quickly. The key point is that from where we stand, we
cannot give a blanket rule determining which information is important,
as this is partly a dynamic calculation influenced both by the
criticality of the information to the task at hand and how much unused
network capacity is available at that moment at that location.

\section{Introduction to Distributed Systems}
\label{sec:background}
In this section we distill some core aspects of distributed systems
theory, following manuscripts by Coulouris et al.
\cite{coulouris2005distributed} and Kshemkalyani and Singhal
\cite{kshemkalyani_singhal_2008}. As in Section
\ref{sec:disaster-response}, our motivating examples of distributed
systems consist of agents, nodes, and sensors in disaster response
environments where infrastructure is deployed on an ad-hoc basis or
carried by persons or vehicles in the field.

In general, a distributed system is a collection of independent
entities that cooperate to solve a problem that cannot be individually
solved \cite{kshemkalyani_singhal_2008}. Singhal and Shivaratri
\cite{10.5555/562065} offer the following definition:
\begin{quote}
  ``A collection of computers that do not share common memory or a common
  physical clock, that communicate by message passing over a communication
  network, and where each computer has its own memory and runs its own
  operating system.''
\end{quote}

The ``distributed'' aspect of the system means its components can only
communicate by passing messages over a network.\footnote{Components of
  a non-distributed system, like processes on the same computer, can
  communicate by writing data to a shared memory location.} Messages
sent over the network experience a finite but unpredictable
\emph{latency} or delay before they are delivered. More precisely, a
system is distributed when network latencies are non-negligible
compared to the timescale of other events in the system. Per the
locality principles discussed in Section \ref{sec:disaster-response},
higher latencies are generally expected as components of the system
become more greatly separated from each other.

This section will explain how the unpredictability of the network
essentially implies that system components at any moment lack a shared
time reference (i.e. synchronized clocks). For the same underlying
reasons, components lack perfect knowledge about the global state of
the system. Of course, we the omniscient observer can see the global
state, but network latencies prohibit the instantaneous knowledge
transfer that would be required for all components to know it. We must
either design mechanisms that attempt to recover notions like shared
time and state, or applications that perform correctly without relying
on them. As any engineer would expect, the design space considers many
possible tradeoffs. In the context of data replication, a fundamental
task, a central theme is a general tradeoff between performance and
consistency.

We assume that nodes in our environments have only intermittent access
to the network. Indeed, groups of nodes may need to coordinate without
end-to-end connectivity, meaning without being on the same network (or
network partition) at the same time.\footnote{Networking without
  end-to-end connectivity requires the use of delay-tolerant
  networking (DTN) concepts, as will be discussed in Section
  \ref{sec:networking}.}  Consequently, we consider latencies on an
order ranging from seconds when communicating with nearby agents to
potentially hours or days for system-wide coordination.
Communications engineers and system designers at the hardware level
have limited control over these latencies, so we can only hope to
build systems that tolerate real conditions as optimally as possible
at the level of software.
% In a tactical environment, packet loss can be on the order of
% \%xx. \citationneeded

\subsection{Message Passing and Logical Time}
\label{ssec:message-passing}
One can model a distributed system as a set
\(\mathcal{P} = \{P_i\}_{i\in I}\) of \emph{processes} which undergo
atomic (indivisible) state changes known as \emph{events}. We divide
events into three types: internal events, representing state changes
inside a single process, and send and receive events for passing
messages between processes. For now we treat processes and the network
as opaque blackboxes, focusing on the ramifications of unpredictable
communication delays for coordination.  For simplicity's sake, we will
assume that all messages are actually delivered at some point. These
assumptions are properly known as the asychronous network model.

Figure \ref{fig:message-latencies} shows several time diagrams for
messages $\{m_i\}_{i=1}^N$ sent between three processes $P_1$, $P_2$,
and $P_3$. The $x$-axis of the diagram depicts the flow of real time
from left to right. For each process a worldline is shown depicting
the events occurring in that process. Each message $m$ starts off as a
send event $\msend{}$ indicating the moment the message is sent over
the network by its originating process; its delivery generates a
receive event $\mrecv{}$. The (send, receive) pairs are connected by
an arrow and said to be corresponding events. We write subscripts on
messages to disambiguate them to the reader, but note that these are
not part of the message itself.

\begin{figure}[p]
  \setlength\belowcaptionskip{5ex}
  \begin{subfigure}{1\textwidth}
    \centering
    \input{images/pgf/mpEx1.pgf}
    \caption{$P_1$ has a somewhat lower-latency connection to $P_2$ than to $P_3$}
    \label{fig:message-latencies-a}
  \end{subfigure}
  \begin{subfigure}{1\textwidth}
    \centering \input{images/pgf/mpEx2.pgf}
    \caption{$P_1$ has a much lower-latency connection to $P_2$ than to $P_3$}
    \label{fig:message-latencies-b}
  \end{subfigure}
  \caption{Message-passing time diagram examples}
  \label{fig:message-latencies}
\end{figure}

Message arrows in time diagrams are diagonal, rather than vertical, as
to represent the varying effect of network latencies on message
delivery. Because they arrive delayed, messages may be delivered in a
different order than they were sent in. In Figure
\ref{fig:message-latencies-a}, $P_1$ sends messages $m_2$ and $m_4$ in
that order, but $m_4$ arrives at its recipient before $m_2$. This
scenario might occur if $P_1$ and $P_3$ are far apart and have a
high-latency communication link. In Figure
\ref{fig:message-latencies-b}, $m_1$ is the first message sent but the
last message to be delivered. This may depict a scenario where the
link between $P_1$ and $P_3$ has deteriorated, perhaps because they
have grown farther apart or because of inclement weather.

\subsubsection{Causality and  Sychrony}
\label{ssec:causality}
A fundamental challenge is to order events globally in a way that
individual components can calculate and agree upon. What makes this
task challenging is that the components do not share a common time
base. That is, participants in the system do not have instantaneous
access to a globally shared clock, nor are they equipped with
perfectly synchronized local clocks, so any two nodes do not
necessarily agree on physical time. We briefly elaborate on the
challenge of producing physically sychronized clocks before discussing
conventional ways around the problem for our purposes.

Physical clocks, especially consumer-grade ones, suffer from
\emph{drift}, which is to say they do not all run at the same
rate. Experienced system administrators will testify that clocks can
also be quite prone to misconfiguration, say if the date, time,
timezone, or daylight saving time policy (etc.) is set
incorrectly. Our devices may spend a long time sitting unpowered in
storage without maintaining an always-on clock. For these sorts of
reasons, physical clocks are not very reliable out-of-the-box. We
would not want to rest the integrity of a safety-related system on the
assumption that a numerous and diverse assortment of devices have
precisely synchronized clocks.

Clock drift can be corrected for using, for instance, signals from GPS
satellites, but for example in the wildfire setting GPS signals are
not reliably available. Protocols like the Network Time Protocol (NTP)
\cite{rfc1119} try to bring local clocks into synchronization with
respect to authoritative clocks, typically to within 100ms even over a
somewhat challenged internet connection, but this may not extend to
highly chaotic networking environments such as ours. For such reasons
we do not assume typical corrective measures will be able to reliably
synchronize the clocks in our system with great precision.

Fortunately, what seems most important about time for many purposes is
that \emph{the future cannot influence the past} \cite{1989mattern}, and
this is the sort of invariant we can enforce with appropriate
measures. The key relation between events we must respect is their
\emph{causal precedence}, also called Lamport's ``happens before''
relation \cite{1978:lamportclocks}. We begin with some definitions.

\begin{definition}
  For two events $e$ and $e'$ that both occur in process $P_i$, we
  write $e <_{P_i} e'$ if $e$ occurs before $e'$ in $P_i$'s
  worldline.
\end{definition}
The previous definition is ``local'' in that it only relates events
within the same process. It is unambiguous because we assume events at
one process occur in discrete, non-overlapping moments. The next
definition describes an order among all events.

\begin{definition}[Causal precedence]
  \label{def:causalprecedence}
  We define a binary relation $\to$ on the set of events as follows:
  \[e \to e' \iff
  \begin{cases}
    e <_{P_i} e' \textrm{ for some process $P_i$}
    \textbf{ or} \\
    e = \msend{} \textrm{ and } e' =\mrecv{}
    \textbf{ or} \\
    \textrm{there is some } e'' \textrm{ such that } e \to e'' \textrm{ and } e'' \to e'
  \end{cases}
  \]
  If $e \to e'$, we say $e$ has \emph{causal precedence over} $e'$ or
  \emph{happens before} $e'$.
\end{definition}

In words, there are three ways $e$ could have causal precedence over
$e'$.  First it may be that the $e$ and $e'$ occur in the same process
and $e$ physically precedes $e'$. It may also be that $e$ is a
send event and $e'$ is the corresponding receive event at another
process. Finally, it could be that $e$ has precedence over some
intermediate event $e''$ with precedence over $e'$, i.e. we take
the transitive closure of the other conditions. Visually, $e \to e'$
holds whenever one can put a finger on $e$ in the time diagram and
trace a ``path of causality'' to $e'$ by following worldlines or
arrows.

Figure \ref{fig:causal-precedence} illustrates the causal precedence
relation corresponding to the time diagrams in Figure
\ref{fig:message-latencies}. For readability we just depict arrows
between (send, receive) pairs or adjacent events in each process,
hiding redundant transitive arrows. By comparison to Figure
\ref{fig:message-latencies}, we note that Figure
\ref{fig:causal-precedence} only suggests a logical relation among
events, but not their absolute time or where they occurred.

Mathematically, causal precedence is an irreflexive partial
order. Events $e$ and $e'$ that satisfy neither $e \to e'$ nor
$e' \to e$ are said to be \emph{logically synchronous}, denoted
$\sync{e}{e'}$. The reader is warned that logical synchronicity is not
a transitive relation: it is possible to have $\sync{e}{e'}$ and
$\sync{e'}{e''}$ but not $\sync{e}{e''}$. For instance, in Figure
\ref{fig:message-co-a}, $\mrecv{2}$ is synchronous with both
$\mrecv{1}$ and $\msend{4}$, but $\mrecv{1} \to \msend{4}$. In Figure
\ref{fig:message-co-b}, $\msend{1}$ is logically synchronous with
every event except $\mrecv{1}$, but those other events are totally
ordered by causality and not synchronous with each other. Relations
like $\sync{}{}$ that are reflexive and symmetric but not necessarily
transitive are sometimes called \emph{compatibility relations}.

Incidentally, ``causal precedence'' and ``happens before'' are
misnomers: $e \to e'$ does not mean caused $e'$ in any philosophical
sense, and conversely $e$ might occur before $e'$ in physical time
without having $e \to e'$. Intuitively, $e \to e'$ conveys merely the
possibility that information from $e$ influenced $e'$. The goal is
then to avoid systems that might act as if $e'$ takes effect before
$e$. This proscription ensures users never observe the system behaving
as if the future can influence the past.

\begin{figure}
  \begingroup
  \setlength\belowcaptionskip{4ex}
  \begin{subfigure}{1\textwidth}
    \centering
    \input{images/pgf/mpEx1CO.pgf}
    \caption{Causal precedence among the events in Figure \ref{fig:message-latencies-a}}
    \label{fig:message-co-a}
  \end{subfigure}
  \endgroup
  \begin{subfigure}{1\textwidth}
    \centering
    \input{images/pgf/mpEx2CO.pgf}
    \caption{Causal precedence among the events in Figure \ref{fig:message-latencies-b}}
    \label{fig:message-co-b}
  \end{subfigure}
  \caption{Causal precedence relations for Figure \ref{fig:message-latencies} (transitive arrows not shown)}
  \label{fig:causal-precedence}
\end{figure}

\subsubsection{Virtual Time}
\label{ssec:timestamps}
Distributed applications can systematically track causality by
employing a system of \emph{logical} clocks. Such ``clocks'' measure
the logical flow of time by storing non-negative integers that are
advanced according to certain rules. Three major variants are common:
scalar, vector, and matrix clocks. These fall on a kind of spectrum.
Scalar clocks are simple but provide only coarse-grained information,
while vector and matrix clocks track more precise information but
impose greater administrative overheads.

All processes timestamp their events using their local clocks. For
each event $e$, let $C(e)$ denote the timestamp attached to that
event. The fundamental property we want to satisfy is that if $e$
causally precedes $e'$, it should receive a lesser timestamp. This is
called the clock consistency condition, or just the clock condition.
\begin{definition}
  A system of timestamps satisfies the \emph{clock consistency
  condition} if the following monotonicity property holds:
  \[ \textrm{For all events $e$ and $e'$, } e \to e' \implies C(e) < C(e') \label{eq:mp}\tag{CC} \]
\end{definition}

Importantly, the clock consistency condition does \emph{not} imply
that we can decide if events are causally related by comparing
timestamps. The utility of \eqref{eq:mp} is best seen by re-expressing
it in terms of the following logically equivalent condition.
\[ \textrm{For all events $e$ and $e'$, }C(e) \leq C(e') \implies e'
  \not\to e \label{eq:mp-conv} \] That is, we can be sure that some
ordering of events $e_1, e_2, e_3\ldots$ does \emph{not} violate the
causal precedence relation simply by verifying that
$C(e_{i}) \leq C(e_{i+1})$ for all $i$.

For many applications, we would like to unambiguously determine
whether two events are causally related by comparing timestamps. That
is, one would like to have the following stronger condition.
\begin{definition}
  A system of timestamps satisfies the \emph{strong} clock consistency
  condition if the following property holds.
  \[ \textrm{For all events $e$ and $e'$, } e \to e' \iff C(e) < C(e') \label{eq:sc}\tag{SC} \]
  Note that $\iff$ is notation for ``if and only if,'' i.e. logical equivalence.
\end{definition}
\ref{eq:sc} strengthens \ref{eq:mp} by stipulating that we can always
compare timestamps to infer whether events are causally related. A
system of scalar clocks, defined below, can enforce the weaker
condition but not the strong one.

\subsubsection{Scalar clocks}
\label{sssec:scalar-clocks}
\begin{figure}
  \setlength\belowcaptionskip{5ex}

  \begin{subfigure}{1\textwidth}
    \centering \input{images/pgf/mpEx1Sc.pgf}
    \caption{Figure \ref{fig:message-latencies-a} redepicted with scalar clocks}
    \label{fig:message-latencies-scalar-a}
  \end{subfigure}

  \begin{subfigure}{1\textwidth}
    \centering \input{images/pgf/mpEx2Sc.pgf}
    \caption{Figure \ref{fig:message-latencies-b} redepicted with scalar clocks}
    \label{fig:message-latencies-scalar-b}
  \end{subfigure}

  \caption{Scalar clock examples}
  \label{fig:message-latencies-scalar}
\end{figure}

Scalar clocks, introduced by Lamport \cite{1978:lamportclocks},
require each process $P_i$ to maintain a single scalar value $C_i$, a
non-negative integer initialized at $0$. There are two update rules:
\begin{enumerate}
\item[\textbf{R1}] At each process $P_i$, before a message is sent or
  before an internal event, $C_i$ is updated according to the rule
  \[C_i := C_i + 1.\] The new value is the timestamp attached to the
  event, and for messages the value is sent (``piggybacked'')
  alongside the message as metadata.
\item[\textbf{R2}] When $P_i$ receives a message timestamped with value $C$, it
  updates $C_i$ according to the rule
  \[C_i := \max(C, C_i) + 1.\]
  The updated value is the timestamp associated with the receive
  event.
\end{enumerate}
Figure \ref{fig:message-latencies-scalar} depicts the same events in
Figure \ref{fig:message-latencies} alongside the scalar timestamp,
shown in parentheses, that would be assigned to each event. For send
events, the piggybacked timestamp is shown as a label attached to the
arrow.

It is clear that scalar clocks satisfy the clock condition
\eqref{eq:mp}, which is observed by tracing the path of causality
between related events and seeing that the clock is incremented at
each step.
\begin{lemma}
  Scalar clocks satisfy $e \to e' \implies C(e) < C(e')$.
\end{lemma}

Scalar clocks do not satisfy the strong condition (\ref{eq:sc})
because knowing $e$ has a lesser timestamp rules out $e' \to e$ but
does not determine whether $e \to e'$. For example, in Figure
\ref{fig:message-latencies-scalar-b}, $\msend{1}$ has a globally
minimal timestamp value of $1$. However, we see clearly in the Figure
that it does not causally precede any event except $\mrecv{1}$.

\subsubsection{Vector clocks}
\label{sssec:vector-clocks}
\newcommand{\vt}{\textrm{vt}}

The strong clock condition \eqref{eq:sc} cannot hold either using
scalar clocks or even synchronized physical clocks. This is because
both mechanisms assign timestamps whose values form a total order, meaning any
non-equal timestamps $C_1, C_2$ have to satisfy either $C_1 < C_2$ or
$C_2 < C_1$. However, logically synchronous events $\sync{e}{e'}$ do not
satisfy either $e \to e'$ or $e' \to e$. Thus, the strong consistency
requirement would require us to set $C(e) = C(e')$ for all logically
synchronous events. This is impossible, as we already saw that logical
concurrency is not transitive. For example, recall in Figure
\ref{fig:message-co-b} that $\msend{1}$ is logically
synchronous with every event except $\mrecv{1}$, so they would
all have to receive the same timestamp as $\msend{1}$, which
violates the fact that they are not logically synchronous with each
other.

\begin{figure}
  \setlength\belowcaptionskip{5ex}

  \begin{subfigure}{1\textwidth}
    \centering
    \input{images/pgf/mpEx1Vec.pgf}
    \caption{Figure \ref{fig:message-latencies-a} redepicted with vector clocks}
    \label{fig:message-latencies-vector-a}
  \end{subfigure}

  \vspace{4ex}

  \begin{subfigure}{1\textwidth}
    \centering \input{images/pgf/mpEx2Vec.pgf}
    \caption{Figure \ref{fig:message-latencies-b} redepicted with vector clocks}
    \label{fig:message-latencies-vector-b}
  \end{subfigure}

  \caption{Vector clock examples}
  \label{fig:message-latencies-vector}
\end{figure}
\afterpage{\clearpage}

The way out of this impasse is to assign timestamps from a partial
order, meaning two non-equal timestamps may not satisfy $C_1 < C_2$ or
$C_1 > C_2$. Vector clocks achieve this by storing one scalar value
for each process in the system. For a set of $N$ processes, $P_i$
maintains a vector $\vt_i[1 \ldots N]$ of non-negative integers, with
all values initialized to $0$. As with scalar clocks there
are two update rules:
\begin{enumerate}
\item[\textbf{R1}] Before an internal event or a new message is sent by $P_i$,
  $\vt_i$ is updated according to the rule
  \[\vt_i[i] := \vt_i[i] + 1.\]
  For messages, the entire updated vector $\vt_i$ is piggybacked with
  the message.
\item[\textbf{R2}] When a message is received by $P_i$ with a
  piggybacked timestamp $\vt$, $\vt_i$ is updated according to
  \[\vt_i[x] := \max(\vt[x], \vt_i[x]) \quad \textrm{for all $x = 1\ldots N$}.\]
  After this, $P_i$ advances its own local time according to the rule
  \[ \vt_i[i] := \vt_i[i] + 1.\]
  This new vector is the timestamp attached to the receive event.
\end{enumerate}

These rules are perhaps more intuitively explained by demonstration.
Figure \ref{fig:message-latencies-vector} redepicts the diagrams in
Figure \ref{fig:message-latencies} showing the vector timestamp that
would be assigned to each event. The $i^\textrm{th}$ component of
$P_i$'s vector clock, or $\vt_i[i]$, is called the \emph{local time}
for $P_i$. For all other $j \neq i$, the $j^\textrm{th}$ component
$\vt_i[j]$ of $P_i$'s clock represents $P_i$'s \emph{estimate} of
$P_j$'s local time. This estimate is always a lower bound, since
$P_j$'s local time may have advanced in the meantime, but we never
advance our estimate ahead of $P_j$'s actual local time. By examing
timestamps sent to it by other processes, $P_i$ can learn that $P_j$'s
local clock has advanced, even without directly communicating with
$P_j$.

Vector timestamps are compared component-wise. This is of course a
partial order, as one vector may be greater than another in some
components and less in others.
\begin{definition}[Vector comparison]
  Let $v, w$ be two vector clocks. We define the following relations:
  \begin{align*}
             v = w &\iff \forall i, v[i] = w[i] \\
  v \preccurlyeq w &\iff \forall i, v[i] \leq w[i] \\
         v \prec w &\iff v \preccurlyeq w \textrm{ and } \exists i, v[i] < w[i] \\
            \sync{v}{w} &\iff \textrm{ neither } v \preccurlyeq w \textrm{ nor } w \preccurlyeq v
  \end{align*}
  That is, $v \prec w$ if all of $w$'s components are at least as
  great as $v$'s, and at least one of its components is strictly
  greater. When two non-equal vectors are compared, and neither is
  greater than the other, we write $\sync{v}{w}$ and say the vectors
  are \emph{incomparable}.
\end{definition}

Note we have slightly overloaded the notation $||$ to mean either that
two events are logically synchronous or that two vectors are
incomparable. This mild abuse is justified by the fact that vector
clocks satisfy \ref{eq:sc}, so these otherwise distinct notions will
coincide.
\begin{lemma}
  Vector clocks satisfy the strong clock consistency condition. That
  is, where $C(e)$ is the vector timestamp of an event, then
  \[ e \to e' \iff C(e) \prec C(e'). \]
  From this it follows that for non-equal events $e$ and $e'$ we have
  \[ \sync{e}{e'} \iff \sync{C(e)}{C(e')}. \]
\end{lemma}

For reasons of space we omit a proof of the preceding lemma.  The
direction $e \to e' \implies C(e) \prec C(e')$ is
straightforward. More tedious is the right-to-left direction
$C(e) \prec C(e') \implies e \to e'$, but the key idea is that if
$C(e) \prec C(e')$, then it is possible to follow a backwards chain of
send and receive events connecting $e$ to $e'$, hence $e \to e'$.

% \begin{proof}
%   The direction $e \to e' \implies C(e) \prec C(e')$ is
%   straightforward. Slightly more tedious is the right-to-left
%   direction $C(e) \prec C(e') \implies e \to e'$.

%   Assume two events satisfy $C(e) \prec C(e')$. If the events occur on
%   the same process $P_i$, then $C(e')$ must have a strictly greater
%   $i^\textrm{th}$ component, i.e. $C(e)[i] \prec C(e')[i]$, in which
%   case $e <_{P_i} e'$ and hence $e \to e'$.

%   The last case to consider is that $C(e) \prec C(e')$ with $e$ on
%   process $P_i$ and $e'$ on process $P_j$ with $j \neq i$. If
%   $e \to e'$ then we are done, so suppose $e \not \to e'$.
%   Without loss of generality, let $e'$ be the first event (in physical
%   time) satisfying both
%   \[ C(e) \prec C(e') \textrm{ and } e \not \to e'.\] We will derive a
%   contradiction. If $e$ is a message receive event, let $x$ be the
%   corresponding send event. $x$ cannot satisfy


%   . Let $e_p$ be the event immediately
%   preceding $e'$ on process $j$. By assumption,
%   $C(e) \not \prec C(e_p)$, as otherwise our we contradict our
%   assumption that $e_p$ is the first event satisfying both
%   $C(e) \prec C(e_p)$ and $e \not \to e_p$. However, $C(e_p)$ and
%   $C(e')$ can only differ in their $j^\textrm{th}$ component, as

%   .  If $e'$ is a message \emph{send} event, then


%   We can prove this
%   direction by its contrapositive:
%   \[ e \not\to e' \implies C(e) \not\prec C(e'). \]

%   Assume $e \not\to e'$ where event $e'$ occurs on process $P_i$.  If
%   $e'$ also occurs on process $i$, then because events on $P_i$ are
%   totally ordered with respect to $P_i$'s local time, it must be that
%   $e' <_{P_i} e$, i.e. $e'$ happened first. In this case,
%   $C(e')[i] < C(e)[i]$, hence $C(e) \not \prec C(e')$.

%   Suppose $e'$ occurs on $P_j$ where $j \neq i$. Without loss of
%   generality, consider the \emph{first} event $e'$ (in physical time)
%   with the property $e \not\to e'$. If $e'$ is a message send event,

%   . So let $e'$ occur on some other process.
%   \textbf{TODO}
% \end{proof}

\subsubsection{Matrix clocks}
\label{sssec:matrix-clocks}
If a vector clock stores an estimate of every other process's local
time, a matrix clock stores an estimate of every other process's
vector clock. We give the details below. Figure
\ref{fig:message-latencies-matrix} shows how our running examples
would use matrix clock timestamps.

Each process $P_i$ stores an $N\times{}N$ matrix $\vt_i$, initialized to
all zeros, with the following interpretation. The $i^{\textrm{th}}$
row from the top, $\vt_i[i, -]$, stores $P_i$'s vector time. All other
rows $\vt_i[j,-]$ store $P_i$'s estimate of $P_j$'s vector
time. Matrix timestamps are piggybacked with messages, and the
receiver uses the sender's vector clock to update their own vector
clock as usual, and takes the pointwise maximum of all other rows.

\begin{enumerate}
\item[\textbf{R1}] Before a new message is sent, $\vt_i[i]$ is updated according to the rule
  \[\vt_i[i,i] := \vt_i[i,i] + 1.\]
  The entire updated matrix $\vt_i$ is piggybacked with the message.
\item[\textbf{R2}] When a message is received from $P_j$ with a piggybacked timestamp $\vt$,
  $\vt_i$ is updated according to two cases
  \begin{enumerate}
  \item Update the row $\vt_i[i, -]$ according to
    \[\vt_i[i, k] := \max(\vt_i[i,k], \vt[j, k]) \quad \textrm{for all $k = 1\ldots N$}.\]
  \item Update all other rows $\vt_i[j', -]$ according to
    \[\vt_i[j', k] := \max(\vt_i[j',k], \vt[j', k]) \quad \textrm{for all $k = 1\ldots N$}.\]
  \end{enumerate}
  After this, $P_i$ advances its own local time according to the rule
  \[ \vt_i[i,i] := \vt_i[i,i] + 1.\]
  This new matrix is the timestamp attached to the receive event.
\end{enumerate}

\begin{figure}[p]
  \begingroup
  \setlength\belowcaptionskip{4ex}
  \begin{subfigure}{1\textwidth}
    \input{images/pgf/mpEx1Mat.pgf}%
    \caption{Matrix clock timestamps for the events in Figure \ref{fig:message-latencies-a}}
    \label{fig:message-latencies-matrix-a}
  \end{subfigure}
  \begin{subfigure}{1\textwidth}
    \input{images/pgf/mpEx2Mat.pgf}%
    \caption{Matrix clock timestamps for the events in Figure \ref{fig:message-latencies-b}}
    \label{fig:message-latencies-matrix-b}
  \end{subfigure}
  \caption{Figure \ref{fig:message-latencies} depicted with matrix clocks}
  \label{fig:message-latencies-matrix}
  \endgroup
\end{figure}

By comparing Figures \ref{fig:message-latencies-vector} and
\ref{fig:message-latencies-matrix}, see that $\vt_i[i,-]$ acts
precisely like a vector clock. All other rows in $\vt_i$ are lower
bound estimates of other process's vector clocks. In Section
\ref{sec:background}, we indicated a connection between distributed
systems and epistemic logic, i.e. reasoning about not just what is
true but what is known. Matrix clocks can be applied when processes
require knowledge about the knowledge had by others.

Suppose a group of processes collaborate to replicate updates to a
shared data structure. We imagine that all updates are submitted by
users to some process, say $P_O$ ($O$ for ``origin''), whereupon it is
timestamped with $P_O$'s local time (value $\vt_O[O,O]$) and with the
value $O$ to indicate which process originally accepted the update
from a user. As an invariant, we ensure that no process $P_j$ updates
its estimate $\vt_j[j, O]$ of $O$'s local time to a value $t$ until
$P_j$ has been informed about all updates submitted to $P_O$ with
timestamps less than or equal to $t$. With these assumptions, any
process $P_i$ can use the value $\vt_i[j, O]$ as a lower bound
estimate of which updates originating from $P_O$ have already been
seen by $P_j$. In particular, once we have the
condition
\[ \textrm{for all $j$}, \vt_i[j, O] \geq t
\]
for some logical time $t$, $P_i$ can be sure that all other processes
have seen all updates with timestamps less than or equal to $t$
originating at $P_O$. Using this principle, matrix clocks have been
used to discard obsolete information during database replication
\citationneeded. In Section \ref{sec:continuous-consistency} we will
see an example of how this sort of scheme can be applied in practice.

\subsection{Message Ordering and Group Communication}
We have seen that the network latencies cause messages to be delayed
and arrive in a different order than they were sent in. Coulouris et
al. \cite{coulouris2005distributed} aptly summarized why this is a
problem.
\begin{quote}
  This lack of an ordering guarantee is not satisfactory for many
  applications. For example, in a nuclear power plant it may be
  important that events signifying threats to safety conditions and
  events signifying actions by control units are observed in the same
  order by all processes in the system.
\end{quote}
In this section we summarize some different assumptions we could
impose about how messages are ordered. Then we discuss generalizing
our model to allow messages sent to multiple parties at once, such as
in a group chat application, where predictable message ordering
becomes particularly critical.

\subsubsection{Message Ordering}
In general, applications should not show messages to the user
immediately when they come in from the network, precisely because the
network tends to deliver messages in unexpected orders. Thus, we may
want to wait for other messages to ``catch up'' to present the
illusion that messages arrive in the right order. Thus, we make a
distinction between message \emph{receipt} and \emph{delivery}. A
message is received at a time determined by the network, but it is not
delivered until the process (which could be something like a network
protocol driver, a distributed middleware, or a user-facing
application) is able to enforce whatever ordering guarantees it is
meant to provide.

\paragraph{FIFO ordering}
A modest requirement is the \emph{first-in, first-out} (FIFO)
condition, which stipulates that on any logical communication link
between two nodes in the system, messages arrive in the order they
were sent. The restrictive phrase here is ``any logical communication
link''---by definition there is one link for any \emph{pair} of
processes. Hence, FIFO does not impose any conditions on messages
unless they are from the same sender and to the same recipient.

\begin{definition}[FIFO condition]
  \label{def:fifo}
  A FIFO execution is one that satisfies the following condition. Let
  $P_i$ and $P_j$ be any two processes and $m_1$ and $m_2$ be two
  messages sent from $P_i$ to $P_j$. Then
  $\msend{1} \to \msend{2} \implies \mrecv{1} \to \mrecv{2}$.
\end{definition}

FIFO ordering is simple to implement and often built into the
transport layer of a network protocol stack, so that some distributed
applications can take FIFO for granted. For instance, although the
Internet Protocol (IPv4) by itself does not provide FIFO semantics,
the associated and higher-level TCP protocol, which many applications
use, does provide FIFO. As TCP demonstrates, maintaining a FIFO
channel can be as simple as marking outgoing messages with consecutive
increasing numbers (e.g. $1,2,3\ldots$). If message $1$ arrives and
then $3$ arrives, we infer that $2$ is lagging behind, delivering $1$
to the user immediately but withholding delivery of $3$ until after
$2$ is received and delivered. As far as the end user can tell, the
messages arrive in the right order.

The guarantees provided by FIFO are minimal because they only apply on
a per-link basis: every link requires its own numbering scheme, so
message numbers across different links, meaning messages with
different senders \textbf{or} receivers, cannot be compared.

\begin{figure}[p]
  \setlength\abovecaptionskip{0ex}
  \setlength\belowcaptionskip{4ex}
  \begin{subfigure}[t]{0.475\textwidth}
    \centering
    \input{images/pgf/ordEx1.pgf}
    \caption{A non-FIFO execution}
    \label{fig:ordex-non-fifo}
  \end{subfigure}
  \begin{subfigure}[t]{0.475\textwidth}
  \centering
  \input{images/pgf/ordEx2.pgf}
  \caption{A CO (therefore FIFO) execution}
  \label{fig:ordex-co-1}
\end{subfigure}
\begin{subfigure}[b]{0.475\textwidth}
  \centering
  \input{images/pgf/ordEx3.pgf}
  \caption{A CO execution}
  \label{fig:ordex-co-2}
\end{subfigure}
\begin{subfigure}[b]{0.475\textwidth}
  \centering
  \input{images/pgf/ordEx6.pgf}
  \caption{A CO execution}
  \label{fig:ordex-co-3}
\end{subfigure}
\begin{subfigure}[b]{0.475\textwidth}
  \centering
  \input{images/pgf/ordEx5.pgf}
  \caption{A FIFO and non-CO execution}
  \label{fig:ordex-non-co-1}
\end{subfigure}\hfill
\begin{subfigure}[b]{0.475\textwidth}
  \centering
  \input{images/pgf/ordEx4.pgf}
  \caption{A FIFO and non-CO execution}
  \label{fig:ordex-non-co-2}
\end{subfigure}
\caption{Message ordering examples}
\label{fig:message-ordering}
\end{figure}

\paragraph{Causal ordering}
Causal order is an order consistent with causality. A network
satisfies the causally ordered (CO) delivery model if it satisfies the
following property.
\begin{definition}(Causally ordered delivery)
  \label{def:causalorder}
  For any process $P_\mathrm{dest}$ in the system, if we consider all
  messages $m$ and $n$ sent to $P_\mathrm{dest}$ (possibly by
  different senders), if $\msend{} \to n^\textrm{send}$ then
  $\mrecv{} \to n^\textrm{recv}$. That is, each destination
  receives messages in an order consistent with causality between send
  events.
\end{definition}
Unlike FIFO, the this condition enforces a partial order among
messages with possibly different senders. In mathematical terms, if
we consider the set of all messages sent to some fixed destination
$P_{\mathrm{dest}}$, the function mapping send events to corresponding
receive events must be monotonic with respect to causal precedence.

Figure \ref{fig:message-ordering} demonstrates different message
ordering conditions. We make a few observations for emphasis.

\begin{itemize}
\item \ref{fig:ordex-non-fifo} violates FIFO because messages $m_1$
  and $m_2$ are both sent from $P_1$ to $P_2$, but the arrive in the wrong order.
\item \ref{fig:ordex-co-1} satisfies CO and therefore FIFO. Messages
  $m_1$ and $m_2$ arrive in the opposite order but they are sent to
  different destinations.
\item \ref{fig:ordex-non-co-1} violates CO because the send event of
  $m_1$ happens before that of $m_3$ via the chain
  $\msend{1} \to \msend{2} \to \mrecv{2} \to \msend{3}$ but
  $\mrecv{3} \to \mrecv{1}$.
\item \ref{fig:ordex-non-co-2} violates CO because it is equivalent to
  \ref{fig:ordex-non-co-1} with the roles of $P_2$ and $P_3$ swapped.
\end{itemize}

\subsubsection{Group Communication}
Now we generalize the above definitions to the group communication
setting. We will allow messages $m$ with multiple recipients. For
simplicity we suppose messages are sent to all other recipients, known
as a broadcast. More generally we could consider messages sent to a
subset of other processes, known as a multicast.

At a lower level of abstraction, broadcasting could be implemented by
sending distinct messages which for present purposes would be treated
as a single unit. Alternatively we might send a single message
specially marked as a broadcast and rely on lower-level network
protocols to ensure each recipient receives a copy. Regardless of
implementation, because messages are now associated with multiple
receive events, the importance of consistent message ordering is
particularly acute.

The FIFO and CO conditions in the broadcast setting are just
adaptations of Definition \ref{def:fifo} and
\ref{def:causalorder}. Additionally we can consider a new notion of
total ordering (TO), defined below.

\begin{definition}[FIFO broadcast]
  \label{def:fifo-bcast}
  A broadcast primitive satisfies the FIFO semantics if it satisfies
  the following condition. For any process $P_i$, if $P_i$ broadcasts
  $m_1$ before $m_2$, then all recipients receive $m_1$ before $m_2$.
\end{definition}

\begin{definition}[CO broadcast]
  \label{def:causalorder-bcast}
  A broadcast primitive satisfies CO semantics if for any broadcasts
  $m$ and $n$, if $\msend{} \to n^{\textrm{send}}$, then all
  destinations deliver $\mrecv{}$ before $n^{\textrm{recv}}$.
\end{definition}
In the above definition, the happens before relation is defined just
as in the unicast (non-broadcast) setting by following a path of
causality along worldlines and message arrows.

\begin{figure}[h]
  \centering \input{images/pgf/mpEx3.pgf}
  \caption{Broadcast example that satisfies FIFO but violates CO}
  \label{fig:broadcast-fifo-1}
\end{figure}

Figure \ref{fig:broadcast-fifo-1} shows a violation of CO. We might
imagine the following conversation has taken place:
\begin{itemize}
\item [$P_1$]: ``I need an ambulance at location A.''
\item [$P_2$]: ``Understood, the last ambulance has been dispatched.''
\end{itemize}
However, $P_3$ witnesses $P_2$'s answer before seeing $P_1$'s
question. This results in $P_3$ hearing a rather different snippet of
the conversation:
\begin{itemize}
\item [$P_2$]: ``Understood, the last ambulance has been dispatched.''
\item [$P_1$]: ``I need an ambulance at location A.''
\end{itemize}
From $P_3$'s perspective, it seems that $P_1$ is requesting resources
that are not available. The sort of conflict can lead to requests
going unanswered, or possibly handled twice, leading to a chaotic
situation and misallocation of resources. Hence, respecting causal
order is often a critical consideration.

\paragraph{Total order broadcast}
A total order ensures that all recipients receive the messages in the
same order. Note that whatever the final order is, it is not required
to satisfy any particular constraints except that all recipients agree
on it. Such a model is appropriate when it is more important that
everyone agrees on a common order of events but the order itself is
not necessarily critical.

\begin{definition}(Totally ordered broadcast)
  \label{def:totalorderbroadcast} For any processes $P$ and $P'$ and
  messages $m$ and $m'$ that arrive at \emph{both} destinations, $m$
  arrives before $m'$ at both processes or $m'$ arrives before $m$ at
  both processes.
\end{definition}

\begin{figure}[p]
  \setlength\belowcaptionskip{5ex}
  \begin{subfigure}{1\textwidth}
    \centering
    \input{images/pgf/bcastEx1.pgf}
    \caption{Broadcast example that satisfies FIFO but violates CO and TO}
    \label{fig:bcast-order-examples-1}
  \end{subfigure}
  \begin{subfigure}{1\textwidth}
    \centering
    \input{images/pgf/bcastEx2.pgf}
    \caption{Broadcast example that satisfies CO but violates TO}
    \label{fig:bcast-order-examples-2}
  \end{subfigure}
  \begin{subfigure}{1\textwidth}
    \centering
    \input{images/pgf/bcastEx3.pgf}
    \caption{Broadcast example that satisfies TO but violates FIFO}
    \label{fig:bcast-order-examples-3}
  \end{subfigure}
  \caption{Multicast ordering examples}
  \label{fig:bcast-ordering-examples}
\end{figure}

Figure \ref{fig:bcast-ordering-examples} depicts examples of broadcast
message orders. Figure \ref{fig:bcast-order-examples-1} trivially
satisfies FIFO because no process sends more than one multicast, but
causality is violated because
$\msend{1} \to \mrecv{1,4} \to \msend{2}$, but $P_2$ receives $m_2$
before $m_1$. Total order is also violated because $P_2$ and $P_3$
receive the messages in opposite orders.  Figure
\ref{fig:bcast-order-examples-2} violates TO for the same reason, but
satisfies causality because the two send events have no causal
relation. Figure \ref{fig:bcast-order-examples-3} satisfies TO, but
$m_2$ arrives at both processes before $m_1$ so FIFO is violated.

\paragraph{Self-delivered messages}
In some contexts one considers multicast primitives that include the
original sender among the recipients of a message. For instance,
suppose the broadcasts involve instructions to apply an update to a
database, for which purpose one might use totally ordered broadcasts
to ensure everyone applies updates in the same order. In this setting,
one should not apply one's own update to a local copy of the database
immediately, but must wait until others have been notified, so that
all updates are applied in a common order. In this case, one must wait
to receive a copy of one's own message with update instructions, as
this ensures we apply the update at the same (logical) time that all
other processes are applying it. For simplicity, the examples in this
section have not shown this sort of self-delivery.

\subsection{Shared Memory}
\label{ssec:shared-memory}
Framing a distributed application directly in terms of message passing
can be challenging, as the low-level details required to implement the
correct message passing behavior distract from the high-level
application design. In this section we summarize a more abstract
paradigm called the \emph{distributed shared memory} (DSM) framework.

In the DSM setting, programmers think in terms of reading and writing
values to memory locations rather than transmitting messages over the
network. The key feature is that all instances of the application act
as if they are reading and writing from the same pool of memory
locations, when they are actually running on distributed computers
that do not share memory. In a sense, instead of focusing on a logical
notion of time, in the DSM model we directly focus on logical notion
of shared state.

To implement DSM, a ``middleware'' layer called the
\emph{memory manager} handles read and write requests issued by the
application. Under the hood, the middleware might coordinate with
other nodes by passing messages to inform them about updates to data
in a way that facilitates the illusion that processes share a common
virtual memory. The programmer is not directly concerned with how this
is accomplished. They are, however, required to understand what
guarantees are and are not provided by the memory management layer, in
other words what kinds of \emph{semantics} the virtual memory
implements. Generally, the design space involves a fundamental
tradeoff between safety guarantees and performance.

\begin{figure}
    \centering
    \input{images/pgf/smEx1NoEdges.pgf}
    \caption{Time diagram for memory operations}
    \label{fig:smEx1}
\end{figure}

\newcommand{\Op}{\mathrm{Op}}

Figure \ref{fig:smEx1} depicts an exemplary time diagram for the
shared memory abstraction, not unlike the diagrams for message
passing. Two kinds of operations are shown:
\begin{description}
\item[Reads] A \emph{read} request $\mathsf{R}(x)$ indicates reading the value
  in memory at location $x$, which returns some value $v$. When it is
  important to indicate that returned value is $v$, we write $\memreadVal{x}{v}$.
\item[Writes] A \emph{write} operation $\memwrite{x}{v}$ indicates writing
  value $v$ to memory location $x$. In pseudocode this might be written
  $x := v$ or $x \leftarrow v$.
\end{description}
We write $\Op$ to indicate an arbitrary (read or write)
operation. Each operation consists of a horizontal span along the
worldline of a process, which represents the duration of the
operation. Each operation begins at a time $\memstart{\Op}$, beginning
when the programmer invokes the memory manager. The operation ends at
some time $\memstop{\Op}$, when a response is given to the caller
(either an acknowledgement for a write operation, or a value for a
read operation). The entire collection of operations, as depicted in a
time diagram, is called a \emph{history}. If $H$ is a history, we
write $H|_P$ for the set of operations that happen on process $P$,
called the \emph{local history} of $P$.

In the background, the memory management layer coordinates with other
processes over the network. While executing a read request
$\memread{x}$ it may, for example, send messages to other nodes to ask
them about the current value of some $x$. This behind-the-scenes
coordination is transparent to the programmer and not reflected in the
diagrams.

\subsection{Semantics and consistency}

For a sequential application, i.e. one running on a single computer,
it is clear how read and write requests should be interpreted. A read
request $\memread{x}$ should return the most recent value $v$ that was
written to $x$ by a write $\memwrite{x}{v}$ (or resort to some default
behavior if no such write exists, but we will not consider such
examples). This interpretation is unambiguous because we assume that
operations running on a single process do not overlap in time, so it
always makes sense to ask which of two events happened before the
other.
\begin{figure}
  \input{images/pgf/smEx0.pgf}
  \caption{Memory operations on a single process}
  \label{fig:smEx0}
\end{figure}

\begin{example}
  \label{exmpl:updatesoneprocess}
  Consider Figure \ref{fig:smEx0}, depicting just a single
  process. Since there is no ambiguity in the order of events, it is
  clear that this process should execute in the following order.
  \[ \memwrite{x}{0} \to \memwrite{y}{5} \to \underline{\memreadVal{x}{0}} \to \memwrite{x}{3} \to \underline{\memreadVal{y}{5}} \to \underline{\memreadVal{x}{3}}. \]
  Note read requests (underlined with their results shown) return the value of most recent write operation to each location.
\end{example}

In the distributed context, operations on different processes can
execute concurrently, so there is no obvious total order that can be
used to compare events. Without such a comparison, the notion of
``most recent'' operation is ambiguous, which makes it difficult to
say exactly how the memory requests should even be interpreted.

\begin{figure}
  \input{images/pgf/smEx2.pgf}
  \caption{Concurrent updates by two processes}
  \label{fig:smEx2}
\end{figure}

\begin{example}
  \label{exmpl:concurrentupdates}
  Consider Figure \ref{fig:smEx2}. Two writes are depicted that overlap
  in physical time, making it unclear whether $\memwrite{x}{3}$ or $\memwrite{x}{5}$
  should be considered as happening first.
\end{example}

In Example \ref{exmpl:concurrentupdates}, it seems likely intuitively
that the programmer should expect each $\memread{x}$ operation to return
either 3 or 5. It should definitely not return 37, but perhaps 4 could
be expected in some situations. After some thought, especially about
how to implement the memory manager, a few other questions seem less
obvious.
\begin{itemize}
\item Must the read requests on $P_1$ and $P_2$ return the same value?
\item May the second read at $P_2$ return a different value
  than the preceding one?
\item Is there a world in which an $\memread{x}$ could return 4?
\end{itemize}

It is possible to consider different ways of answering these
questions. The role of a \emph{memory model} is to specify exactly
which possibilities are allowed and which are prohibited, which comes
down to constraining which kinds of values can be returned by read
operations under different conditions. An application designed
assuming one memory model may misbehave if executed in an environment
that actually implements a different one. On the other hand, memory
models that provide greater guarantees to the programmer also impose
greater constraints on the memory manager and, ultimately, the
network. Thus, the selection of a memory model requires balancing the
needs and expectations of the application against the practical
constraints of the environment.

\subsubsection{Concurrency and External order}
\label{ssec:externalorder}
As in Section \ref{ssec:causality}, one might hope to disambiguate
the global order of memory operations by assigning them with
sufficiently fine-grained timestamps indicating when they start and
terminate. However, comparing physical timestamps between operations
on different processes requires them to have synchronized clocks,
which we have seen we cannot assume.

A fundamental relation playing a role somewhat like causal precedence
in Section \ref{ssec:causality} (but without any connotations of
causality) is that of \emph{external order}. Intuitively, it is the
partial order that orders non-overlapping events by their physical
times, but does not assign an ordering to events whose executions
overlap in physical time.

\begin{definition}[External order]
  Let $H$ be a history. An operation $\Op^1$ \emph{externally
    precedes} operation $\Op^2$ if
  $\memstop{\Op^1} < \memstart{\Op^2}$. This induces an irreflexive
  partial order on $H$ called external order.
\end{definition}
The definition states that one operation externally precedes another
if it stops before the other is invoked. Note that we are comparing
events in terms of real, physical time: external order is the partial
order that would be expected by an outside observer who can see
operations executing globally in real time.
\begin{definition}[Physical concurrency]
  Consider two operations $\Op^1$ and $\Op^2$. If neither externally
  precedes the other, in another words if there is some moment in time
  during which both operations are executing, the operations are said
  to be \emph{physically concurrent}, written $\Op^1 || \Op^2$.
\end{definition}
If one operation finishes
before another starts, this must surely come first. If two operations
are ever seen executed at the same time, then we do not assume
anything about which one should be considered to execute first.

%Because each process handles operations one-at-a-time, a local history
%$H|_P$ is totally ordered by external order. However, operations by
%different process may overlap.

Physical concurrency is a reflexive and symmetric relation, but
typically not a transitive one. Such structures are generally known as
compatibility relations, the intuition being that if \(A\) and \(B\) are
both ``compatible'' with \(C\), it need not be the case that \(A\) and
\(B\) are compatible with each other. The reader should convince
themselves that if $\Op^1 || \Op^2$ and $\Op^2 || \Op^3$, it need not
be the case that $\Op^1 || \Op^3$.

For performance reasons, memory models commonly allow distributed
operations to have semantics that do not strictly conform to external
order. For instance, below we show how sequential consistency
(Definition \ref{def:sequentiallyconsistent}), one of two ``strong''
models, allows for this possibility. Yet even this model imposes too
many constraints to be workable for networking environments as
unpredictable as ours.

\begin{figure}
    \centering
    \input{images/pgf/smEx1DAG.pgf}
    \caption{External order relation among operations in Figure \ref{fig:smEx1}}
    \label{fig:smEx1DAG}
\end{figure}

Figure \ref{fig:smEx1DAG} shows the external order among the
operations in Figure \ref{fig:smEx1} in the form of a directed
acyclic graph (DAG).

\begin{definition}[Sequential history]
  Let $H$ be a history of invocation/response events. A
  \emph{sequential history} of $H$ is any choice of total order among
  the events in $H$---that is, some rearrangement of the operations as
  to ensure none of them overlap in time.
\end{definition}

\subsection{Strong Consistency Models}
\label{ssec:strong-consistency}
The first two memory models in the next section require, among other
things, that any history $H$ must be consistent with some sequential
rearrangement of $H$.

\subsubsection{Linearizability}
\label{sssec:linearizability}

\emph{Linearizability}, the strongest consistency model, can be
concisely defined as providing the appearance that ``each operation
applied by concurrent processes takes effect instantaneously at some
point between its invocation and response.''
\cite{10.1145/78969.78972} The same condition is known variously as
atomic consistency, strict consistency, and sometimes external
consistency. In the context of database transactions (which come with
other database-specific guarantees, particularly serializability), the
analogous condition is called strict serializability.

A linearizable history is defined by three features.
\begin{definition}[Linearizable history]
  \label{def:linearizable}
  A \emph{linearizable history} is one satisfying the following three rules.
\begin{enumerate}
  \tightlist
\item[\textbf{L1}] All processes act like they all see some common sequential
  history of operations.
\item[\textbf{L2}] Responses are semantically correct, meaning a read request
  \(\memreadVal{x}{a}\) returns the value of the most recent write request
  \(\memwrite{x}{a}\) to \(x\).
\item[\textbf{L3}] The sequential history is consistent with external
  order. That is, if the response of $\Op$ precedes the invocation of
  $\Op'$, then $\Op$ precedes $\Op'$ in the sequential history.
\end{enumerate}
\end{definition}

We return to Example \ref{exmpl:concurrentupdates}. Rule 1 of
Definition \ref{def:linearizable} requires $P_1$ and $P_2$ to act like
they both witness the same total order of operations, which in
particular assigns a total order on the three write operations
$\memwrite{x}{4}, \memwrite{x}{3},$ and $\memwrite{x}{5}$. By Rule 3 this order must agree with
external order, meaning $\memwrite{x}{4}$ comes first, followed by $\memwrite{x}{3}$ and
$\memwrite{x}{5}$ in either order (due to their physical concurrency), followed
by the three read operations. By Rule 2, the three read operations
must return the value of $x$ assigned by whichever of $\memwrite{x}{3}$ or
$\memwrite{x}{5}$ happens last. Altogether, linearizability leaves only two
possibilities: all three read operations return $3$, or all three
return $5$. These are known as the possible \emph{linearizations} of
the history shown in Figure \ref{fig:smEx1}.

A more visually intuitive way of approaching linearizability is by
defining it in terms of \emph{linearization points.}

\begin{definition}
  A \emph{linearization point} $t \in \mathbb{R} \in [\Op.s, \Op.t]$
  for an operation $\Op$ is a time between the event's invocation and
  response, at which time the operation appears to take effect in
  whole and instantaneously.
\end{definition}

%\begin{example}
%  \label{exmpl:linearizations}
The two possible linearizations of Figure \ref{fig:smEx1} are shown in
Figure \ref{fig:smEx3}. Possible choices for linearization points are
shown in yellow. Note that only the relative order of linearization
points is important. These examples demonstrate that linearizability
leaves room for non-determinism in the execution of distributed
applications. In particular, the read operations in Figure
\ref{fig:smEx1} can return 3 or 5, as long as they all return the same
value.

\begin{figure}[p]
  %\setlength\belowcaptionskip{4ex}
  \begin{subfigure}{1\textwidth}
    \centering
    \input{images/pgf/smEx3L1.pgf}
    \caption{A linearization where the read operations return 3}
    \label{fig:smEx1L1}
  \end{subfigure}
  \begin{subfigure}{1\textwidth}
    \input{images/pgf/smEx3L2.pgf}
        \caption{A linearization where the read operations return 5}
    \label{fig:smEx3L2}
  \end{subfigure}
  \caption{Two possible linearizations of Figure \ref{fig:smEx2} with linearization points shown in yellow}
  \label{fig:smEx3}
\end{figure}

Note that is a condition on individual executions of a distributed
application. When only linearizable executions are permitted by the
memory manager, the entire system is said to be linearizable.

\begin{definition}[Linearizable system]
  A memory management layer is linearizable if all possible histories
  of the system are linearizable.
\end{definition}

\subsubsection{The CAP Theorem}
Real-world systems often fall short of behaving as a single perfectly
coherent system. The root of this phenomenon is a deep and
well-understood tradeoff between system coherence and performance.
Enforcing consistency comes at the cost of additional communications,
and communications impose overheads, often unpredictable ones.

\begin{definition}[Network partition]
  A \emph{network partition} is a span of time where some nodes are
  unable to communicate with another set of nodes on the network.
\end{definition}

Fox and Brewer \cite{1999foxbrewer} are crediting with observing a
particular tension between the three competing goals of consistency,
availability, and partition-tolerance. This tradeoff was precisely
stated and proved in 2002 by Gilbert and Lynch
\cite{2002gilbertlynchCAP}.  The theorem is often somewhat
misunderstood, as we discuss, so it is worth clarifying the terms
used.

\begin{description}
\item[Consistency] Gilbert and Lynch define consistency as linearizability.
\item[Availability] A CAP-available system is one that responds to
  every client request in a finite time.
\item[Partition tolerance] A partition-tolerant system continues to
  function, in the face of arbitrary partitions in the network. (It is
  possible that a partition never recovers, say if a critical
  communications cable is permanently severed.)
\end{description}

Some reflection shows that the full set of requirements is
unattainable---a partition tolerant system simply cannot enforce both
consistency and availability. We give only the informal sketch here,
leaving the interested reader to consult the more formal analysis by
Gilbert and Lynch. The key technical assumption is that a process's
behavior can only be influenced by the messages it actually
receives---it cannot be affected by messages that are sent to it but
never delivered.

\begin{theorem}[The CAP Theorem]
  \label{thm:cap}
  In the presense of indefinite network partitions, a distributed system
  cannot guarantee both linearizability and eventual-availability.
\end{theorem}
\begin{proof}
  Consider the concurrent writes by separate processes in Figure
  \ref{fig:smEx2}.  Figure \ref{fig:smEx3} showed that both possible
  linearizations of this history require all reads to return the same
  value, either $3$ or $5$. Now suppose the network is unavailable, so
  the processes cannot communicate, in particular to share write
  updates.

  Now there are only two possibilities. First, the processes could
  proceed despite the lack of communication. In this case, because
  processes do not otherwise affect each other, $P_1$ could not see
  the $\memwrite{x}{5}$ operation and its read must return the value
  $3$. Likewise, $P_2$ does not see $\memwrite{x}{3}$ and its reads return
  $5$. This situation violates linearizability, hence the 'C' of CAP.

  Alternatively, the processes might detect that the network is
  unavailable and refuse to respond to read requests. However, this
  indefinitely suspects progress until the network recovers, which
  violates the 'A' of CAP.
\end{proof}

To provide both 'C' and 'A', the last remaining option is to assume
that the network never suffers from partitions, in other words to
sacrifice the 'P' of CAP. Clearly this is unacceptable in the systems
we are considering. Consequently, our systems cannot ensure atomic
consistency and availability.

\begin{comment}
A partition-tolerant CAP-available system cannot indefinitely suspend
handling a request to wait for network activity like receiving a
message. In the event of a partition that never recovers, this would
mean the process could wait indefinitely for the partition to heal,
violating availability.

On the other hand, a CAP-consistent system is not allowed to return
anything but the most up-to-date value in response to client
requests. Keep in mind that any (other) process may be the originating
replica for an update.
\end{comment}

\subsubsection{Sequential consistency}
\label{sequential-consistency}
Enforcing atomic consistency means that an access \(E\) at process
\(P_i\) cannot return to the client until every other process has been
informed about \(E\). For many applications this is an unacceptably
high penalty. A weaker model that is still strong enough for most
purposes is \emph{sequential} consistency. This is an appropriate
model if a form of strong consistency is required, but the system is
agnostic about the precise physical time at which events start and
finish, provided they occur in a globally agreed upon order.

A sequentially consistent system ensures that any execution is
equivalent to some global serial execution, even if this is serial order
is not the one suggested by the real-time ordering of events. When
real-time constraints are not important, this provides essentially the
same benefits as linearizability. For example, it allows programmers to
reason about concurrent executions of programs because the result is
always guaranteed to represent some possible interleaving of
instructions, never allowing instructions from one program to execute
out of order.

\begin{definition}[Legal sequential history]
Let $H$ be a history of invocation/response events occurring on a set
of processes $\{P_i\}_{i = 1 \ldots N}$. A \emph{sequential history}
of $H$ is any choice of total order among the events in $H$---that is,
a rearrangement of the operations as to ensure none of them overlap in
time. If each $P_i$ issues
$r_i$-many requests for some non-negative integer $r_i$, observe there
are a total of
\[
\frac{\left(\sum_{i = 1}^N r_i\right)!}{\prod_{i = 1}^N r_i!}
\]
possible sequential histories.
\end{definition}

\begin{definition}
  \label{def:sequentiallyconsistent}
  A \emph{sequentially consistent} execution is
  characterized by three features:
  \begin{itemize}
  \item All processes act like they agree on a single, global total order
    defined across all accesses.
  \item This sequential order is consistent with the program order of each process.
  \item Responses are semantically correct, meaning reads return the most recent writes (as determined by the global order)
  \end{itemize}
\end{definition}

\begin{figure}[p]
  %\setlength\belowcaptionskip{5ex}
  \begin{subfigure}{1\textwidth}
    \centering
    \input{images/pgf/smEx4S1.pgf}
    \caption{An example where the processes read different values}
    \label{fig:smEx1L1}
  \end{subfigure}
  \begin{subfigure}{1\textwidth}
    \input{images/pgf/smEx4S2.pgf}
    \caption{An example where the first write operation affects a read}
    \label{fig:smEx4S2}
  \end{subfigure}
  \caption{Sequentially consistent, nonlinearizable executions of Figure \ref{fig:smEx2}}
  \label{fig:smEx3}
\end{figure}
\begin{figure}[p]
  %\setlength\belowcaptionskip{5ex}
  \begin{subfigure}{1\textwidth}
    \centering
    \input{images/pgf/smEx4S3.pgf}
    \caption{}
    \label{fig:smEx1L1}
  \end{subfigure}
  \begin{subfigure}{1\textwidth}
    \input{images/pgf/smEx4S4.pgf}
    \caption{}
    \label{fig:smEx4S2}
  \end{subfigure}
  \caption{Two non-sequentially-consistent executions of Figure \ref{fig:smEx2}}
  \label{fig:smEx3}
\end{figure}

Processes in a sequentially consistent system are required to agree on
a total order of events, presenting the illusion of a shared database
from an application programmer's point of view. However, this order
need not be given by external order. Instead, the only requirement is
that sequential history must agree with process order, i.e.~the events
from each process must occur in the same order as in they do in the
process.  This is nearly the definition of linearizability, except
that external order has been replaced with merely program order. We
immediately get the following lemma.

\begin{lemma}
  \label{lem:linearsequential}
  A linearizable execution is sequentially consistent.
\end{lemma}
\begin{proof}
  This follows because process order is a subset of external order.
\end{proof}

Visually, sequential consistency allows reordering an execution by
sliding events along each process's time axis like beads along a
string.  Two events from the same process cannot pass over each other
(as this would precisely be a violation of program order), but events
on different processes may be commuted past each other, contravening
external order. This sliding allow noe to construct a fairly arbitrary
interleaving of events, a totally ordered execution with no events
overlapping. From this perspective, while linearizability requires the
existence of a linearization, sequential consistency requires the
existence of an interleaving.

It may seem strange to consider executions such as the one shown in
REF in which operations appear to take effect at different times for
different processes, or at times that do not agree with external
order. The reader should remember that in the background, these
processes would be engaged in message-passing over the network and are
therefore subject to all the complexities previous discussed in
Section \ref{sec:message-passing}, including delayed and out-of-order
messages. Looser requirements by the memory model impose fewer
constraints on the message passing layer requiring less coordination,
and allowing for greater performance.

The converse of Lemma \ref{lem:linearsequential} does not hold. For
example, Figures \ref{fig:smEx4S1} and \ref{fig:smEx4S2} are
nonlinearizable executions of Figure \ref{fig:smEx2} that are
nonetheless sequentially consistency.

\subsubsection{The CAP Theorem for SC}

Recall Figure \ref{fig:smEx1L1}. This example demonstrated that
sequential consistency allows the history in Figure \ref{fig:smEx2} to
return non-equal values for the two read operations. That is, if $P_1$
reads $3$, and both reads on $P_2$ return $5$, the execution is
sequentially consistent. This raises the possibility that sequential
consistency is not subject to the limits of the CAP
theorem. Unfortunately the hope is fleeting: like linearizability,
sequential consistency is CAP-unavailabile.

\begin{lemma}[CAP for sequential consistency]
  \label{thm:cap-sequential}
  An eventually-available system cannot provide sequential consistency in the presense of network partitions.
\end{lemma}
\begin{proof}
  The proof is an adaptation of Theorem \ref{thm:cap}. Suppose $P_1$ and
  $P_2$ form of CAP-available distributed system and consider the
  following execution: $P_1$ reads $x$, then assigns $y$ the value
  $1$. $P_2$ reads $y$, then assigns $x$ the value $1$. (Note that this
  is the sequence of requests shown in Figure \ref{fig:nonsequential1},
  but we make no assumptions about the values returned by the read
  requests). By availability, we know the requests will be handled (with
  responses sent back to clients) after a finite amount of time. Now
  suppose $P_1$ and $P_2$ are separated by a partition so they cannot
  read each other's writes during this process. For contradiction,
  suppose the execution is equivalent to a sequential order.

  If $\memwrite{y}{1}$ precedes $\memread{y}$ in the sequential order, then $\memread{y}$ would
  be constrained to return to $1$. But $P_2$ cannot pass information to
  $P_1$, so this is ruled out. To avoid this situation, suppose the
  sequential order places $\memread{y}$ before $\memwrite{y}{1}$, in which case $\memread{y}$
  could correctly return the initial value of $0$. However, by
  transitivity the $\memread{x}$ event would occur after $\memwrite{x}{1}$ event, so it
  would have to return $1$. But there is no way to pass this information
  from $P_1$ to $P_2$. Thus, any attempt to consistently order the
  requests would require commuting $\memwrite{y}{1}$ with $\memread{x}$ or $\memwrite{x}{1}$ with
  $\memread{y}$, which would violate program order.
\end{proof}

\subsection{Causal consistency}
\label{ssec:causal-consistency}

\emph{Causal} consistency\citationneeded is a weaker memory model than
sequential consistency. Whereas sequential consistency requires the
system as a whole to behave as if all write operations take place in
some total order (as long as this is consistent with program order),
causal consistency allows different processes to see write operations
take effect in different orders. Instead, only write operations that
are \emph{causally related} need to take effect in a common order
at all processes.

We have not defined what it means for memory operations to be causally
related. The notion is very similar to causal precedence in the
context of message passing (Definition \ref{def:causalprecedence}),
with a small wrinkle: in a message passing context, each receive event
is uniquely paired with a send event. However, because multiple
operations can write the same value into the same location, there is
generally not a unique way to associate each read operation with the
write operation that ``caused'' it to read a particular value.

\begin{definition}[Writes-into order]
  Consider a set of memory operations. A ``writes into''
  $\rightsquigarrow$ order is any binary relation that satisfies the
  following conditions:
  \begin{itemize}
  \item If $o \rightsquigarrow o'$, then $o = \memwrite{x}{v}$ and $o' = \memreadVal{x}{v}$ for memory location $x$ and value $v$.
  \item For each $o'$, there is exactly one $o$ such that $o \rightsquigarrow o'$
  \end{itemize}
\end{definition}

This definition is a slight simplication of that found
in\citationneeded: To avoid talking having to talk about default
values, we only consider cases here where all read operations are
preceded by some write operation. Intuitively, the idea is that a
writes-into order pairs reads with the write operations whose values
they are ``reading from.'' If all values written into each location
are unique, the writes-into order is unambiguous, as an operation that
reads $v$ from location $x$ could only be paired with the operation
that writes $v$ into $x$.

\begin{definition}[Causal order on memory operations]
  \label{def:memorycausalprecedence}
  For some choice of a writes-into order, we define a binary relation
  $\to$ on the set of memory operations as follows:
  \[o \to o' \iff
  \begin{cases}
    o <_{P_i} o' \textrm{for some process $P_i$}
    \textbf{ or} \\
    o \rightsquigarrow o'
    \textbf{ or} \\
    \textrm{there is some } o'' \textrm{ such that } o \to o'' \textrm{ and } o'' \to o'
  \end{cases}
  \]
  If $o \to o'$, we say $o$ causally precedes $o'$.
\end{definition}

We can now define causally consistent executions of memory operations.

\begin{definition}[Causal consistency]
  \label{def:causalconsistency}
  A history is causally consistent if each process (acts like it) sees
  write requests take place in some order that is consistent with
  causal order for some choice of writes-into order.
\end{definition}

\begin{figure}[p]
  \begin{subfigure}{1\textwidth}
    \centering
    \input{images/pgf/smEx4S3.pgf}
    \caption{}
    \label{fig:smEx1L1}
  \end{subfigure}

  \begin{subfigure}{1\textwidth}
    \input{images/pgf/smEx4S4.pgf}
    \caption{}
    \label{fig:smEx5}
  \end{subfigure}

  \caption{PLACEHOLDER: Causally consistent and inconsistent executions}
  \label{fig:smEx3}
\end{figure}

\afterpage{\clearpage}

\subsubsection{CAP-availability of Causal Consistency}
\label{the-cap-theorem}

\begin{lemma}[Causal consistency is CAP-available]
  \label{thm:cap-causal}
  Causal consistency can be enforced during network partitions. That
  is, causal consistency is not subject to the CAP thereom.
\end{lemma}
\begin{proof}
  Consider processes that execute read and write operations purely
  locally. That is, they never send messages to other processes and
  they only read to their own writes. In this situation, the causal
  relation generated by Definition \ref{def:memorycausalprecedence} is
  essentially unique: operations at $P_i$ are causally preceded by the
  previous operations at $P_i$, and no others. In the absence of a
  non-trivial causal order, causal consistency imposes no constraints
  and the execution is vacuously consistent.
\end{proof}

Unfortunately, the proof of the prior lemma has more to do the
weakness of the promises made causal consistency. The example allows
different processes to deviate arbitrary far from consistency, in the
sense that no processes need to agree on any of the updates applied to
the database. The effect is that causal consistency is too weak to
apply any kind of bound on divergence, which suggests it is not strong
enough for the kinds of safetly-related applications we have in mind.

\subsubsection{Consequences of CAP}
\label{interpretation-of-the-cap-theorem}
While the CAP theorem is conceptually simple, its interpretation is
subtle and has been the subject of much discussion since it was
introduced \cite{2012CAP12Years}. It is sometimes assumed that the CAP
theorem claims that a distributed system can only offer two of the
properties C, A, and P.  In fact, the theorem constrains, but does not
prohibit the existence of, applications that apply some relaxed amount
of all three features. The CAP theorem only rules out their
combination when all three are interpreted in a highly idealized
sense.

In practice, applications can tolerate much weaker levels of
consistency than linearizability. Furthermore, network partitions are
usually not as dramatic as an indefinite communications blackout. Real
conditions in our context are likely to be chaotic, featuring many
smaller disruptions and delays and sometimes larger
ones. Communications between different clients may be affected
differently, with nearby agents generally likely to have better
communication channels between them than agents that are far
apart. Finally, CAP-availability is a suprisingly weak condition.
Generally one cares about the actual time it takes to handle user
requests, but the CAP theorem exposes difficulties just ensuring the
system handles requests at all. Altogether, the extremes of C, A, and
P in the CAP theorem are not the appropriate conditions to apply to
many, perhaps most, real-world applications.

\subsection{Summary}
\label{sec:background-summary}

We have seen that a distributed system is built from geographically
distant components that must communicate over a network. Sending
messages over the network causes messages to suffer unpredictable
delays. Particularly in the context of broadcasts sent to multiple
members of a group at once, this varying delay often causes messages
to arrive in different orders to different members of the group, which
can lead to chaos if a message-ordering discipline is not imposed.

As part of taming this chaos, it is critical for distributed systems
to track the causal precedence relation between events. Physical
clocks cannot usually be relied upon to maintain adequate
synchronization for this purpose. Instead, logical clocks may be
employed. The different paradigms---scalar, vector, and matrix
clocks---vary in how much information they track and how much overhead
they impose on the system with respect to bandwidth and storage
space. The latter two require nodes to know about every other member
of the group, as is typical for many mechanisms in distributed
systems. If groups can change dynamically, as in our scenarios, then a
group membership protocol is also needed.

Programmers may find it easier to frame distributed applications in
terms of reading and writing from a shared pool of virtual memory
instead of sending messages over a network. However, the fact that
many nodes can access this virtual memory at the same time leads to
non-determinism in how the memory accesses are ordered, which makes it
non-trivial to decide what it means for the system to be
consistent. The two strongest notions of consistency---linearizability
and sequential consistency---essentially provide the illusion of a
single source of truth, but the CAP theorem makes it virtually
impossible to realize these consistency models in the kinds of chaotic
networks we are considering. The causal consistency model ensures a
minimum of coherence and has the advantage that it is not subject to
the limitations of the CAP theorem. However, it makes no guarantees
about how far apart replicas of the same data may diverge, which makes
this model too weak for the kinds of safety-related applications we
have in mind.

\section{Resilient Network Architectures}
\label{sec:networking}

A priori, a network may not deliver a message at all. Alternatively it
might deliver the message multiple times, for example if a device is
unaware that a message has already been delivered. In either case, the
network itself does not alert the sender or receiver to the fact---the
responsibility for detecting such conditions belongs to the sorts of
protocols considered here and in Section \ref{sec:networking}.

If we were to inspect the network blackbox, we would expect to find
so-called ``transport'' protocols like TCP\citationneeded being used
to provide the abstraction of reliable delivery over an otherwise
unreliable network. For example, they might add metadata to messages
that allow the receiver to arrange them in their intended order,
discard duplicates, and verify their integrity. Transport protocols
might also handle retranmission when messages become corrupted in
transit. We consider these kinds of low-level details in Section
\ref{sec:networking}; for now, one could say we are working at a level
of abstraction above the transport layer. Note that transport
reliability mechanisms contribute to the latency in passing messages,
so they cannot solve the problems under consideration in this section.

\subsection{Ad-hoc networking}
\label{ad-hoc-networking}

\subsubsection{Physical communications}
\label{physical-communications}

The details of the physical communication between processes is outside
the scope of this memo. We make just a few high-level observations about
the possibilities, as the details of the network layer are likely to
have an impact on distributed applications, such as the shared memory
abstraction we discuss below and in Section
\ref{sec:continuous-consistency}. For such applications, it may be
important to optimize for the sorts of usage patterns encountered in
real scenarios, which are affected by (among other things) the low-level
details of the network.

The \emph{celluar} model (Figure \ref{fig:centralized}) assumes nodes
are within range of a powerful, centralized transmission station that
performs routing functions. Message passing takes place by transmitting
to the base station (labeled \(R\)), which routes the message to its
destination. Such a model could be supported by the ad-hoc deployment of
portable cellphone towers transported into the field, for instance.

The \emph{ad-hoc} model (Figure \ref{fig:decentralized}) assumes nodes
communicate by passing messages directly to each other. This requires
nodes to maintain information about things like routing and the
approximate location of other nodes in the system, increasing complexity
and introducing a possible source of inconsistency. However, it may be
more workable given (i) the geographic mobility of agents in our
scenarios (ii) difficult-to-access locations that prohibit setting up
communication towers (iii) the inherent need for system flexibility
during disaster scenarios.

\begin{figure}[h]
  \centering
  \begin{subfigure}[b]{0.48\textwidth}
    \centering
    \includegraphics[width=\textwidth]{images/Centralized.png}
    \caption{Cellular network topology}
    \label{fig:centralized}
  \end{subfigure}
  \hfill
  \begin{subfigure}[b]{0.48\textwidth}
    \centering
    \includegraphics[width=\textwidth]{images/Decentralized.png}
    \caption{Ad-hoc network topology}
    \label{fig:decentralized}
  \end{subfigure}
  \caption{Network topology models for geodistributed agents. Edges represent communication links (bidirectional for simplicity).}
  \label{fig:nettopology}
\end{figure}

One can also imagine hybrid models, such as an ad-hoc arrangement of
localized cells. In general, one expects more centralized topologies to
be simpler for application developers to reason about, but to require
more physical infrastructure and support. On the other hand, the ad-hoc
model is more fault resistant, but more complicated to implement and
potentially offering fewer assurancess about performance. In either
case, higher-level applications such as shared memory abstractions
should be tuned for the networking environment. It would be even better
if this tuning can take place dynamically, with applications
reconfiguring manually or automatically to the particulars of the
operating environment. This requires examining the relationship between
the application and networking layers, rather than treating them as
separate blackboxes.

\subsubsection{Delay-tolerant networking}
\label{ssec:delay-tolerant-networking}

\subsection{Ad-hoc DTNs}
\label{ad-hoc-dtns}
An interesting possibility is for the \emph{network} to automatically
configure itself to the quality-of-service needs of the application. For
example, a client that receives a lot of requests may be marked as a
preferred client and given higher-priority access to the network. If UAV
vehicles can be used to route messages by acting as mobile transmission
base stations, one can imagine selecting a flight pattern based on
networking needs. For example, if the communication between two
firefighting teams is obstructed by a geographical feature, a UAV could
be dispatched to provide overhead communication support. Such an
arrangement could greatly blur the line between the networking and
application layers.

\subsection{Software-defined networking}
\label{software-defined-networking}

\subsection{Verification of networking protocols}
\label{verification-of-networking-protocols}

\section{Continuous Consistency}
\label{sec:continuous-consistency}
Strong (i.e. atomic or sequential) consistency, as defined in Section
\ref{ssec:strong-consistency}, is an all-or-nothing proposition: an
application provides strong consistency or it does not. Because of
considerations such as the CAP theorem, this sort of guarantee is an
unrealistic requirement for our setting. Causal consistency (Section
\ref{ssec:causal-consistency}) and other kinds of weaker assumptions
are tenable, but by themselves they provide no upper bound on the
divergence between replicas of data items. Because of the
safety-related nature of our system, we would like to do better than
this.

This section summarizes the key ideas behind \emph{continuous}
consistency. We specifically outline the conit (``consistency unit'')
model proposed by Yu and Vahdat \citationneeded. Continuous
consistency is motivated by the observation that for many real-world
applications, it evidently makes sense to work with data that is
consistent up to some error bound $\epsilon \geq 0$. It is not
immediately obvious what this means formally, but loosely speaking one
hopes to measure the difference between the user-observed value of a
database operation and the value that would have been returned if we
were maintaining strong consistency. By relaxing our error bound, in
other words by increasing $\epsilon$, we hope sacrifice some
consistency to gain some availability, unlike the CAP theorem which
apparently forces a choice of one or the other. Thus, we shift from
thinking about consistency as an all-or-nothing condition, towards
consistency as a quantiative upper bound on inconsistency.

A quantitative definition of consistency seems specific to each
application, but ideally our consistency model can be defined and
implemented in an application-generic manner. Thus we have a few
requirements.
\begin{itemize}
\item A mechanism to measure (in)consistency of data items
\item A mechanism for applications to set upper bounds on the allowed
  inconsistency of each item
\item A mechanism to efficiently enforce these inconsistency bounds
\end{itemize}

To implement a continuous consistency model that is not tied to one
application, we must first explain how inconsistency can be quantified
generically in the first place. Second, we must explain how a generic
protocols can be used to enforce.

\subsection{The conit model}
\label{ssec:conits}

We assume a set $\{P_i\}_{i \in I}$ of processes that coordinate to
replicate a database $D$. Note that we read ``database'' very loosely
to mean any kind of data items that can be updated in response to user
requests.

\begin{itemize}
\item The database is replicated in whole at each site.
\item The group members are fixed in advance. Otherwise we would
  require a group-membership protocol to update the set of
  participants dynamically.
\end{itemize}

The application could be essentially anything---relevant examples for
our environment could be a group chat application, a system for
requesting and dispatching resources, or a system for disseminating
information about weather and fire conditions. Clients can submit
requests to read or write (or update) values from the database at any
site. However, the application itself never updates the database
directly, but instead submits requests to a lower-level ``middleware''
that is logically situated below the application but above the network
transport layer (see Section \ref{ssec:shared-memory}). The middleware
mediates between the application and the local database.

When the application submits a database operation (read or write), the
middleware does not necessarily respond to the request
immediately. Instead, it may block (become unavailable) while it
coordinates with other processes to enforce the sort of consistency
guarantees defined below. During this time, the middleware can inform
other sites about new updates, or request that other sites report to
it any updates that could be relevant to the user's
request. Eventually, the middleware reads or writes from the local
copy of the database and returns a value back to the application, who
gives it to the user.

Separately from the local database itself, the middleware maintains a
\emph{write log} that stores a history of recent write requests it has
seen.

\subsection{Measuring consistency on conits}
\label{measuring-consistency-on-conits}

We discuss the three consistency dimensions and how they can be
implemented. Real-time staleness and order error are both bounded by a
\emph{pull}-based approach: the originating replica of a database
operation may block while contacting other sites in order to request
information relevant to the request. Numerical error in contrast is
bounded by a \emph{push}-approach: A node may have to block during a
request in order to proactively inform other nodes about the update
before it can be applied.

\subsubsection{Real time staleness}
\label{sssec:real-time-consistency}
Take, say, an application for disseminating the most up-to-date
visualization of the location of a fire front. It may be acceptable if
this information appears 5 minutes out of date to a client, but
unacceptable if it is 30 minutes out of date. That is, we could
measure consistency with respect to \emph{time}. One should expect the
exact tolerance for \(\epsilon\) will be depend very much on the
client, among other things. For example, firefighters who are very
close to a fire have a lower tolerance for stale information than a
central client keeping only a birds-eye view of several fire fronts
simultaneously.

To enforce real time staleness, we assume that each site has loosely
synchronized physical clocks. Note that while we do not assume
physical clocks can be synchronized precisely enough for the kind of
causality tracking discussed in Section \ref{sec:background}, it may
be tenable to assume synchronization precise enough for the purposes
discussed in this section, depending on how small the allowed
real-time divergence values are. For example, we estimate that
synchronization within, say, 30 minutes is tenable using appropriate
synchronization mechanisms even in deeply challenging network
environments.

Each site maintains a vector of \emph{physical} timestamp values. The
\emph{staleness} of a conit is the physical time elapsed since the
last update not seen by this replica was submitted.

The update rule is simple. When the real time.

\newcommand{\vtphys}[2]{\mathrm{vt}_{#1}}
\begin{itemize}
\item First, check whether $t_{\mathrm{now}} - \vt{i}[j] < \delta$
  holds for each entry $j$ in the real time vector.
\item If $t_{\mathrm{now}} - \vt{i}[j] \geq \delta$, $P_i$ sends a
  request to $P_j$ to pull any updates seen by $P_j$ but not by $P_i$.
\item After receiving the updates, $P_i$ reads the conit's value and
  returns it to the user.
\end{itemize}

\paragraph{Order consistency}
\label{order-consistency}
When the number of tentative (uncommitted) writes is high, TACT
executes a write commitment algorithm. This is a \emph{pull-based}
approach which pulls information from other processes in order to
advance \(P_i\)'s vector clock, raising the watermark and hence
allowing \(P_i\) to commit some of its writes.

\paragraph{Numerical consistency}
\label{numerical-consistency}

\subsubsection{Enforcing inconsistency bounds}
\label{enforcing-inconsistency-bounds}

\paragraph{Numerical consistency}
\label{numerical-consistency-1}
We describe split-weight AE. Yu and Vahdat also describe two other
schemes for bounding numerical error. One, compound AE, bounds absolute
error trading space for communication overhead. In their simulations,
they found minimal benefits to this tradeoff in general. It is possible
that for specific applications the savings are worth it. They also
consider a scheme, Relative NE, which bounds the relative error.

\paragraph{Order consistency}
\label{order-consistency-1}

\paragraph{Real time consistency}
\label{real-time-consistency-1}

\subsection{Variations and additional features}

One can imagine various ways that the conit model can be augmented
with additional capabilities.

\paragraph{Dynamic bounds}
Because real-time staleness and order error are bounded by pull
approach, it is straightforward to allow the user to dynamically
change the error bounds at each site. However, numerical error is
bounded by a push approach that requires every process to be aware of
all other processes's bounds and proactively cooperation to ensure
this invariant is maintained. Therefore, dynamically tuning numerical
error bounds requires a consensus mechanism that allows informing
other processes of any updates to these bounds. The application cannot
guarantee the new consistency bound will be enforced until it knows
that all other processes have seen the newly updated bounds.

\paragraph{Dynamic conits}
Mechanism for conits to be created. Because the application (whose
source code is of course is fixed) must specify the weight of each
update to each conit, this requires that the weight of an update can
be calculated. Donkervliet's master's thesis \citationneeded explored
dynamic conits in the context of massive multiplayer online games,
particularly Minecraft.

\paragraph{Dynamic network tuning}
We expect a rich interplay between the network protocol and a
conit-based replication protocol. We previously mentioned an example
where a UAV or a message ferry could be deployed dynamically to
provide greater throughput in a particular geographical area. Such a
resource could be dispatched if the application signals to a network
controller that it is struggling to enforce conit bounds in a timely
manner.

Network packets, or DTN bundles, could be specially marked as
containing database updates alongside any metadata (such as the weight
of an update to various conits) that could be used by the network for
quality-of-service purposes. Such usage may run contrary to the
conventional wisdom that networking protocols should be agnostic to
the actual content of a message, e.g. routers should be concerned only
with the data in IP packet headers but not the data contained in the
packet. This sort of atypical usage is potentially justified in our
setting because of a heightened requirement to optimize the user of
very scarce networking resources, even at the cost of blurring the
line between the network and application layers. We conjecture that
SDN would be particularly suitable because it is easier to modify or
customize software-defined networking protocols, so that custom
hardware is not required even for extremely specialized networking
needs.

\subsection{Old material}

The definition of \(\epsilon\) evidently requires a more or less
application-specific notion of divergence between replicas of a shared
data object.

Now suppose many disaster-response agencies coordinate with to update
and propagate information about the availability of resources. A client
may want to lookup the number of vehicles of a certain type that are
available to be dispatched within a certain geographic range. We may
stipulate that the value read by a client should always be \(4\) of the
actual number, i.e.~we could measure inconsistency with respect to some
numerical value.

In the last example, the reader may wonder we should tolerate a client
to read a value that is incorrect by 4, when clearly it is better to be
incorrect by 0. Intuitively, the practical benefit of tolerating weaker
values is to tolerate a greater level of imperfection in network
communications. For example, suppose Alice and Bob are individually
authorized to dispatch vehicles from a shared pool. In the event that
they cannot share a message.

Or, would could ask that the the value is a conservative estimate,
possibly lower but not higher than the actual amount. In these examples,
we measure inconsistency in terms of a numerical value.

As a third example,

By varying \(\epsilon\), one can imagine consistency as a continuous
spectrum. In light of the CAP theorem, we should likewise expect that
applications with weaker consistency requirements (high \(\epsilon\))
should provide higher availability, all other things being equal.

Yu and Vahdat explored the CAP tradeoff from this perspective in a
series of papers \cite{2000tact,2000tactalgorithms,10.5555/1251229.1251250,DBLP:conf/icdcs/YuV01,2002tact}
propose a theory of \emph{conits}, a logical unit of data subject to
their three metrics for measuring consistency. By controlling the
threshold of acceptable inconsistency of each conit as a continuous
quantity, applications can exercise precise control the tradeoff between
consistency and performance, trading one for the other in a gradual
fashion.

They built a prototype toolkit called TACT, which allows applications to
specify precisely their desired levels of consistency for each conit. An
interesting aspect of this work is that consistency can be tuned
\emph{dynamically}. This is desirable because one does not know a priori
how much consistency or availability is acceptable.

The biggest question one must answer is the competing goals of
generality and practicality. Generality means providing a general notion
of measuring \(\epsilon\), while practicality means enforcing
consistency in a way that can exploit weakened consistency requirements
to offer better overall performance.

\begin{itemize}
\item
  The tradeoff of CAP is a continuous spectrum between linearizability
  and high-availability. More importantly, it can be tuned in real time.
\item
  TACT captures neither CAP-consistency (i.e.~neither atomic nor
  sequential consistency) nor CAP-availability (read and write requests
  may be delayed indefinitely if the system is unable to enforce
  consistency requirements because of network issues).
\end{itemize}

\begin{comment}
\hypertarget{causal-consistency-1}{%
  \subsection{Causal consistency}\label{causal-consistency-1}}

Causal consistency is that each clients is consistent with a total order
that contains the happened-before relation. It does not put a bound on
divergence between replicas. Violations of causal consistency can
present clients with deeply counterintuitive behavior.

\begin{itemize}
  \tightlist
\item
  In a group messaing application, Alice posts a message and Bob
  replies. On Charlie's device, Bob's reply appears before Alice's
  original message.
\item
  Alice sees a deposit for \$100 made to her bank account and, because
  of this, decides to withdraw \$50. When she refreshes the page, the
  deposit is gone and her account is overdrawn by \(50\). A little while
  later, she refreshes the page and the deposit reappears, but a penalty
  has been assessed for overdrawing her account.
\end{itemize}

In these scenarios, one agent takes an action \emph{in response to} an
event, but other processes observe these causally-related events taking
place in the opposite order. In the first example, Charlie is able to
observe a response to a message he does not see, which does not make
sense to him. In the second example, Alice's observation at one instance
causes her to take an action, but at a later point the cause for her
actions appears to have occurred after her response to it. Both of these
scenarios already violate atomic and sequential consistency because
those models enforce a system-wide total order of events. Happily, they
are also ruled out by causally consistent systems. The advantage of the
causal consistency model is that it rules out this behavior without
sacrificing system availability, as shown below.

Causal consistency enforces a global total order on events that are
\emph{causally related}. Here, causal relationships are estimated very
conservatively: two events are potentially causally if there is some way
that the outcome of one could have influenced another.

\begin{figure}
  \center
  \includegraphics[scale=0.4]{images/causal1.png}
  \caption{A causally consistent, non-sequentially-consistent execution}
\end{figure}

\begin{lemma}
  Sequential consistency implies causal consistency.
\end{lemma}
\begin{proof}
  This is immediate from the definitions. Sequential consistency
  requires all processes to observe the same total order of events,
  where this total order must respect program order. Causal consistency
  only requires processes to agree on events that are potentially
  causally related. Program order is a subset of causal order, so any
  sequential executions also respects causal order.
\end{proof}

However, causal consistency is not nearly as strong as sequential
consistency, as processes do not need to agree on the order of events
with no causal relation between them. This weakness is evident in the
fact that the CAP theorem does not rule out highly available systems
that maintain causal consistency even during network partitions.

\begin{lemma}
  A causally consistent system need not be unavailabile during partitions.
\end{lemma}
\begin{proof}

  Suppose $P_1$ and $P_2$ maintain replicas of a key-value store, as
  before, and suppose they are separated by a partition. The strategy is
  simple: each process immediately handles read requests by reading from
  its local replica, and handles write requests by applying the update
  to its local replica. It is easy to see this leads to causally
  consistent histories. Intuitively, the fact that no information flows
  between the processes also means the events of each process are not
  related by causality, so causality is not violated.  \end{proof}

Note that in this scenario, a client's requests are always routed to the
same processor. If a client's requests can be routed to any node, causal
consistency cannot be maintained without losing availability. One
sometimes says that causal consistency is ``sticky available'' because
clients must stick to the same processor during partitions.

The fact that causal consistency can be maintained during partitions
suggests it is too weak. Indeed, there are no guarantees about the
difference in values for \(x\) and \(y\) across the two replicas.
\end{comment}

\subsection{TACT system model}
\label{tact-system-model}

As in Section \ref{sec:background}, we assume a distributed set of
processes collaborate to maintain local replicas of a shared data object
such as a database. Processes accept read and write requests from
clients to update items, and they communicate with each other to ensure
to ensure that all replicas remain consistent.

However, access to the data store is mediated by a middleware library,
which sits between the local copy of the replica and the client. At a
high level, TACT will allow an operation to take place if it does not
violate user-specific consistency bounds. If allowing an operation to
proceed would violate consistency constraints, the operation blocks
until TACT synchronizes with one or more other remote replicas. The
operation remains blocked until TACT ensures that executing it would
not violate consistency requirements.

\[\textrm{Consistency} = \langle \textrm{Numerical error, \textrm{Order error}, \textrm{Staleness}} \rangle.\]

Processes forward accesses to TACT, which handles commiting them to the
store. TACT may not immediately process the request---instead it may
need to coordinate with other processes to enforce consistency. When
write requests are processed (i.e.~when a response is sent to the
originating client), they are only commited in a \emph{tenative} state.
Tentative writes eventually become fully committed at some point in the
future, but when they are commited, they may be reordered. After
fullying committing, writes are in a total order known to all processes.

\begin{figure}[h]
  \center
  \includegraphics[scale=0.4]{images/TACT Logs.png}
  \caption{Snapshot of two local replicas using TACT}
  \label{fig:tact_logs}
\end{figure}

A write access \(W\) can separately quantify its \emph{numerical weight}
and \emph{order weight} on conit \(F\). Application programmers have
multiple forms of control:

Consistency is enforced by the application by setting bounds on the
consistency of read accesses. The TACT framework then enforces these
consistency levels.


\section{Data Fusion}
\label{sec:data-fusion}

\cite{1999:lucien-datafusion}

Strong consistency models provide the abstraction of an idealized global
truth. In the case of conits, the numerical, commit-order, and real-time
errors are measured with respect to an idealized global state of the
database. This state may not exist on any one replica, but it is the
state each replica would converge to if it were to see all remaining
unseen updates.

We consider distributed applications that receive data from many
different sources, such as from a sensor network (broadly defined). It
will often be the case that some sources of data should be expected to
agree with each other, but they may not. A typical scenario, we want to
integrate these data into a larger model of some kind. Essentially take
a poll, and attempt to synthesize a global picture that agrees as much
as possible with the data reported from the sensor network.

Here, we need a consistency model to measure how successful our attempts
are to synthesize a global image. And to tell us how much our sensors
agree. Ideally, we could use this system to diagnose disagreements
between sensors, identifying sensors that appear to be malfunctioning,
or to detect abberations that necessitate a response.

\subsection{Fusion centers}
\label{fusion-centers}

\subsubsection{Sheaf theory}
\label{sheaf-theory}

\subsubsection{Introduction to presheaves}
\label{introduction-to-presheaves}

\begin{definition}
  A \emph{partially order-indexed family of sets} is a family of sets indexed by a partially-ordered set,
  such that orders between the indices correspond to functions between the sets.
\end{definition}

We can also set \((P, \leq)\) \emph{acts on} the set
\(\{S_i\}_{i \in I}\).

\begin{definition}
  A \emph{semiautomaton} is a monoid paired with a set.
\end{definition}

This is also called a \emph{monoid action} on the set.

\begin{definition}
  A copresheaf is a *category acting on a family of sets*.
\end{definition}

\begin{definition}
  A presheaf is a *category acting covariantly on a family of sets*.
\end{definition}

\subsubsection{Introduction to sheaves}
\label{introduction-to-sheaves}

To be written.

\subsubsection{The consistency radius}
\label{the-consistency-radius}

To be written.

\section{Conclusion}
\label{conclusion}

\label{sec:conclusion}

To be written.

\section*{Bibliography}\label{bibliography}
\addcontentsline{toc}{section}{Bibliography}

\bibliographystyle{abbrv}
\bibliography{bibliography}
\end{document}

%%% Local Variables:
%%% mode: LaTeX
%%% TeX-master: t
%%% End:


\begin{figure}[p]
  \begin{subfigure}[a]{1\textwidth} \center
    \includegraphics[scale=0.4]{images/linear1.png} \caption{A
      linearizable execution. Any choice of linearization works here.}
    \label{fig:linear_example11} \end{subfigure}
  \begin{subfigure}[b]{1\textwidth} \center
    \includegraphics[scale=0.4]{images/nonlinear0.png} \caption{A
      non-linearizable execution. The request to read $y$ returns a
      stale value. } \label{fig:linear_example12} \end{subfigure}
  \caption{A linearizable and non-linearizable execution.}
  \label{fig:linear_example1} \end{figure}

\begin{figure}[p]
  \begin{subfigure}[a]{1\textwidth}
    \center
    \includegraphics[scale=0.4]{images/linearTemplate.png}
    \caption{An execution with read responses left unspecified.}
    \label{fig:nonlinear}
  \end{subfigure}
  \begin{subfigure}[b]{1\textwidth}
    \center
    \includegraphics[scale=0.4]{images/linear3.png}
    \caption{A linearizable execution for which both reads return $1$.}
  \end{subfigure}
  \begin{subfigure}[c]{1\textwidth}
    \center
    \includegraphics[scale=0.4]{images/linear2.png}
    \caption{A linearizable execution for which both reads return $2$.}
  \end{subfigure}
  \caption{Two linearizable executions of the same underlying events that return different responses. Possible linearization points are shown in red.}
  \label{fig:linearization}
\end{figure}

\begin{figure}[p]
  \begin{subfigure}[a]{1\textwidth}
    \center
    \includegraphics[scale=0.4]{images/nonlinear1.png}
    \caption{A nonlinearizable execution with the read access returning disagreeing values. We will see later (Figure \ref{fig:sequential}) that this execution is still sequentially consistent. }
    \label{fig:nonlinear1}
  \end{subfigure}
  \begin{subfigure}[b]{1\textwidth}
    \center
    \includegraphics[scale=0.4]{images/nonlinear2.png}
    \caption{Another nonlinearizable execution with read access values swapped. This execution is not sequentially consistent.}
    \label{fig:nonlinear2}
  \end{subfigure}
  \caption{Two non-linearizable executions of the same events shown in Figure \ref{fig:linearization}.}
  \label{fig:nonlinearizable}
\end{figure}

\begin{figure}
  \begin{subfigure}[a]{1\textwidth}
    \center
    \includegraphics[scale=0.4]{images/sequential1.png}
    \caption{A non-linearizable, sequentially consistent execution.}
    \label{fig:sequential1}
  \end{subfigure}
  \begin{subfigure}[b]{1\textwidth}
    \center
    \includegraphics[scale=0.4]{images/sequential2.png}
    \caption{An equivalent interleaving of \ref{fig:sequential1}.}
    \label{fig:interleaving1}
  \end{subfigure}
  \caption{A sequentially consistent execution and a possible interleaving.}
  \label{fig:sequential}
\end{figure}

\begin{figure}
  \begin{subfigure}[a]{1\textwidth}
    \center
    \includegraphics[scale=0.4]{images/nonsequential1.png}
    \caption{A non-sequentially consistent execution.}
    \label{fig:nonsequential1}
  \end{subfigure}
  \begin{subfigure}[b]{1\textwidth}
    \center
    \includegraphics[scale=0.4]{images/nonsequential_x.png}
    \caption{The sequentially consistent history of $x$.}
    \label{fig:sequentialx}
  \end{subfigure}
  \begin{subfigure}[b]{1\textwidth}
    \center
    \includegraphics[scale=0.4]{images/nonsequential_y.png}
    \caption{The sequentially consistent history of $y$.}
    \label{fig:sequentialy}
  \end{subfigure}
  \caption{A non-sequentially consistent execution with sequentially-consistent executions at each variable.}
  \label{fig:nonsequential}
\end{figure}

%%% Local Variables:
%%% mode: LaTeX
%%% TeX-master: t
%%% End:
