% !TeX document-id = {beb7ced9-b3cd-42b2-b16a-3ed3c633a1d9}
\documentclass[]             % options: RDPonly, coveronly, nocover
{NASA}                       %   plus standard article class options
%\DeclareRobustCommand{\mmodels}{\mathrel{|}\joinrel\Relbar}

\usepackage[utf8]{inputenc}
\usepackage{setspace}
\usepackage{csquotes}
\usepackage{hyperref}
\usepackage{amsmath, amssymb, amscd, amsthm, amsfonts}
\usepackage{mathtools}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{amsthm}
\usepackage[english]{babel}
\usepackage{stmaryrd}
\usepackage{proof}
\usepackage{tikz-cd}
\tikzcdset{scale cd/.style={every label/.append style={scale=#1},
    cells={nodes={scale=#1}}}}
% Added for subfigures
\usepackage{caption}
\usepackage{subcaption}
\usepackage{afterpage}
\usepackage{comment}
\usepackage{rotating}%sidewaysfigure
\usepackage{pdflscape}%alt to sidewaysfigure

\newtheorem{theorem}{Theorem}[section]
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{lemma}[theorem]{Lemma}
\theoremstyle{definition}
\newtheorem{example}[theorem]{Example}
\newtheorem{definition}[theorem]{Definition}
\include{macros.tex}

% Globally redefine pgfpicture to use \Large fonts
\let\origpgfpicture=\pgfpicture
\def\pgfpicture{\origpgfpicture\small}

% Try loading this package to prevent so much hyphenation
% as recommended by https://stackoverflow.com/questions/1609837/latex-breaking-up-too-many-words
\usepackage{microtype}

\title{Distributed Systems Challenges in Wildland Firefighting Environments}

\author{Lawrence Dunn and Alwyn E. Goodloe}

\AuthorAffiliation{Lawrence Dunn \\ Department of Computer and Information
  Science \\ University of Pennsylvania \\ Philadelphia, PA \\ Alwyn Goodloe\\                                          % for cover page
  NASA Langley Research Center, Hampton, Virginia
}
\NasaCenter{Langley Research Center\\Hampton, Virginia 23681-2199}
\Type{TM}                    % TM, TP, CR, CP, SP, TT
\SubjectCategory{64}         % two digit number
\LNumber{XXXXX}              % Langley L-number
\Number{XXXXXX}              % Report number
\Month{12}                   % two digit number
\Year{2022}                  % four digit number
\SubjectTerms{Distributed Systems, Formal Methods, Logic, }     % 4-5 comma separated words
\Pages{46}                   % all the pages from the front to back covers
\DatesCovered{}              % 10/2000--9/2002
\ContractNumber{}            % NAS1-12345
\GrantNumber{}               % NAG1-1234
\ProgramElementNumber{}
\ProjectNumber{}             % NCC1-123
\TaskNumber{}                % Task 123
\WorkUnitNumber{}            % 123-45-67-89
\SupplementaryNotes{}
\Acknowledgment{The work was conducted during a summer internship at the NASA Langley Research Center in the Safety-Critical Avionics Systems Branch focusing on distributed computing  issues arising in the Safety Demonstrator challenge in the NASA Aeronautics System Wide Safety (SWS) program.}

%Added for Pandoc
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}


\abstract{The System Wide Safety (SWS) program has been investigating
  how crewed and uncrewed aircraft can safely operate in shared
   airspace. Enforcing safety requirements for distributed agents
  requires coordination by passing messages over a communication
  network. Unfortunately, the operational environment will not admit
  reliable high-bandwidth communication between all agents,
  introducing theoretical and practical obstructions to global
  consistency that make it more difficult to maintain safety-related
  invariants. Taking disaster response scenarios, particularly
  wildfire suppression, as a motivating use case, this self-contained
  memo discusses some of the distributed systems challenges involved
  in system-wide safety through a pragmatic lens. We survey topics
  ranging from consistency models and network architectures to data
  replication and data fusion, in each case focusing on the practical
  relevance of topics in the literature to the sorts of scenarios and
  challenges we expect from our use case.  }

\begin{document}
\newpage
\setcounter{tocdepth}{2}
\tableofcontents
\newpage

\section{Introduction}
\label{sec:introduction}
Civil aviation has traditionally focused primarily on the efficient
and safe transportation of people and goods via the airspace. Despite
inherent risks, the application of sound engineering practices and
conservative operating procedures has made flying the safest mode of
transport today. Now, the industry's strong requirements for safety
make it difficult to integrate unmanned vehicles into the airspace,
accomodate emerging applications, and keep pace with significant
recent growth in commercial aviation. To that end, the NASA
Aeronautics' Airspace Operations and Safety Program (AOSP) has
initiated the System Wide Safety (SWS) project to investigate
technologies and methods to enable crewed and uncrewed aircraft to
safely operate in shared airspace.

This memo surveys topics in computing that are relevant to maintaining
system-wide safety across large, physically distributed data and
communication systems. It is intended to be self-contained and
accessible to a technical audience without a deep background in
distributed systems. Our primary motivating use cases come from civil
emergency response scenarios, especially wildfire suppression and
hurricane relief. These were chosen primarily for three
reasons. First, improved technology for wildfire suppression,
especially related to communications and data sharing, is frequently
cited as a national priority \cite{pcast2023}.  Second, the rules for
operating in the US national airspace are typically relaxed during
natural disasters and relief efforts, so this is a suitable setting
for testing new technologies. Finally, this setting is an excellent
microcosm for the sorts of general challenges faced by other,
non-emergency applications.

One theme visited throughout this document is \emph{continuity} in the
sense considered by topology.\footnote{For an introductory textbook
  see \cite{mendelson2012introduction}.}  The systems we examine must
function under harsh operating conditions that limit their
performance. For example, wireless communication is less reliable
during severe weather. To design a system whose behavior and
performance is predictable---this is clearly a prerequisite for
safety---it must flexible enough to perform reliably under a wide
range of adverse conditions. In other words, the behavior of a safe
system should in some sense be a \emph{continuous} function of its
inputs and environment. Achieving this sort of robust design is
challenging because distributed systems designers must navigate
delicate tradeoffs between competing objectives. At a high level,
these tradeoffs stem from an inherent tension between designing a
system that handles user requests quickly and one that prioritizes a
strong level of global consistency between system components.

The key insight the reader should take away from this document is that
achieving system-wide safety is not solely a matter of improving
communications hardware and physical infrastructure. It is also in
large part a computer science and software problem concerned with the
high-level applications that execute on top of the communications
infrastructure. These software systems must be engineered to maintain
a common operating picture between distributed users while making
efficient use of networking and compute resources in a dynamic and
even adversarial environment.

\subsection{Summaries of the sections}
\label{ssec:summaries-of-the-sections}
Section \ref{sec:disaster-response} opens with a practical overview of
disaster response and some of the computing challenges encountered in
this setting. The heavily disruptive nature of the communications
network in these environments invariably raises issues fundamental to
the science of distributed systems. Real-world examples from disaster
response scenarios are presented that demonstrate how these challenges
affect system-wide safety.

Section \ref{sec:background} summarizes fundamental concepts and
mechanisms used in distributed systems, culminating in the classic
``CAP'' theorem for both the linearizable and sequential consistency
models (Theorems \ref{thm:cap} and \ref{thm:cap-sequential}). CAP is
considered a ``negative'' result, as it proves that a distributed
system cannot guarantee strong consistency and remain available to
users when the communication network is disconnected. The practical
implication of the theorem is that agents in emergency response
environments will always operate with incomplete information about the
global system.  While the CAP theorem is often presented as a kind of
unfortunate prohibition, it merely highlights a general kind of
tradeoff. Furthermore, real-world systems often exhibit a kind of
``locality'' that mitigates some of the constraints implied by the
theorem.

Section \ref{sec:tsae} presents Golding's Timestamped Anti-Entropy
(TSAE) protocol. TSAE provides a fault-tolerant message propagation
mechanism that ensures messages are eventually delivered to all
parties. This implements a weak (eventual) consistency model,
representing an optimistic approach to consistency where replicas of
shared state are allowed to diverge temporarily and have their
differences reconciled periodically. However, it has the downside that
it cannot enforce bounds on how far apart any two replicas may diverge
before this occurs. Therefore, users are not provided any guarantees
limiting how far apart a data item observed by the user may be from
its ``true'' value.

Section \ref{sec:continuous-consistency} describes a TSAE-based
replication mechanism suitable for networks with frequent disruptions,
but where it is also important to measure and control how far apart
replicas may diverge at any moment. This uses the theory of
\emph{conits} (short for ``consistency unit'') developed by Yu and
Vahdat \cite{2002tact}, which provides a \emph{continuous} consistency
model that balances the competing objectives of consistency and
availability in a quantifiable way. The idea is that applications can
tolerate some level of inconsistency between replicas of a data item,
as long as the divergence remains less than some defined upper
bound. Using the conit framework allows applications to define units
of replicated state of interest, enforce policies limiting
inconsistency between their replicas, and adapt these policies
dynamically in response to the environment. By combining the enforced
guarantees of strong consistency models with the flexbility and
resilience of weak consistency, this model is suitable where the
tradeoffs implied by the CAP theorem may need to be carefully
calibrated to support both performance and safety.


\begin{comment}
We conclude in Section \ref{sec:conclusion} by recapping some of the
main themes in this document and highlighting areas for further
investigation. Ultimately, building distributed systems requires
design decisions tailored to the environment and application. We
expect that many of these decisions will involve a combination of
simulation and real-world testing.
\end{comment}

\section{Coordination Challenges in Disaster Response}
\label{sec:disaster-response}
This section explores key aspects of disaster response, particularly
firefighting, that shape the focus of this document. We highlight how
real-world environments create fundamental challenges that require
solutions based on distributed computing principles. Even with the
best communications technologies, core issues arise when distributed
agents need to coordinate their actions across wide areas.

Disaster response settings, like wildfire suppression or hurricane
relief, are marked by systemic communications challenges. A 2023
report by the President’s Council of Advisors on Science and
Technology (PCAST) highlights the need to address ``the
vulnerabilities and shortfalls in wildland firefighter communications,
connectivity, and technology interoperability'' as its top
recommendation for wildland firefighting modernization
\cite{pcast2023}. Many of these vulnerabilities and shortfalls stem
from factors inherent to disaster response: remote locations,
difficult terrain, damaged infrastructure, harsh weather, and limited
power, to name a few.

Field agents often face high message loss, distorted signals, and
unpredictable delays in communication. A cautious approach suggests
preparing for the worst performance at critical times---network
failures often coincide with the sorts of conditions that demand
urgent, reliable contact. Disasters often damage and degrade the
communications infrastructure, which is accompanied by a sudden surge
in user demand that can overwhelm a network completely. This was
starkly evident in the immediate aftermath of the September
$11^\textrm{th}$ attacks, when sudden user demand and severed trunk
cables crippled New York public and private communication networks,
including dedicated networks for first responders
\cite{2011:Reardon}. These failures later became the impetus for the
creation of FirstNet \cite{2021:firstnet, 2021:firstnet2}, a national
public safety broadband network (NPSBN).

Unreliable networks make coordinating distributed agents a significant
challenge. Coherent decision-making and coordinated action require
consistency, meaning agreement on the data shared between agents. We
define consistency more precisely in Section \ref{sec:background}, but
the idea is clear: it is critical for everyone to agree which
firetrucks should respond to which areas, where helicopters should
land, which tasks should be prioritized, or which radio frequencies
are in use. Achieving stronger standards for consistency requires
sending more information in a shorter time frame, which places a
heavier strain on the network. When a communications link is slow,
system components may have to pause and wait before agreement can be
reached, diminishing the efficacy of the system. To avoid waiting in
such scenarios, standards for consistency may have to be relaxed,
meaning distributed agents have less agreement, which comes with its
own challenges. In sum, there is an inherent tension between
consistency and responsiveness.

\subsection{Communication and User Safety}
\label{ssec:communication-and-safety}
We turn our attention to the implications of the
consistency/responsiveness tradeoff from a user safety
perspective. Operational safety depends on agents quickly gathering
and responding to information about their environment. This
information is relayed through communication networks, so poor
communication becomes a safety problem. When communication falters,
agents face a difficult choice: either wait for more information
before acting, or act now with incomplete knowledge. Both inaction and
uninformed action carry risks. This dilemma is closely related to a
fundamental computer science principle known as the safety/liveness
tradeoff.

\begin{figure}
  \centering
  \includegraphics[scale=0.4]{images/dc10.jpg}
  \caption{A DC-10 airtanker, rated for 9,400 gallons, drops retardant
    above Greer, Arizona. Image source: Kari Greer/US Forest Service.}\label{fig:airtanker}
\end{figure}
% TODO: How to cite picture?
% https://www.flickr.com/photos/apachesitgreavesnf/5837741382
% Also appears at https://www.nifc.gov/resources/aircraft/airtankers

\begin{figure}
  \centering
  \includegraphics[scale=0.15]{images/forestfire-videox-scaled.jpg}
  \caption{Screenshot of a firefighter using TAK, where the left
    panel shows a map and the right is a video stream from an air
    vehicle. Image source: Andreas ``AJ'' Johansson}\label{fig:atak}
\end{figure}
% TODO: How to cite picture?
%https://www.civtak.org/2020/08/04/tak-used-in-ca-firefighting-w-aircraft-video/

For example, consider the use of firefighting airtankers, particularly
Very Large Airtankers (VLATs), which can carry over 8,000 gallons of
water or fire retardant \cite{2019:airtankerops}
(Figure \ref{fig:airtanker}). The largest VLATs can drop more than
20,000 gallons---about 170,000 pounds' worth---in a single pass. In
the U.S., these drops are typically made from just 250 feet above the
tree canopy \cite{2019:airtankerops}, and sometimes lower in
practice. This sort of maneuver can easily crush a ground vehicle
\cite{2019:stickney}. In 2018, a firefighter was killed, and three
others were injured, when an unexpectedly powerful drop from a Boeing
747-400 Supertanker knocked down an 87-foot Douglas Fir tree
\cite{2018:calfire}.

Improving firefighter communications can be expected to lead to better
safety outcomes. One such improvement is the through the use of
applications like TAK, the Team Awareness Kit, developed by the
U.S. military in 2010 and later released in a civilian
version. Wildland firefighters are increasingly using TAK, extended
with aftermarket plugins, on ordinary cell phones to coordinate their
activities in the field (Figure \ref{fig:atak}). A key application of
this tool could be tracking the real-time GPS coordinates of
firefighters for safety monitoring.

Given the risks of VLAT drops, a seemingly reasonable safety measure
might be to disallow drops unless a VLAT's computers have up-to-date
information about the location of ground personnel. Unfortunately,
system-wide safety is not so easily achieved, as the proposed measure
is precisely the sort of thing subject to the safety/liveness
tradeoff. Here, it is important to recognize a linguistic nuance: in
the context of distributed systems, ``safety'' refers to a specific
type of system property. The concept is not inherently related to the
safety of people. A \emph{safety} property is a prohibition that stops
a system from taking an action that might be ``bad'' in some way. Here
is an exemplary safety property for the example above:
\begin{quote}
  $\Psafe$: Ground agents are known to be at least
  100 feet outside the drop zone, and this information is current to
  within 30 seconds, or airtankers will not perform a drop.
\end{quote}
By contrast, a \emph{liveness} property demands some kind of action
from a system, usually one that is ``good'' in some way. A
characteristic of liveness properties is that they place an upper
bound on the allowable delay of something. An exemplary liveness
property for our scenario might be the following:
\begin{quote}
  $\Plive$: A VLAT on the ground will take off and
  perform a drop within 20 minutes of receiving a request from an
  incident commander. \footnote{The Chief of Flight Operations for Cal
    Fire cited 20 minutes as an upper bound on the response time for
    aerial firefighting units within designated responsibility areas
    in an interview with PBS \cite{2021:aerialfirefighting}.}
\end{quote} Note that $\Plive$
is a liveness property, not a safety property in the narrow technical
sense, but it impacts human safety: it might be critical for VLATs to
perform drops quickly if a wildfire is threatening the safety of
ground personnel.

Safety and liveness are frequently dual mandates that cannot be
guaranteed simultaneously. Such is the case in our example: though
$\Psafe$ and $\Plive$ are both desirable, certain situations will
force decision makers to prefer one over the other. Consider the fact
that the wildland firefighting environment is frequently GPS-denied.
Heavy smoke, multipath effects, and so on can easily prevent a
consumer-grade cellphone from obtaining reliable GPS
coordinates. Additionally, factors like a damaged radio tower or
environmental obstructions like a tall mountain can prevent
communications between the air and ground. Such conditions would
prevent a VLAT's computers from knowing the locations of ground
agents, which immediately presents a dilemma: should the crew proceed
without knowing the locations of ground personnel, maintaining
$\Plive$ at the cost of $\Psafe$, or should it be cautious and wait
for more information, maintaining $\Psafe$ at the cost of $\Plive$?
There is no simple answer, with either choice presenting a downside
with respect to the broader goal of system-wide safety.

Besides the safety/liveness tradeoff, the previous example exhibits
two other themes important in distributed systems, both of which will
be explored further in this document. The first is the
\emph{epistomological} nature---concerned with what information is
\emph{known} by \emph{whom}---of reasoning about distributed
systems. This aspect is reflected in wording of $\Psafe$ in VLAT
example: Ground agents are known (by the VLAT's computers) to be
outside of a dangerous area. This situation requires a deeper and more
sophisticated analysis than one simply considering what is
true. Mathematically, the logic of distributed agents is not the
ordinary propositional logic but the modal logic S5, which extends
propositional logic with additional axioms governing
knowledge.\footnote{The application of S5 to reason about distributed
  systems is the topic of \cite{kshemkalyani_singhal_2008}, Chapter
  8.} Distributing knowledge requires communication between agents
over a period of time over the network, which is not instantaneous and
reliable, and it is from these imperfections that the safety/liveness
tradeoff arises.

The second aspect exhibited above, albeit negatively, is that of
\emph{continuity}. A continuous system can flexibly adopt to its
environment, but a discontinuous system is rigid and may exhibit
suddenly different behavior in response to only small changes in the
environment, such as a transient network failure. The properties
$\Psafe$ and $\Plive$ exhibit a stark lack of continuity because they
are inflexible, all-or-nothing propositions. Suppose that agents are
known to be $500$ feet outside the drop zone, but the information is
only current to within 31 seconds---this extra second technically
violates $\Psafe$, though it should be inferrable that the ground
agents are well away from danger. In particular, this example
highlights that system-wide safety is more of a quantitative concept
than a Boolean (true-or-false) one. A distributed system in a
network-challenged environment should exhibit smoothly varying
properties in response to its inputs. Ideally one can ``tune'' the
system's properties for the particulars of its environment at any
moment. The technical aspects of this theme are the focus of Section
\ref{sec:continuous-consistency}.

\subsection{Communication Patterns in the Field}
\label{ssec:communication-patterns}
We now consider some of the communication patterns that occur in
wildland firefighting. Readers may be surprised to learn that the
state of the art is somewhat primitive, largely due to the sparse
permanent communications infrastructure that exists in this
setting. This makes wildfires an interesting and generalizable example
for other kinds of civil disaster environments where the network is
unreliable.

One important concept to draw attention to is a kind of ``geospatial
locality of reference'' that system designers should consider. By
this, we mean the concomitance of two observations which, while not
guaranteed rules, are approximately true in many circumstances. The
first observation states that nearby agents have aligned interests:
\begin{quote}
  $\textbf{O}_1$: Agents with the most urgent need to coordinate their
  actions will usually be located closer together and require similar
  kinds of information.
\end{quote}
The second observation states that nearby agents have more reliable
communications:
\begin{quote}
  $\textbf{O}_2$: Agents that are located closer together generally
  enjoy more reliable communications between them than agents that are
  far apart. Conversely, information that travels long distances tends to
  be delayed or degrade in quality.
\end{quote}

These related observations are what is meant by simply the
``locality'' principle. Locality is a crucial factor to analyze
because, as presented in Section \ref{sec:background}, there are major
theoretical and practical limits to how well agents can coordinate
\emph{globally}, meaning with all agents knowing and agreeing on
everything. To the extent the system exhibits locality, coordination
can be achieved using more efficient short-range communication than
less efficient long-range communication. Here, ``efficient'' should be
read broadly, measured with respect to things like battery life,
message delay, reliability, cost-effectiveness, equipment weight, and
so on. This raises the question of how to most efficiently utilize
network resources to achieve adequate levels of consistency. Aspects
of this question are revisited in Section
\ref{sec:continuous-consistency}, in the context of a framework for
weighing the relative importance of updates.

\subsubsection{Communication on the Ground}
\label{sssec:ground-communication}
In the field, communication between firefighters and other agents is
often facilitated by handheld (analog) land-mobile radio (LMR). These
radios are inherently limited in their battery life, bandwidth,
effective range, and ability to work around environmental factors like
foliage and smoke.

As an alternative to using a radio, it is common for wildland
firefighters in the field simply to shout commands and notifications
to nearby personnel. This exhibits the locality principle: a
substantial amount of communication occurs directly between nearby
firefighters working on related tasks that can communicate without
network infrastructure. In a future environment where agents might be
equipped with body-worn sensors and or even some form of heads-up
display (HUD), this sort of low-range local communication might be
facilitated by relatively inexpensive, low-power technologies such as
Bluetooth or mesh Wi-Fi, without the need for more sophisticated (and
heavy) equipment.

Communication over a long distance requires infrastructural support,
such as the use of cell towers and repeater stations. Typically,
disaster response environments have scarce permanent infrastructure:
in a wildland fire setting, perhaps a few repeaters mounted to a
nearby watch tower. Ad-hoc infrastructure, such as Cells On Wheels
(COWs) or Cells on Light Trucks (COLTs)---i.e. portable cellular
towers---can sometimes be deployed on an as-needed basis if the
location allows for it. Similar kinds of equipment can also be mounted
to backpacks and carried into the field. A common issue is making sure
that all equipment is properly configured, for instance that radios
are listening on the correct frequencies. Configuration is especially
critical when different agencies and groups need to
interoperate---another problem highlighted during the September
$11^\textrm{th}$ attacks.

\begin{figure}[t]
  \centering
  \includegraphics[scale=0.085]{images/ironside.jpg}
  \caption{The Ironside Mountain lookout and radio repeater station,
    shown here with protective foil on August $10^\textrm{th}$, 2015
    during the 2015 River Complex fire. This particular fire burned
    77,077 acres over 77 days.}
  \label{fig:ironside}
\end{figure}
% TODO: How to cite picture?
%https://web.archive.org/web/20150923190323/http://inciweb.nwcg.gov/incident/photograph/4431/44/45122/

Use of centralized infrastructure comes with the potential for
widespread failure when the infrastructure breaks down. For example,
in California, the Ironside Mountain lookout/repeater station (seen in
Figure \ref{fig:ironside}) was destroyed during the 2021 Monument
Fire, which burned approximately 223,124 acres over 88 days
\cite{2021:monumentfire}. The Ironside Mountain station had strategic
importance, being located on a tall ridge. According to a video blog
from a volunteer firefighter involved in the incident, its loss
prevented communication between operators on different sides of the
ridge, in networking parlance creating a \emph{partition} that lasted
until crews could ascend the ridge to deploy a temporary station:
\begin{quote}
  ``When {[}the Ironside Mountain lookout station{]} burned down the
  radio repeater went with it. And so communications were lost across
  the fire\ldots{} one side of the fire couldn't talk to the other
  side\ldots.  So it was kind of a critical job to get that road
  cleared so that the radio crews could go back up there and set up a
  temporary radio tower.'' \cite{2022:mechfire}% See also https://web.archive.org/web/20220809061927/https://www.youtube.com/watch?v=4F2dDKMgAME
\end{quote}
A scenario where communication between two groups is completely
severed is exactly the sort of thing considered by the CAP theorem in
Section \ref{sec:background}.

\paragraph{Ground vehicles}
Large numbers of ground vehicles---sometimes on the order of 100
during a major response---are involved in wildfire
suppression. Various firetrucks, bulldozers and similar vehicles are
commonly used to control the landscape and perimeter of the fire. An
advantage of vehicles is that they can carry heavier and higher-power
communications equipment than a human. For instance, a vehicle could
be equipped with a BGAN or VSAT satellite terminal to maintain a
connection back to a central location. Additionally equipping the
vehicle with something like a Wi-Fi or cellular base station using the
satellite connection as backhaul would let the vehicle act as a bridge
between agents in the field and central coordinators such as incident
commanders or 911 dispatchers.

\subsubsection{Communication in the Air}
Wildland firefighting increasingly involves the use of helicopters and
fixed wing aircraft. Civil aviation has traditionally employed simpler
communication patterns than this use case demands. For instance,
aircraft equipped with Automatic Dependent Surveillance-Broadcast
(ADS-B) monitor their location using GPS and periodically broadcast
this information to air traffic controllers and nearby aircraft. This
sort of scheme has worked well in traditional applications, where
pilots typically only monitor the general locations of a few nearby
aircraft. The locality principle is exhibited here, too: aircraft have
the highest need to coordinate when they are physically close and
therefore in range of each other's ADS-B broadcasts.

In our setting, a large number or aircraft, easily on the order of 10 or
more, may need to operate in a small area, near complex terrain,
during adverse conditions, often at low altitude. In other words, the
demands are many and the margins for error are small. This sort of use
case calls for more sophisticated coordination schemes between
airborne and ground-based elements than solutions like ADS-B provide
by themselves.

As aircraft generally have better line-of-site to ground crews than
ground crews have to each other, firefighters sometimes relay messages
to air-based units over the radio, which in turn is relayed back down
to other ground units. The locality principle comes into play for this
sort of message relaying scheme, but in the negative direction:
relaying allows knowledge to travel farther but requires more resources and effort,
and the extended reach comes at the cost of introducing delays and
possible degradation of message quality, as in the classic game of
``telephone.'' Hence, this mode of communication has generally been reserved for
more critical information.

The Communications Program of the Civil Air Patrol (a civilian
auxiliary of the U.S. Air Force) is sometimes deployed to provide
communications for firefighters on the ground using airplane-mounted
radio repeaters. In this future, this sort of service could be
provided autonomously by portable infrastructure mounted to unmanned
aerial vehicles (UAVs), which might perform additional functions such
as tracking the fire perimeter.

In future environments, we envision resilient networks formed from
heterogeneous collections of smaller networks, incorporating various
communication technologies such as digital radios, Wi-Fi, 4G/LTE, and
satellite communications. Communications in the field may incorporate
aspects of mesh networks and mobile ad-hoc networks (MANETs). Given
the environmental challenges, we assume that two agents will often
only have intermittent end-to-end connectivity, if any. Facilitating
communication through such a dynamic and chaotic mobile network calls
for a disruption-tolerant networking (DTN) architecture, which
provides a custody transfer and store-carry-forward model that is
resilient to disruption \cite{2021:intro-dtn}. The exact form of such
a network remains a question for future investigation.

\subsection{Data Collection and Processing}
\label{ssec:data-collection}
Perhaps the most universally acknowledged expectation for future
disaster response environments is a heavy reliance on data
gathered from both humans and sensors. Besides improvements to
communications that facilitate information sharing, we expect advances
in machine intelligence to greatly influence how this data is
handled.

Agents in disaster response environments will be both producers and
consumers of data, and this data will need to processed by humans and
machines in ways that agents can readily make sense of to support
their decision-making. We list just some of the possible sources and
types of pertinent data:
\begin{itemize}
\item Free-form communication, especially real-time or recorded voice messages
  broadcast to many agents at once, which may need to be processed by
  machines to extract the most pertinent information into a more
  actionable format
\item The exact or estimated location of victims, firefighters,
  vehicles, hazards, and so on displayed on applications like TAK
\item Medical information gathered from victims, perhaps stored in and
  collected from electronic triage tags \cite{2009:triagetag}
\item Data about current and predicted fire behavior gathered from
  systems like the Fire Integrated Real-time Intelligence System
  (FIRIS) or NASA's Fire Information for Resource Management System
  (FIRMS)
\item Weather data from the National Weather Service
\item Topographic information about the terrain, highlighting for
  instance the location of rivers and roads that could form a fire
  control line
\item Planned escape routes, rendezvous points, safety zones, and
  landing zones
\item Availability and dispatching of assets, e.g.~ambulances,
  airtankers, or crews on standby, such as the prototype application
  considered by Monares et al. \cite{2011:monares}
\end{itemize}
In a perfect environment, such information would be shared with all
necessary agents in whole and instantly. In reality, agents will be
presented with information that is sometimes incomplete, out of date,
or contradictory---all problems that are further exacerbated by an
unreliable network. A competing concern is that the information
presented will be \emph{overcomplete}, filled with petty details that
distract agents from their important tasks.

In some ways, future systems for disaster response will bear
resemblence to future systems for warfighting, such as the conceptual
\emph{Internet of Battle Things} (IoBT) \cite{2016:iobt}. To quote
from that paper, agents ``under extreme cognitive and physical
stress'' will be subject to a highly dynamic and dangerous
environment. Various kinds of technology will assist humans by
providing data to support sensemaking, but a contraindicating concern
will be flooding agents with a ``massive, complex, confusing, and
potentially deceptive ocean of information.'' To avoid ``swimming in
sensors and drowning in data'' \cite{2010:magnuson}:
\begin{quote}
``Humans seek well-formed, reasonably-sized, essential information
  that is highly relevant to their cognitive needs, such as effective
  indications and warnings that pertain to their current situation and
  mission.'' \cite{2016:iobt}
\end{quote}

The field of Human-Computer Interaction (HCI) is concerned with
``design, evaluation and implementation of user interfaces for
computer systems that are receptive to the user's needs and habits.''
\cite{2009:hci-definition} As emergency control becomes more
data-driven, particularly in hubs like dispatch centers that aggregate
diverse streams of information, the challenge of ensuring users can
interact effectively with these systems will become increasingly
important. We propose that researchers in HCI take up the question of
how agents under stress can process and respond to the flood of
complex information they may face. Relevant topics for exploration
include structuring interfaces to avoid cognitive overload and
facilitate intuitive control. Attention should also be given to
helping agents avoid subtle but critical mistakes, such as dispatching
resources to the wrong location---for instance, \emph{S. Example Rd.}
instead of \emph{N. Example Rd}.


\subsubsection{Adversarial Behavior}
One feature of the Internet of Battle Things worth highlighting is
``the adversarial nature of the environment.'' This feature is common
also to disaster environments, whether resulting incidentally from
``fog of war'' effects or deliberately caused by malicious actors
seeking to exploit a civil disaster. Section
\ref{ssec:communication-patterns} cited a real-world example of a
critical communications station destroyed by wildfire, perhaps
comparable to an attack by enemy forces.

There is a growing trend in disaster response to rely on
``crowdsourced'' information, where public safety officials process
reports from the general public over non-traditional channels like
social media. However, a significant vulnerability of crowdsourcing
information is the potential for confusing of contradictory reports,
which can resemble intentional deception. Rumors frequently plague
disaster relief environments, which are quite susceptible to
misinformation. For instance, during Hurricane Harvey in 2017, there
were unconfirmed rumors of shots being fired at volunteer rescuers
\cite{2017:cajun-navy-rumors}. Tracing reports back to their source is
often difficult. Even fully malicious activity like
``swatting''---placing a fake 911 call to cause a large police
response---is often observed in public safety. Whether misinformation
is spread with malicious intent or through well-meaning confusion, the
proliferation of false information in this chaotic environment can
have adversarial effects. This should be anticipated as part of a
careful approach to modernizing systems in this space.

\subsubsection{Allocation of Network Resources}
\label{sssec:allocation-of-network-resource}
Communications in disaster response environments might even be less
reliable than in the battlefield (setting aside offensive behavior
like signal jamming), requiring a greater emphasis on the preservation
of scarce network resources. For instance, a group of volunteer
firefighters would have fewer resources than a tactical military unit,
relying on commercial off-the-shelf (COTS) equipment rather than best
in class hardware like sophisticated handheld satellite links. High
bandwidth channels will often be in short supply, while adverse
conditions like inclement weather are assumed.

Given the heavy reliance on data and the scarcity of reliable
communication channels, we expect a complex interaction between the
high-level needs of distributed applications (e.g. an application for
sharing real time weather data) and low-level concerns about network
resources. This is because only the applications have enough
information to determine which data is the most important and must be
shared with whom first, which has ramifications for network-level
mechanisms designed to prioritize important messages.

There is a widely accepted wisdom in computing---the end-to-end
principle \cite{1984:end-to-end}---which suggests roughly that
applications should not make assumptions about the network, and that
the network should be relatively agnostic to high-level application
logic. However, in natural disaster environments where resources are
scarce and reliable communication is critical, these subsystems may
need to be more tightly integrated in how they influence each other to
achieve the best performance. This approach would not contravene the
end-to-end principle, but would involve carefully considering its
application in this relatively extreme context.

Consider, say 5 to 10 years in the future, a centralized data fusion
application that running in an edge data center.\footnote{An
  \emph{edge} data center is one located closer to a network's edge,
  nearer to users, to provide low-latency communications for
  time-sensitive applications. Edge centers support applications that
  require significant amounts of information processing---enough that
  the application must be hosted in a datacenter, where compute
  resources can be scaled dynamically, rather than colocated with
  users where resources are limited.} This application could detect
critical events like a fire crossing a control line (a phenomenon
called \emph{slopover}) and alert ground responders. It might also
warn responders who have strayed too far from an escape route or
safety zone. These are high-priority notifications, so it would be
worthwhile to allocate scarce network resources to convey them to the
relevant parties in real-time.

On the other hand, while it may be beneficial for each firefighter to
have real-time information about the location of every other
firefighter, this may not always be critical. If transmitting this
data strains the network, then perhaps only the general location of
nearby teams should be sent. If the network is extremely constrained,
communication may be restricted to only information strictly relevant
to preserving life to ensure swift, reliable delivery. Thus, network
allocation is a dynamic calculation influenced both by the criticality
of the information (which is determined by the application logic) and
the availability of network resources at a particular
location. Network services should provide mechanisms like Quality of
Service (QoS) indicators to allow prioritizing certain
communication. Such mechanisms can be incorporated into a control loop
where applications generate feedback that drives the decision-making
process in lower-level parts of the network. However, simple QoS
mechanisms may not be enough---even the routing protocol of the
network may need to be more specialized to higher-level applications
than in traditional environments.


\section{Introduction to Distributed Systems}
\label{sec:background}
In this section, we distill two core topics in the theory of
distributed systems: causality and timekeeping, along with shared
memory consistency.  Our discussion is primarily informed by the
manuscripts of Coulouris et al.  \cite{coulouris2005distributed} and
Kshemkalyani and Singhal \cite{kshemkalyani_singhal_2008}. We focus on
building applications relevant to the scenarios described in Section
\ref{sec:disaster-response}, aiming to highlight obstacles and
strategies for developing distributed systems that can endure the
delays and disruptions inherent to these communication-challenged
environments. Readers interested in a summary of the highlights may
skip to Section \ref{ssec:background-summary}.

At its core, a distributed system is a network of independent entities
working together to solve problems too complex for any one part to
tackle alone. From a bird’s-eye view, the systems we envision are
intricate and complex, made up of diverse, interconnected elements:
field agents like firefighters, their handheld devices, airborne and
ground vehicles loaded with communication and computing tools, swarms
of sensors and IoT devices, and so on. These decentralized components
operate alongside more centralized hubs: data fusion centers, incident
command (IC) posts, public safety answering points (PSAPs), and
emergency operations centers (EOCs). We imagine these components being
woven together by a patchwork of communication technologies ranging
from analog and digital radios to Bluetooth, Wi-Fi, LTE, 5G, satellite
communications, and ad-hoc mesh networks like Meshtastic\footnote{The
  website of the Meshtastic project describes it as an ``open source,
  off-grid, decentralized, mesh network built to run on affordable,
  low-power devices''. See \url{https://meshtastic.org}.}, DECT-2020
NR\footnote{DECT-2020 NR is a non-cellular 5G
  standard intended for Internet of Things (IoT) operations. For a
  technical discussion see \cite{2022:dect-2020-nr}.}, and others. The
system is thus a dynamic mosaic of elements cooperating to protect
lives and property.

Given the unpredictable nature of the environment and the locality
principle outlined in Section \ref{sec:disaster-response},
communication between edge components---such as field operators---and
centralized hubs is often inconsistent, sometimes available only
intermittently. As a result, information flow is subject to
appreciable delays compared to the timescale of critical events like a
fire shifting direction or a dangerous condition being detected. In
other words, the computing landscape is unmistakably
\emph{distributed}. Singhal and Shivaratri \cite{10.5555/562065}
define a distributed computing system as:
\begin{quote}
  ``A collection of computers that do not share common
  memory or a common physical clock, that communicate by message
  passing over a communication network, and where each computer has
  its own memory and runs its own operating system.''
\end{quote}
This stands in contrast to a centralized computing environment, where
processes can seamlessly share data through common memory, and memory
access times are considered negligible.

For our use cases, message-passing latencies are not only significant
but unpredictable and difficult to control. As a result, we can safely
assume that some parts of the wider system will not have
instantaneous, complete knowledge of every new piece of
information. Only a few components, if any, will be able to maintain a
global systemwide awareness. While deploying additional infrastructure
in the field, such as COWs (Cells on Wheels), can help, the inherently
distributed nature of the environment cannot be fully overcome or
abstracted away. This reality must be embedded in the design of the
software and networking architecture itself. Typically, this manifests
in a shared ``middleware'' layer to coordinate the moving parts,
ensuring they function as a unified system.

The fragmented flow of information presents several challenges for
system designers. Foundationally, one of the primary computer science
problems is that unpredictable latencies make it difficult for
components to maintain a common understanding about the global
sequence of events. For similar reasons, it becomes challenging for
processes to synchronize and agree on shared values, such as the
current number of firetrucks available for dispatch. The remainder of
this section will delve into these issues with more technical depth.

\subsection{Physical Synchronization}
\label{ssec:physical-synchronization}
A lot of challenges in distributed computing could be
straightforwardly overcome if we assume that all participants have
instantaneous access to a common time base, i.e.  synchronized
clocks. Let us explore why fine-grained synchronization is not a
tenable assumption for all purposes.

Physical clocks, especially consumer-grade ones, suffer from
\emph{drift}, which is to say they do not all run at the same
rate. Experienced IT administrators will testify that clocks can also
be prone to misconfiguration. An incorrect date, time, timezone, or
daylight saving time policy setting is a common source of IT issues,
typically causing time-based security mechanisms like TLS
authentication to misbehave. Consider also that devices may spend a
long time sitting unpowered in storage without maintaining an
always-on clock. For these sorts of reasons, we would not want to rest
the integrity of a safety-related system on the assumption that a
numerous and diverse assortment of devices have precisely synchronized
internal clocks.

Clock drift can be corrected for using, for instance, signals from GPS
satellites, but as mentioned in Section \ref{sec:disaster-response},
civil disaster environments are frequently GPS-denied: factors like
mountainous terrain, heavy smoke, and subterranean operations can lead
to errors or block signals entirely. Protocols like the Network Time
Protocol (NTP) \cite{rfc1119} work to bring clocks into
synchronization with respect to an authoritative source. On the public
internet, NTP typically achieves synchronization to within values on the
order of tens of milliseconds \cite{rfc1128}, but it is not clear the
level of synchronization that can be expected from NTP in the sorts of use
cases we have in mind. A field device initialized without internet
access may have no idea what the time is, but it must still operate.

For our use case, what seems most important about time is that
\emph{the future cannot influence the past}
\cite{1989mattern}. Fortunately, this sort of invariant can be
enforced with mechanisms that do not rely on measuring real
time. Below, we explain how so-called logical clocks can be used to
measure and enforce a key relation between events, this being their
\emph{causal precedence}.


\subsection{Message Passing and Causality}
\label{ssec:message-passing}
We model a distributed system abstractly as a fixed set
$\AllProc = \{P_1, P_2, \ldots P_N\}$ of $N$ \emph{processes} which
undergo atomic (indivisible) state changes known as
\emph{events}. Events are divided into three types: internal events,
representing state changes inside a single process, and send and
receive events corresponding to \emph{messages} passed between
processes. Note that this framework is quite abstract and applies to
any kind of packet-based communication technology. To draw out the
core issues surrounding messaging, the diagrams in this section do not
depict any internal events, as they represent state changes that are
not directly viewable to the network or other processes.

Throughout the section, processes and networks are opaque blackboxes,
which concentrates our attention on the ramifications of unpredictable
network latencies. We implicitly assume the reliable asynchronous
network model: when a message is sent between processes, it certainly
arrives at some point in the future, but we cannot say anything about
when or in what order compared to other messages. At times, we
consider the possibility that a message may never arrive. The choice
depends on which networking technology (or which layer of the OSI
networking model \cite{1983:osi-reference-model}) is under
consideration.

Figure \ref{fig:message-latencies} illustrates a series of time
diagrams for messages exchanged between three processes: $P_1$, $P_2$,
and $P_3$. The $x$-axis represents the flow of real time from left to
right, which each process represented by a worldline depicting the
events occurring within that process. Each message, $m$, originates
from a send event $\msend{}$, marking the moment the message is
dispatched across the network by its source process. The delivery of
the message corresponds to a receive event, $\mrecv{}$. For now we
assume messages have a single receiver. We write subscripts on
messages to distinguish them for clarity, but these are not inherent
to the messages themselves.

\begin{figure}[p]
  \setlength\belowcaptionskip{5ex}
  \begin{subfigure}{1\textwidth}
    \centering
    \input{images/pgf/mpEx1.pgf}
    \caption{$P_1$ has a somewhat lower-latency connection to $P_2$ than to $P_3$}
    \label{fig:message-latencies-a}
  \end{subfigure}
  \begin{subfigure}{1\textwidth}
    \centering \input{images/pgf/mpEx2.pgf}
    \caption{$P_1$ has a much lower-latency connection to $P_2$ than to $P_3$}
    \label{fig:message-latencies-b}
  \end{subfigure}
  \caption{Message-passing time diagram examples}
  \label{fig:message-latencies}
\end{figure}

In these diagrams, arrows connect corresponding send and receive
events, with their diagonal slant representing the latencies
experienced as messages traverse the network. Because messages arrive
with varying delays, they might arrive in a different order than they
were sent in. In Figure \ref{fig:message-latencies-a}, for example,
$P_1$ sends messages $m_2$ and $m_4$ sequentially, but $m_4$ arrives
before $m_2$, which might occur if $P_1$ and $P_3$ are separated by a
high-latency communication link. In Figure
\ref{fig:message-latencies-b}, $m_1$ is the first message sent but the
last to be delivered, potentially indicating a deteriorating link
between $P_1$ and $P_3$, perhaps due to increased distance or
inclement weather.

For many applications, it is critical to maintain a natural ordering
of events known as \emph{causal precedence}, or Lamport's ``happens
before'' relation \cite{1978:lamportclocks}. To formalize this, we
first consider the intuitive way to order events within a
\emph{single} process:
\begin{definition}
  For two events $e$ and $e'$ occurring in process $P$, we
  write $e <_{P} e'$ if $e$ occurs before $e'$ in $P$'s
  worldline.
\end{definition}
The previous definition is local to one process and unambiguous, as we
assume events within a process occur at discrete, non-overlapping
points in time. To extend this to a system-wide definition of causal
precedence, we relate corresponding send and receive events, then take
the transitive closure of the relation.

\begin{definition}[Causal precedence]
  \label{def:causalprecedence}
  We define a binary relation $\to$ on the set of events as follows:
  \[e \to e' \iff
  \begin{cases}
    e <_{P} e' \textrm{ for some process $P$}
    \textbf{ or} \\
    e = \msend{} \textrm{ and } e' =\mrecv{}
    \textbf{ or} \\
    \textrm{there is some } e'' \textrm{ such that } e \to e'' \textrm{ and } e'' \to e'
  \end{cases}
  \]
  If $e \to e'$, we say $e$ has \emph{causal precedence over} $e'$ or
  \emph{happens before} $e'$.
\end{definition}
Visually, $e \to e'$ holds when one can put a finger on $e$ in the
diagram and trace a ``path of causality'' to $e'$ by following
worldlines or arrows. We use the notation $e \not \to e'$ to mean
$e \to e'$ does not hold. Note that $e \not \to e'$ does not imply
$e' \to e$.

Incidentally, ``causal precedence'' and ``happens before'' can be
misnomers, as $e \to e'$ only conveys the possibility that information
from $e$ could have influenced $e'$. The requirement to ``not let the
future affect the past'' means that if $e$ might have influenced $e'$,
then applications must avoid situations where, from the user's point
of view, it appears that $e'$ happened before $e$. For example, this
proscription means an application cannot let an ``answer'' appear
before the underlying ``question''.

\begin{example}
  Figure \ref{fig:causal-precedence} illustrates the causal precedence
  relation corresponding to the time diagrams in Figure
  \ref{fig:message-latencies}. For readability we suppress redundant
  transitive arrows. The visual difference between Figures
  \ref{fig:message-latencies-b} and \ref{fig:message-co-b} reflects
  the fact that causal order only captures a logical relationship
  between events, but does not reflect their absolute time or within
  which process they occurred.
\end{example}

Mathematically, causal precedence is an irreflexive partial order:
\emph{irreflexive} because $e \not \to e$ (an event does not precede
itself), and \emph{partial} because any two events $e$ and $e'$ may
satisfy neither $e \to e'$ nor $e' \to e$.

\begin{definition}[Logical synchronicity]
  \label{def:logically-synchronous}
  Events $e$ and $e'$ that are not related by causality are said to be
  \emph{logically synchronous}, denoted $\sync{e}{e'}$.
\end{definition}

Note that logical synchronicity is not usually transitive, meaning it
is possible to have $\sync{e}{e'}$ and $\sync{e'}{e''}$ but not
$\sync{e}{e''}$.  Relations like $\left(\sync{}{}\right)$ that are
reflexive and symmetric but not necessarily transitive are sometimes
called \emph{compatibility relations}.
\begin{example}
  \label{ex:synchronous-intransitive}
  In Figure \ref{fig:message-co-a}, $\sync{\mrecv{1}}{\mrecv{2}}$ and
  $\sync{\mrecv{2}}{\msend{4}}$, but $\mrecv{1} \to \msend{4}$. In
  Figure \ref{fig:message-co-b}, $\msend{1}$ is logically synchronous
  with every event except $\mrecv{1}$, but those other events are
  totally ordered by causality and not synchronous with each other.
\end{example}

\begin{figure}
  \begingroup
  \setlength\belowcaptionskip{4ex}
  \begin{subfigure}{1\textwidth}
    \centering
    \input{images/pgf/mpEx1CO.pgf}
    \caption{Causal precedence among the events in Figure \ref{fig:message-latencies-a}}
    \label{fig:message-co-a}
  \end{subfigure}
  \endgroup
  \begin{subfigure}{1\textwidth}
    \centering
    \input{images/pgf/mpEx2CO.pgf}
    \caption{Causal precedence among the events in Figure \ref{fig:message-latencies-b}}
    \label{fig:message-co-b}
  \end{subfigure}
  \caption{Causal precedence relations for Figure \ref{fig:message-latencies} (transitive arrows not shown)}
  \label{fig:causal-precedence}
\end{figure}

\subsection{Virtual Clocks}
\label{ssec:timestamps}
Distributed applications systematically track causality by employing
\emph{logical} clocks, which measure the logical flow of time by
timestamping events with (possibly sets of) non-negative integers that
are advanced according to certain rules. The three major variants are
scalar, vector, and matrix clocks, which form a kind of
spectrum. Scalar clocks are simple but provide coarse-grained
information, while vector and matrix clocks track increasingly more
precise information at the cost of greater administrative overheads.

All processes timestamp their events using their local clocks. For
each event $e$, let $C(e)$ denote the timestamp attached to that
event. The fundamental property we want to satisfy is that if $e$
causally precedes $e'$, it should receive a lesser timestamp. This is
called the clock consistency condition, or simply the clock condition.

\begin{definition}
  A system of timestamps satisfies the \emph{clock consistency
  condition} if the following monotonicity property holds:
\begin{equation}
  \textrm{For all $e$ and $e'$, if $e \to e'$ then $C(e) < C(e')$} \label{eq:mp}\tag{CC}
\end{equation}
\end{definition}

This notation states that if one event causally precedes another, then
the earlier one receives a lesser timestamp. Somewhat subtly, the
clock condition does \emph{not} imply that we can decide if events are
causally related by comparing timestamps. Rather, it provides a way of
\emph{ruling out} causal precedence. This is seen by expressing
\eqref{eq:mp} in terms of the following logically equivalent
condition.
\begin{equation}
  \textrm{For all $e$ and $e'$, if $C(e') \leq C(e)$ then $e \not\to e'$} \label{eq:mp-conv}\tag{CC$'$}
\end{equation}

If \eqref{eq:mp-conv} holds, we can be sure that a particular sequence
of events $e_1, e_2, e_3\ldots$ does \emph{not} list any $e$ before an
event that might have influenced $e'$ by checking that
$C(e_{i}) \leq C(e_{i+1})$ for all $i$. We emphasize that this does
not give us a definite way to tell whether two events are in fact
causally related. If it is important to determine conclusively whether
events are causally related, one is led to consider the following
stronger requirement from a system of logical timestamps.
\begin{definition}
  An event-timestamping mechanism satisfies the \emph{strong} clock   condition if the following property holds.
  \begin{equation}
    \textrm{For all events $e$ and $e'$, } e \to e' \iff C(e) <
    C(e') \label{eq:sc}\tag{SC}
  \end{equation}
  Note that $\iff$ is notation for ``if and only if,''or logical
  equivalence.
\end{definition}
Scalar clocks, below, satisfy \eqref{eq:mp}, while vector and matrix
clocks satisfy \eqref{eq:sc}.

\subsubsection{Scalar clocks}
\label{sssec:scalar-clocks}
\begin{figure}
  \setlength\belowcaptionskip{5ex}

  \begin{subfigure}{1\textwidth}
    \centering \input{images/pgf/mpEx1Sc.pgf}
    \caption{Figure \ref{fig:message-latencies-a} redepicted with scalar clocks}
    \label{fig:message-latencies-scalar-a}
  \end{subfigure}

  \begin{subfigure}{1\textwidth}
    \centering \input{images/pgf/mpEx2Sc.pgf}
    \caption{Figure \ref{fig:message-latencies-b} redepicted with scalar clocks}
    \label{fig:message-latencies-scalar-b}
  \end{subfigure}

  \caption{Scalar clock examples}
  \label{fig:message-latencies-scalar}
\end{figure}

Lamport's scalar clocks \cite{1978:lamportclocks} require each
process $P$ to maintain a single non-negative scalar value $C$,
initialized to $0$. The clock follows two simple update rules:
\begin{enumerate}
\item[\textbf{R1}:] Before a message is sent or an internal event occurs, $P$
  increments its clock:
  \[C := C + 1.\]
  The new value serves as the event's timestamp and, for messages, is ``piggybacked''
  as part of its metadata.
\item[\textbf{R2}:] When $P$ receives a message with timestamp $C'$, it
  updates $C$ as such:
  \[C := \max(C, C') + 1.\]
  The value is the receive event's timestamp.
\end{enumerate}

\begin{example}
  Figure \ref{fig:message-latencies-scalar} depicts the same events in
  Figure \ref{fig:message-latencies} with scalar timestamps (shown in
  parentheses) assigned to each event. Piggybacked timestamps are
  shown as labels on the message arrows.
\end{example}

Scalar clocks satisfy the clock condition \eqref{eq:mp}. This can be
observed by tracing the path of causality between related events and
seeing that the clock is incremented at each step. However, they do
not satisfy \eqref{eq:sc}.  While $e$ having a lesser timestamp than
$e'$ rules out $e' \to e$, it does not imply $e \to e'$. For instance,
in Figure \ref{fig:message-latencies-scalar-b}, $\msend{1}$ has a
globally minimal timestamp value of $1$, but it does not causally
precede all events with timestamps greater than $1$, or indeed any
event except $\mrecv{1}$.

\subsubsection{Vector clocks}
\label{sssec:vector-clocks}
The strong clock condition \eqref{eq:sc} cannot hold either using
scalar clocks or even synchronized physical clocks because they both
assign timestamps whose values form a total order, meaning any
non-equal timestamps $C_1, C_2$ satisfy either $C_1 < C_2$ or
$C_2 < C_1$. This leaves no way to assign timestamps to synchronous
events that satisfy neither $e \to e'$ nor $e' \to e$, except to make
their timestamps are equal. However, assigning equal timestamps to
logically synchronous events is contradictory, since an event $e$ can
be synchronous with multiple events $e', e''\ldots$ that are not
synchronous with each other (recall Example
\ref{ex:synchronous-intransitive}). The solution is to let timestamps
from a partial order, allowing clock values that are not directly
comparable.


\begin{figure}
  \setlength\belowcaptionskip{5ex}

  \begin{subfigure}{1\textwidth}
    \centering
    \input{images/pgf/mpEx1Vec.pgf}
    \caption{Figure \ref{fig:message-latencies-a} redepicted with vector clocks}
    \label{fig:message-latencies-vector-a}
  \end{subfigure}

  \vspace{4ex}

  \begin{subfigure}{1\textwidth}
    \centering \input{images/pgf/mpEx2Vec.pgf}
    \caption{Figure \ref{fig:message-latencies-b} redepicted with vector clocks}
    \label{fig:message-latencies-vector-b}
  \end{subfigure}

  \caption{Vector clock examples}
  \label{fig:message-latencies-vector}
\end{figure}
\afterpage{\clearpage}

Vector clocks store one scalar value for each process in the system,
which forms a partial order when vectors are compared
component-wise. $P$ maintains a vector $\vt{}$ of $N$ non-negative
integers, one for each process in $\AllProc$, with all values
initialized to $0$. To disambiguate $P$'s vector clock from $Q$'s, we
sometimes add superscripts, e.g. $\vt{P}$ versus $\vt{Q}$.

The $P^\textrm{th}$ component of $P$'s vector clock, denoted
$\vt{}[P]$, is called $P$'s \emph{local time}. For all other processes
$Q$, $\vt{}[Q]$ represents a lower bound of $Q$'s local time. Vector
clocks are updated according to two rules:
\begin{enumerate}
\item[\textbf{R1}:] Before an internal event occurs or a new message is sent, $P$
  increments its local time according to the rule:
  \[\vt{}[P] := \vt{}[P] + 1.\]
  The (entire) updated $\vt{}$ is the event's timestamp and is piggybacked with outgoing messages.
\item[\textbf{R2}:] When $P$ receives a message from $Q$ with timestamp
  $\vt{Q}$, $\vt{}$ is updated according to
  \[\vt{}[X] := \max(\vt{}[X], \vt{Q}[X]) \quad \textrm{for all $X$ in $\AllProc$}.\]
  That is, $\vt{}$ is set to the pointwise maximum of the two
  vectors. After this, $P$ increments its local time:
  \[ \vt{}[P] := \vt{}[P] + 1.\]
  The final vector is the timestamp of the receive event.
\end{enumerate}
These rules are more intuitively understood by demonstration.

\begin{example}
  Figure \ref{fig:message-latencies-vector} depicts the same events as
  Figure \ref{fig:message-latencies} with vector timestamps.
\end{example}

For all other $Q$ in $\AllProc$, $\vt{P}[Q]$ represents $P$'s
conservative estimate of $Q$'s local time, or $\vt{Q}[Q]$. This
estimate is always a lower bound, since $Q$'s local time may advance
without $P$'s knowledge, but $P$ never updates $\vt{P}[Q]$ ahead of
$Q$'s actual local time. $P$ learns about updates to $Q$'s local time
through piggybacked timestamp vectors. This allows $P$ to learn about
$Q$'s time without necessarily communicating directly with $Q$.

Vector timestamps are compared component-wise. This forms a partial
order because one vector may be greater than another in some
components but less than it in others.

\begin{definition}[Vector comparison]
  Let $v, w$ be two vectors. We define the following relations:
  \begin{align*}
             v = w &\iff \forall i, v[i] = w[i] \\
  v \preccurlyeq w &\iff \forall i, v[i] \leq w[i] \\
         v \prec w &\iff v \preccurlyeq w \textrm{ and } \exists i, v[i] < w[i] \\
            \syncts{v}{w} &\iff \textrm{ neither } v \prec w \textrm{ nor } v \succ w
  \end{align*}
  That is, $v \prec w$ if all of $w$'s components are at least as
  great as $v$'s, and at least one of its components is strictly
  greater. When two non-equal vectors are compared, and neither is
  greater than the other, we write $\syncts{v}{w}$ and say the vectors
  are \emph{incomparable}.
\end{definition}

\begin{lemma}
  Vector clocks satisfy the strong clock consistency condition. That
  is, where $C(e)$ is the vector timestamp of an event, then
  \[ e \to e' \iff C(e) \prec C(e'). \]
  From this it follows that for non-equal events $e$ and $e'$ we have
  \[\sync{e}{e'} \iff {C(e) \texttt{\#}\,C(e')}. \]% This isn't typesetting right
\end{lemma}

For reasons of space we omit a proof of the preceding lemma, though
the reader may find it enlighting to formalize the details.

Note that vector clocks and matrix clocks both require the processes
to agree on the set of members in the group. If groups can change,
with members leaving or being added, the timekeeping data structures
would similarly have to be updated. We ignore issues of dynamic group
membership in this document.

\subsubsection{Matrix clocks}
\label{sssec:matrix-clocks}
If a vector clock stores both a local time and a lower bound estimate
of every other process's local time, then a matrix clock stores a
local vector clock and a lower bound estimate of every other process's
vector clock. Each process $P$ stores an $N\times{}N$ matrix
$\mt{}{}{}$, initialized to all zeros, with the following
interpretation. The row corresponding to $P$, $\mt{}{[P]}{}$,
stores $P$'s vector time. For all other $Q$, $\mt{}{[Q]}{}$ store $P$'s
estimate of $Q$'s vector time. Matrix timestamps are piggybacked
with messages, and the receiver uses the sender's vector clock to
update their own vector clock as usual, and takes the pointwise
maximum of all other rows.

\begin{enumerate}
\item[\textbf{R1}:] Before a new message is sent, $\mt{P}{[P]}{[P]}$ is updated according to the rule
  \[\mt{P}{[P]}{[P]} := \mt{P}{[P]}{[P]} + 1.\]
  The entire matrix $\mt{}{}{}$ is piggybacked with the message.
\item[\textbf{R2}:] When a message is received from $Q$ with a piggybacked matrix $\mt{Q}{}{}$,
  $\mt{P}{}{}$ is updated according to two cases
  \begin{enumerate}
  \item Update the row $\mt{P}{[P]}{}$ according to
    \[\mt{P}{[P]}{[X]}:= \max( \mt{P}{[P]}{[X]} ,  \mt{Q}{[Q]}{[X]}) \quad \textrm{for all $X$ in $\AllProc$}\]
  \item Update all rows for each $R \neq P$ according to
    \[\mt{P}{[R]}{[X]} := \max(\mt{P}{[R]}{[X]} ,  \mt{Q}{[R]}{[X]})) \quad \textrm{for all $X$ in $\AllProc$}\]
  \end{enumerate}
  After this, $P$ advances its own local time according to the rule
  \[ \mt{P}{[P]}{[P]} := \mt{P}{[P]}{[P]} + 1.\]
  This new matrix is the timestamp attached to the receive event.
\end{enumerate}

\begin{figure}[p]
  \begingroup
  \setlength\belowcaptionskip{4ex}
  \begin{subfigure}{1\textwidth}
    \input{images/pgf/mpEx1Mat.pgf}%
    \caption{Matrix clock timestamps for the events in Figure \ref{fig:message-latencies-a}}
    \label{fig:message-latencies-matrix-a}
  \end{subfigure}
  \begin{subfigure}{1\textwidth}
    \input{images/pgf/mpEx2Mat.pgf}%
    \caption{Matrix clock timestamps for the events in Figure \ref{fig:message-latencies-b}}
    \label{fig:message-latencies-matrix-b}
  \end{subfigure}
  \caption{Figure \ref{fig:message-latencies} depicted with matrix clocks}
  \label{fig:message-latencies-matrix}
  \endgroup
\end{figure}

\begin{example}
  Figure \ref{fig:message-latencies-matrix} depicts the same events as
  Figures \ref{fig:message-latencies},
  \ref{fig:message-latencies-scalar} and
  \ref{fig:message-latencies-vector} using matrix timestamps. By
  comparison to Figure \ref{fig:message-latencies-vector}, observe
  that for each process $X$, rows of the form $\mt{X}{[X]}$---for
  instance, the top row of matrices in $P_1$---act like ordinary vector
  clocks.
\end{example}

In Section \ref{sec:background}, we mentioned the epistemic nature of
reasoning about distributed systems: a process can only make decisions
based on what it \emph{knows}, which is usually a strict subset of all
(system-wide) truths. In many cases, it is important to take into
account a kind of second-order knowledge: what does a process know
about what other processes know? A typical utility of vector and
matrix clocks is to track which facts known to one process are also
known to another processes. Often it is of interest to compute which
facts are known to \emph{all} other processes. Sections \ref{sec:tsae}
and \ref{sec:continuous-consistency} feature running examples of
mechanisms similar to vector and matrix clocks, called version vectors
and matrices. These are used in the context of database replication,
where they drive decision-making about which updates need to be
propagated and which updates have already been applied everywhere.

\subsection{Message Ordering}
\label{ssec:message-ordering}
Coulouris et al. \cite{coulouris2005distributed} aptly summarized why
it is a problem for unpredictable network latencies to cause messages
to arrive in a different order than they were sent in.
\begin{quote}
  ``This lack of an ordering guarantee is not satisfactory for many
  applications. For example, in a nuclear power plant it may be
  important that events signifying threats to safety conditions and
  events signifying actions by control units are observed in the same
  order by all processes in the system.''
\end{quote}
In this section, we explore different paradigms for message ordering
in distributed systems. As with clocks and timestamps, the choice of
which ordering guarantee to use depends on the needs of the
application. We later generalize the discussion by admitting messages
sent to multiple recipients at once, such as in a group chat
application, where ensuring predictable message ordering is critical.

When ordering is important, applications do not show messages to the
user immediately when they come in from the network---the network can
deliver messages in unexpected and undesirable orders, after all. The
\emph{arrival} time of a message is when it is received from the
network, but instead of acting on it right away, an application may
buffer an arrived message while waiting for other messages (such as
ones with an earlier causal precedence) to ``catch up.'' When a
message is ready to be presented to the user, it is
\emph{delivered}. By waiting to deliver some messages, we can ensure
the stream of messages in order of their delivery satisfies particular
guarantees.

\subsubsection{FIFO ordering}
A modest requirement is the \emph{first-in, first-out} (FIFO)
condition, which stipulates that on any logical communication link
between two processes in the system, messages arrive in the order they
were sent. The restrictive phrase here is ``any logical communication
link''---by definition there is one link for any \emph{pair} of
processes. Hence, FIFO does not impose any conditions on messages
unless they are from the same sender and to the same recipient.

\begin{definition}[FIFO delivery]
  \label{def:fifo}
  The FIFO ordering guarantee is defined by the following condition. Let
  $P$ and $Q$ be any two processes and $m_1$ and $m_2$ be two
  messages sent from $P$ to $P$. Then
  $\msend{1} \to \msend{2} \implies \mrecv{1} \to \mrecv{2}$.
\end{definition}

The Internet Protocol (IPv4 or IPv6) by itself does not provide FIFO
semantics. In the OSI model, FIFO ordering with reliable delivery is
typically provided at the transport layer by the transmission control
protocol (TCP).\footnote{The other classic internet transport, user
  datagram protocol (UDP), does not provide any ordering or
  reliability guarantees.} Applications built on top of TCP or a
similar transport can take therefore take FIFO for granted. Providing
FIFO can be as simple as marking messages sent from $P$ to $Q$ with
consecutive integers. If message $1$ arrives and then $3$ arrives, $Q$
infers that $2$ is lagging behind, delivering $1$ to the user but
withholding delivery of $3$ until after $2$ is received and delivered.

The guarantees provided by FIFO are minimal because they only apply on
a per-link basis: every link requires its own numbering scheme, so
message numbers cannot be meaningfully compared across different
links. To compare messages globally requires something like causal
order, below.

\begin{figure}[p]
  \setlength\abovecaptionskip{0ex}
  \setlength\belowcaptionskip{4ex}
  \begin{subfigure}[t]{0.475\textwidth}
    \centering
    \input{images/pgf/ordEx1.pgf}
    \caption{A non-FIFO execution}
    \label{fig:ordex-non-fifo}
  \end{subfigure}
  \begin{subfigure}[t]{0.475\textwidth}
  \centering
  \input{images/pgf/ordEx2.pgf}
  \caption{A CO (therefore FIFO) execution}
  \label{fig:ordex-co-1}
\end{subfigure}
\begin{subfigure}[b]{0.475\textwidth}
  \centering
  \input{images/pgf/ordEx3.pgf}
  \caption{A CO execution}
  \label{fig:ordex-co-2}
\end{subfigure}
\begin{subfigure}[b]{0.475\textwidth}
  \centering
  \input{images/pgf/ordEx6.pgf}
  \caption{A CO execution}
  \label{fig:ordex-co-3}
\end{subfigure}
\begin{subfigure}[b]{0.475\textwidth}
  \centering
  \input{images/pgf/ordEx5.pgf}
  \caption{A FIFO and non-CO execution}
  \label{fig:ordex-non-co-1}
\end{subfigure}\hfill
\begin{subfigure}[b]{0.475\textwidth}
  \centering
  \input{images/pgf/ordEx4.pgf}
  \caption{A FIFO and non-CO execution}
  \label{fig:ordex-non-co-2}
\end{subfigure}
\caption{Message ordering examples}
\label{fig:message-ordering}
\end{figure}

\subsubsection{Causal ordering}
Causal order is an order guarantee consistent with causal precedence
of events. A network provides causally ordered (CO) delivery if it
satisfies the following property.
\begin{definition}[CO delivery]
  \label{def:causalorder}
  Let $P_\mathrm{dest}$ be any process and consider all messages $m$
  and $n$ sent to $P_\mathrm{dest}$ (possibly by different senders).
  CO is satisfied if $\msend{} \to n^\textrm{send}$ implies
  $\mrecv{} \to n^\textrm{recv}$. That is, each destination receives
  messages in an order consistent with causality between their send
  events.
\end{definition}
In mathematical terms, for each process $P_{\mathrm{dest}}$, the
function mapping send events to corresponding receive events at
$P_{\mathrm{dest}}$ must be monotonic with respect to causal
precedence.  Unlike FIFO, the CO condition enforces a partial order
among messages with (in general) different senders.

\begin{example}
  Figure \ref{fig:message-ordering} demonstrates different message
  ordering conditions. We make a few observations for emphasis.

  \begin{itemize}
    \tightlist
  \item \ref{fig:ordex-non-fifo} violates FIFO because messages $m_1$
    and $m_2$ are both sent from $P_1$ to $P_2$, but the arrive in the wrong order.
  \item \ref{fig:ordex-co-1} satisfies CO and therefore FIFO. Messages
    $m_1$ and $m_2$ arrive in the opposite order but they are sent to
    different destinations.
  \item \ref{fig:ordex-non-co-1} violates CO because the send event of
    $m_1$ happens before the send of $m_3$ via the chain
    $\msend{1} \to \msend{2} \to \mrecv{2} \to \msend{3},$ but
    $\mrecv{3} \to \mrecv{1}$.
  \item \ref{fig:ordex-non-co-2} violates CO because it is equivalent to
    \ref{fig:ordex-non-co-1} with the roles of $P_2$ and $P_3$ swapped.
  \end{itemize}
\end{example}

\subsubsection{Multicasting and Broadcasting}
\label{sssec:multicasting}
We now extend the above definitions to the group communication setting
by allowing messages to have multiple recipients. For simplicity, we
suppose messages are broadcast to all other recipients, though the
definitions can easily generalize to ``multicast'' scenarios where
messages are sent to a subset of recipients.

One way to implement broadcasting is to sending distinct network
messages which, for present purposes, we would treat as a single
unit. Alternatively, we can lean on the network itself for assistance,
sending a single message specially marked as a broadcast, relying on
lower-level protocols in the network to distribute a copy to each
recipient. Regardless of implementation, the challenge and importance
of ensuring consistent message ordering across an entire group is a
paramount concern.

The FIFO and CO broadcast conditions are adapted from Definitions
\ref{def:fifo} and \ref{def:causalorder}. Additionally, we present the
notion of total ordering (TO).

\begin{definition}[FIFO broadcast]
  \label{def:fifo-bcast}
  A broadcast primitive satisfies the FIFO semantics if it satisfies
  the following condition. For any process $P$, if $P$ broadcasts
  $m_1$ before $m_2$, then all recipients receive $m_1$ before $m_2$.
\end{definition}

\begin{definition}[CO broadcast]
  \label{def:causalorder-bcast}
  A broadcast primitive satisfies CO semantics if for any broadcasts
  $m$ and $n$, if $\msend{} \to n^{\textrm{send}}$, then all
  destinations deliver $\mrecv{}$ before $n^{\textrm{recv}}$.
\end{definition}
In the above definition, the happens before relation is defined just
as in the unicast (non-broadcast) setting by following a path of
causality along worldlines and message arrows.

\begin{figure}[h]
  \centering \input{images/pgf/mpEx3.pgf}
  \caption{Broadcast example that satisfies FIFO but violates CO}
  \label{fig:broadcast-fifo-1}
\end{figure}

\begin{example}
  In Figure \ref{fig:broadcast-fifo-1}, causal order is violated. Imagine the following conversation:
  \begin{itemize}
    \tightlist
  \item [$P_1$]: ``I need an ambulance at location A.''
  \item [$P_2$]: ``Understood, the last ambulance has been dispatched.''
  \end{itemize}
  However, $P_3$ receives $P_2$'s response before $P_1$'s request, resulting in this conflicting view:
  \begin{itemize}
    \tightlist
  \item [$P_2$]: ``Understood, the last ambulance has been dispatched.''
  \item [$P_1$]: ``I need an ambulance at location A.''
  \end{itemize}
  From $P_3$'s perspective, $P_1$ appears to be requesting resources
  that are no longer available. The sort of conflict can lead to
  confusion, with requests being duplicated or going
  unanswered. Tracking causal order is crucial to avoid such resource
  misallocations.
\end{example}

A total order broadcast ensures that all recipients receive the
messages in the same order. This order is not required to satisfy any
particular constraints except that all recipients agree on it. Such a
model is appropriate when it is more important that everyone agrees on
a common order of events but the order itself is not necessarily
critical.

\begin{definition}[TO broadcast]
  \label{def:totalorderbroadcast} For any processes $P$ and $Q$ and
  messages $m$ and $m'$ that arrive at both destinations, $m$ arrives
  before $m'$ at both processes or $m'$ arrives before $m$ at both
  processes.
\end{definition}

Total order is independent of causal order, as causality is not total
and a total order does not generally respect causality. Thus it
sensible to consider also a hybrid notion of \emph{total-causal} order
in which all messages are received in a total order respecting
causality.

\begin{figure}[p]
  \setlength\belowcaptionskip{5ex}
  \begin{subfigure}{1\textwidth}
    \centering
    \input{images/pgf/bcastEx1.pgf}
    \caption{Broadcast example that satisfies FIFO but violates CO and TO}
    \label{fig:bcast-order-examples-1}
  \end{subfigure}
  \begin{subfigure}{1\textwidth}
    \centering
    \input{images/pgf/bcastEx2.pgf}
    \caption{Broadcast example that satisfies CO but violates TO}
    \label{fig:bcast-order-examples-2}
  \end{subfigure}
  \begin{subfigure}{1\textwidth}
    \centering
    \input{images/pgf/bcastEx3.pgf}
    \caption{Broadcast example that satisfies TO but violates FIFO}
    \label{fig:bcast-order-examples-3}
  \end{subfigure}
  \caption{Multicast ordering examples}
  \label{fig:bcast-ordering-examples}
\end{figure}

\begin{example}
Figure \ref{fig:bcast-ordering-examples} depicts examples of broadcast
message orders.
\begin{itemize}
  \tightlist
\item Figure \ref{fig:bcast-order-examples-1} trivially satisfies FIFO
  because no process sends more than one broadcast. CO is violated
  because $\msend{1} \to \mrecv{1,4} \to \msend{2}$, while $P_2$
  receives $m_2$ before $m_1$. TO is violated because $P_2$ and $P_3$
  receive the messages in opposite orders.
\item Figure \ref{fig:bcast-order-examples-2} violates TO for the same
  reason above, but satisfies causality because the two send events
  have no causal relation.
\item Figure \ref{fig:bcast-order-examples-3}
  satisfies TO, but $m_2$ arrives at both processes before $m_1$ so
  FIFO is violated.
\end{itemize}
\end{example}
Section \ref{ssec:tsae-message-ordering} describes a conventional
mechanism used to implement total order broadcast.
\paragraph{Self-delivered messages}
In some contexts one considers broadcast primitives that include the
original sender among the recipients of a message. For simplicity, the
examples in this section have not shown this sort of self-delivery,
but it is useful in many cases. Self-delivery is useful when combined
with ordering guarantees. A typical use case is that participants are
using a total order broadcast to maintain local replicas of a state
machine that can be advanced by any participating process by
announcing updates.  This usage is presented in Sections
\ref{sec:tsae} and \ref{sec:continuous-consistency}, with the goal of
replicating shared state over a disruption-heavy network.

\subsection{Shared Memory}
\label{ssec:shared-memory}
Desiging a distributed application using direct message passing can be
challenging due to the complexity of managing the low-level details
surrounding message ordering and reliability. A more abstract
approach, the \emph{distributed shared memory} (DSM) framework,
simplifies this task by allowing programmers to think in terms of
reading and writing to memory locations instead of sending messages
over a network.

The defining feature of DSM is that it allows all processes to
interact as if they had access to a single, unified proof of shared
memory---just like processes running on a single computer---despite
being spread across different, physically separated computers.  This
seamless (subject to caveats explained below) experience is
facilitated by a middleware layer inside the process called the
\emph{memory manager}, which handles all read and write requests
submitted by an application. In the background, not directly visible
to the application, the memory manager coordinates with other
instances over the network to maintain the illusion of a shared state,
or what first responders would call a ``common operating picture.''

Since there is no free lunch, the seamlessness of the DSM model is
subject to caveats, especially this one: usually, a read request
handled by the memory manager does not return the most up-to-date
value of the memory location it reads. Indeed, in the distributed
setting it is not clear a priori what it means for a returned value to
be ``up-to-date'' in the first place, in light of the fact that two
different processes can write conflicting values to the same memory
location at the same time. For developers, understanding the semantics
of the virtual memory layer, meaning knowing what guarantees it makes
concerning what values can be returned at which times, is a crucial
part of building applications that function correctly while providing
reliable performance. Because of the sorts of tensions discussed in
Section \ref{sec:disaster-response}, the design space here is
generally marked by a tradeoff between stronger consistency guarantees
and faster performance.

\begin{figure}
    \centering
    \input{images/pgf/dsm_ex1_WithoutEdges.pgf}
    \caption{Time diagram for memory operations}
    \label{fig:dsm-example-1}
\end{figure}

\begin{figure}
  \centering
  \input{images/pgf/dsm_ex1_DAG.pgf}
  \caption{External order relation among operations in Figure \ref{fig:dsm-example-1} (with edges implied by transitivity not shown)}
  \label{fig:dsm-example-1-DAG}
\end{figure}

Figure \ref{fig:dsm-example-1} depicts an exemplary time diagram for
the shared memory abstraction, similar to those for message
passing. Two kinds of operations are shown: \emph{reads} and
\emph{writes}. A read operation, $\memread{x}$, retrieves the value
stored at (virtual) location $x$, which returns some value $v$. When
we want to indicate the value returned by the read, we write
$\memreadVal{x}{v}$. A write operation, $\memwrite{x}{v}$, indicates
writing $v$ to memory location $x$, which in code might be written as
something like $x := v$.

An operation does not happen instantly, but has a \emph{duration}. An
arbitrary read or write operation, $\Op$, spans from the moment of
time the operation is invoked by the application ($\memstart{\Op}$),
to when it finished ($\memstop{\Op}$), returning either the read value
or an acknowledgment of the write request. During this span, the
memory management layer is usually coordinating in the background with
other processes over the network, say by looking up the current value
of a memory location, but this is not shown in the diagrams. The
entire sequence of requests across all processes forms what we call a
\emph{history}. If $H$ is a history and $P$ is a process, we write
$\localhistory{P}$ to mean just the sequence of operations that happen on $P$, the
so-called \emph{local history} of $P$.

Because they have a duration, memory operations on different
processes, including ones that access the same virtual memory
locations, can occur simultaneously. A fundamental relation among
operations is their \emph{external order}, the partial order that
relates non-overlapping events their physical times, but does not
assign an ordering to events whose executions overlap in physical
time.
\begin{definition}[External order]
  \label{def:external-order}
  Let $H$ be a history. An operation $\Op^1$ \emph{externally
    precedes} operation $\Op^2$ if
  $\memstop{\Op^1} < \memstart{\Op^2}$. This induces an irreflexive
  partial order on $H$ called \emph{external order}.
\end{definition}
The definition states that one operation externally precedes another
if it stops before the other is invoked. Note that we are comparing
events in terms of real, physical time: external order is the partial
order of events witnessed by an outside observer who can watch
operations executing globally in real time. Figure
\ref{fig:dsm-example-1-DAG} depicts the external order among the
operations in Figure \ref{fig:dsm-example-1}, forming a directed
acyclic graph (DAG).

Two events not related by external order are said to be physically
concurrent.

\begin{definition}[Physical concurrency]
  \label{def:physical-concurrency}
  Consider two operations $\Op^1$ and $\Op^2$. If neither externally
  precedes the other, in another words if there is some moment in time
  during which both operations are executing, the operations are said
  to be \emph{physically concurrent}, written $\concurrent{\Op^1}{\Op^2}$.
\end{definition}

Note that we have reused notation between Definitions
\ref{def:logically-synchronous} and \ref{def:physical-concurrency}.
Though they are similar concepts (both are reflexive and symmetric but
generally non-transitive binary relations), physically concurrent
memory operations in the DSM model should not be confused with
logically synchronous events in the message-passing model.

\subsection{Semantics and Consistency}
In a sequential application running on a single computer, it is clear
how read and write requests should be interpreted. A read request
$\memread{x}$ should return the most recent value that was written to
$x$ by a write operation $\memwrite{x}{v}$ (or return a default value
if no such write exists, but we will not consider such examples). This
is unambiguous because we assume that in a single process, memory
operations do not overlap in time, so there is always a sense of which
one happened first.

\begin{example}
  Consider the following history of operations running inside a single process.
  \[\input{images/pgf/dsm_ex_sequential.pgf}\]
  This diagram does not indicate what values are returned by the read
  operations, but since there is no ambiguity in the order of events,
  it is clear what these values \emph{should} be. Each read request
  should return the value (shown in bold below) set by the most recent
  write to that location.
  \[ \memwrite{x}{0} \to \memwrite{y}{5} \to \memreadVal{x}{\textbf{0}} \to \memwrite{x}{3} \to \memreadVal{y}{\textbf{5}} \to \memreadVal{x}{\textbf{3}}. \]
\end{example}

In a distributed system, operations on different processes can run
concurrently, so there is no obvious way to arrange events into a
total order that all processes can agree on. Consequently, the notion
of ``most recent'' operation is ambiguous, so it does not even make
sense to say that read requests always return the most recent written
value.

\begin{figure}[h]
  \input{images/pgf/dsm_ex2.pgf}
  \caption{A history with read return values left unspecified,
    featuring concurrent operations writing to and reading from the
    same location}
  \label{fig:dsm-example-2}
\end{figure}

%\begin{example}
%  \label{exmpl:concurrentupdates}
%\end{example}

Consider the history shown in Figure \ref{fig:dsm-example-2}, which
contains three operation that write to location $x$. Two of these
operations, $\memwrite{x}{3}$ $\memwrite{x}{5}$, are executed at
overlapping moments in time, making it unclear which should be
considered ``first.'' The ambiguity is made concrete by considering
the subsequent read operations---which values should they return? Or
rather, we should ask which values are they \emph{allowed} to return,
since the possibilities are not usually deterministic.

A \emph{memory model} exists precisely to answers questions of the
form, ``Which values might be returned by read requests in which
scenarios?'' In the example above, a model would have to answer
several questions like the following ones:
\begin{enumerate}
\item Do the read operations on $P_1$ and $P_2$ have to return the same value?
\item Can the second read operation at $P_2$ return a different value
  than the first one?
\item Is it ever possible for any of the $\memread{x}$ operations to
  return the value 4?
\end{enumerate}

One can consider many different ways of answering these questions,
depending on the consistency requirements of the high-level
application. The strictest memory model, called
\emph{linearizability}, is a sort of gold standard. If the distributed
system whose history is shown in Figure \ref{fig:dsm-example-2} is
linearizable, the all three read operations must return the same
value, and this must be either $3$ or $5$. This model is intuitive but
too strict for our use case, so programmers must be prepared for less
rigidly prescribed behavior from the memory manager.

Choosing to implement a particular memory model requires balancing the
needs and expectations of the application against its performance
characteristics, including its usage patterns and networking
environment. An application designed for one memory model would
generally misbehave, often in a way that is difficult to diagnose, if
executed in an environment that implements a different model. However,
on the other hand, implementing a stricter memory model may impose a
prohibitive overhead on application performance.

To resolve the ambiguity caused by overlapping memory operations, one
might attempt to assign physical timestamps to them and use this to
define an agreed-upon global total order of operations. However, using
these kinds of orders in practice requires sufficiently fine-grained
timestamps from synchronized clocks. For our environments this is
often infeasible (see Section
\ref{ssec:physical-synchronization}).

\subsection{Strong Consistency Models}
\label{ssec:strong-consistency}
This section considers the two major memory models usually said to
provide ``strong'' consistency: linearizability and sequential
consistency. Both models involve the notion of a sequential history,
or a set of operations arranged into a particular total order.

\begin{definition}[A sequential history]
  \label{def:sequential-history}
  A \emph{sequential history} is a set of memory operations in a
  particular total order. If $H$ is a history, a \emph{serialization}
  is any total order among the operations in $H$.
\end{definition}

Linearizability and sequential consistency both require that all read
operations return values consistent with some serialization of the
global history. That is, they stipulate that among all the operations
in $H$, there is some way of ordering them so that all read operations
return the values of the most recent write operations. Where the two
models differ is in how they constrain which serializations of $H$ are
allowable.

\subsubsection{Linearizability}
\label{sssec:linearizability}

\emph{Linearizability}, a sort of gold standard for memory
consistency, can be concisely defined as a system that acts like
``each operation applied by concurrent processes takes effect
instantaneously at some point between its invocation and response.''
\cite{10.1145/78969.78972} The same condition is known (though
sometimes with subtle variations in meaning) by names like atomic
consistency and external consistency. It means almost the same thing
as strict serializability, except the latter terminology is used to
discuss transactional databases and implies other database-related
guarantees. Formally, a linearizable history is defined by three
features.
\begin{definition}[Linearizable history]
  \label{def:linearizable-history}
  Let $H$ be a history of memory operations. $H$ is
  \emph{linearizable} if it satisfies the following three rules.
\begin{enumerate}
  \tightlist
\item \textbf{Global Agreement on Order}: All processes behave
  (defined below) as if they are observing the same serialization of
  $H$, meaning some particular total order $\sigma$.
\item \textbf{Correct Responses}: Each read request
  \(\memreadVal{x}{a}\) returns the value of the most recent write
  request \(\memwrite{x}{a}\) as ordered by $\sigma$.
\item \textbf{Consistent with External Order}: The serialization of
  $H$ is consistent with external order: if
  $\memstop{\Op^1} < \memstart{\Op^2}$, then $\sigma$ also orders
  $\Op^1$ before $\Op^2$.
\end{enumerate}
\end{definition}

Definition \ref{def:linearizable-history} is concerned with an
individual history of some distributed application. When the
application only ever admits linearizable histories, as permitted by
the memory manager, the entire system is said to be linearizable.

\begin{definition}[Linearizable system]
  \label{def:linearizable-system}
  A DSM application is linearizable if all possible histories of the
  application satisfy Definition \ref{def:linearizable-history}.
\end{definition}

Consider the history Figure \ref{fig:dsm-example-2} again. If the
application is linearizable, the read responses must agree on some
order $\sigma$ consistent with external order, implying that
$\memwrite{x}{4}$ happens before $\memwrite{x}{3}$ and
$\memwrite{x}{5}$, but the latter operations can occur in any
order. Since $P_1$ and $P_2$ have to agree on this order, all three
read responses will return the value written by whichever write is
ordered last (meaning most recent), which is either $3$ or $5$. These
possibilities are illustrated in Figure
\ref{fig:dsm-example-2-linearizations}.

Figure \ref{fig:dsm-example-2-linearizations} also depict an
equivalent, more visually intuitive way, of approaching defining
linearizability. A linearizable history where returned values are
consistent with a choice of linearization point for each operation.
\begin{definition}
  A \emph{linearization point} $t$ for an operation $\Op$ is a time in
  the range $[\memstart{\Op}, \memstop{\Op}]$, between the operation's
  invocation and response. We forbid distinct operations from having
  an overlapping linearization point.
\end{definition}

A history is linearizable if there is some choice of linearization
point for each operation, and returned values are consistent with
operations taking effect in whole and instantaneously at their
linearization points. The subfigures in
\ref{fig:dsm-example-2-linearizations} depict possible choices of
linearization points in yellow.

\begin{figure}
  \begin{subfigure}{1\textwidth}
    \setlength\belowcaptionskip{4ex}
    \centering
    \input{images/pgf/dsm_ex2_linear_1.pgf}
    \caption{A linearization where the read operations return 3}
    \label{fig:dsm-example-2-linearizations-1}
  \end{subfigure}
  \begin{subfigure}{1\textwidth}
    \input{images/pgf/dsm_ex2_linear_2.pgf}
        \caption{A linearization where the read operations return 5}
    \label{fig:dsm-example-2-linearizations-b}
  \end{subfigure}
  \caption{Two possible linearizations of Figure \ref{fig:dsm-example-2} with linearization points shown in yellow}
  \label{fig:dsm-example-2-linearizations}
\end{figure}

\subsubsection{Sequential consistency}
\label{sequential-consistency}

Linearizability offers very strong guarantees related to real-time
constraints, but for many applications this requirement is a burden
for performance. A more relaxed model, sequential consistency,
provides comparably strong guarantees but does not impose the same
constraints with respect to external order. Whereas linearizability
requires operations to be consistent with a serialization respecting
external order, sequential consistency allows any serialization that
respects \emph{program order}.

\begin{definition}[Program order]
  An operation $\Op^1$ precedes another operation $\Op^2$ in
  \emph{program order} if the events occur in the same process $P$ and
  $\memstop{\Op^1} < \memstart{\Op^2}$. In this case we write
  $\Op^1 \programorder{P} \Op^2$.
\end{definition}
If two operations occur in different processes, they are not related
by program order. That is the distinction between program and external
order.

The definition of sequential consistency follows the same structure as
that for linearizability, with program order in place of external
order.

\begin{definition}[Sequentially consistent history]
  \label{def:sequentially-consistent-history}
  An history $H$ is \emph{sequentially consistent} if the following
  three rules are satisfied.
\begin{enumerate}
  \tightlist
\item \textbf{Global Agreement on Order}: All processes behave as if
  they observe the same serialization $\sigma$ of $H$.
\item \textbf{Correct Responses}: Read requests return the value of
  the most recent write request to the same location according to $\sigma$.
\item \textbf{Consistent with Program Order}: $\sigma$ is consistent
  with program order: if
  $\memstop{\Op^1} \programorder{P} \memstart{\Op^2}$ for some $P$, the serial
  history must include $\Op^1$ before $\Op^2$.
\end{enumerate}
\end{definition}

Since external order imposes more constraints than program order,
linearizable histories, like those shown in Figure
\ref{fig:dsm-example-2-linearizations}, are always sequentially
consistent.
\begin{lemma}
  \label{lem:linearsequential}
  A linearizable execution is sequentially consistent.
\end{lemma}

The converse of Lemma \ref{lem:linearsequential} does not hold,
meaning some sequentially consistent executions are not
linearizable. As noted earlier, there are only two linearizable
histories of Figure \ref{fig:dsm-example-2}, and they both require all
three read operations to return the same value. Examples
\ref{ex:dsm-example-2-sequential-1} and
\ref{ex:dsm-example-2-sequential-2}, below, demonstrate sequentially
consistent histories where $P_1$ and $P_2$ read non-equal values.

\begin{figure}
  \begin{subfigure}{1\textwidth}
    \setlength\belowcaptionskip{4ex}
    \centering
    \input{images/pgf/dsm_ex2_seq1.pgf}
    \caption{Sequentially consistent history where $P_1$ and $P_2$ read different values of $x$}
    \label{fig:dsm-example-2-sequential-1-sub}
  \end{subfigure}
  \begin{subfigure}{1\textwidth}
    \input{images/pgf/dsm_ex2_seq1_serial.pgf}
    \caption{A consistent serialization respecting program order}
    \label{fig:dsm-example-2-sequential-1-serial}
  \end{subfigure}
  \caption{A sequentially consistent history and its consistent serialization}
  \label{fig:dsm-example-2-sequential-1}
\end{figure}

\begin{example}
  \label{ex:dsm-example-2-sequential-1}
  Figure \ref{fig:dsm-example-2-sequential-1-sub} depicts a sequentially
  consistent history of the operations depicted in Figure
  \ref{fig:dsm-example-2}. This history is non-linearizable because
  $P_1$ and $P_2$ read different values for $x$. It is sequentially
  consistent because it returns values consistent with the alternate
  history shown in Figure \ref{fig:dsm-example-2-sequential-1-serial},
  which is sequential because has no overlapping operations. The
  latter can be obtained by ``sliding'' the operations in $P_2$ along
  their worldline so they occur after those in $P_1$.
\end{example}

\begin{figure}
  \begin{subfigure}{1\textwidth}
    \setlength\belowcaptionskip{4ex}
    \centering
    \input{images/pgf/dsm_ex2_seq2.pgf}
    \caption{Sequentially consistent history where $P_2$ reads $x$ with value $4$}
    \label{fig:dsm-example-2-sequential-2-sub}
  \end{subfigure}
  \begin{subfigure}{1\textwidth}
    \input{images/pgf/dsm_ex2_seq2_serial.pgf}
    \caption{A consistent serialization respecting program order}
    \label{fig:dsm-example-2-sequential-2-serial}
  \end{subfigure}
  \caption{A sequentially consistent history and its consistent serialization}
  \label{fig:dsm-example-2-sequential-2}
\end{figure}

\begin{example}
  \label{ex:dsm-example-2-sequential-2}
  Figure \ref{fig:dsm-example-2-sequential-2-sub} depicts another
  sequentially consistent history of the operations in Figure
  \ref{fig:dsm-example-2}. In this example, $P_2$ appears to travel
  backwards in time, reading the stale value $4$ immediately after
  reading $5$. This history is consistent with the serialization
  shown in Figure \ref{fig:dsm-example-2-sequential-2-serial}.
\end{example}

The time-traveling nature of Example
\ref{ex:dsm-example-2-sequential-2} can be explained by remembering
that the DSM model is implemented in terms of message-passing. $P_2$
may read a stale value of $4$ because of the time it takes for the
memory manager running on $P_1$ to notify $P_2$ about the
$\memwrite{x}{4}$ operation. This notification may not be received
until after $P_2$ performs the $\memreadVal{x}{5}$ operation.

Sequential consistency is an intuitive property for reasoning about
the possible behaviors of distributed programs. Note that each process
in the system issues memory operations in a particular order---these
can be thought of as individual steps in a program. Before the
application is executed on real computers, there is no guarantee about
the relative timing of program steps that run on different computers,
since different machines may run at different speeds. For instance,
before running the program and observing the series of events shown in
Figure \ref{fig:dsm-example-2-sequential-2-sub}, we did not
necessarily know that the $\memwrite{x}{4}$ operation would precede
the $\memwrite{x}{5}$ operation in real-time---they are both the first
steps of their respective programs, with no relation to each
other. The alternate order of events shown in
\ref{fig:dsm-example-2-sequential-2-serial}, where $\memwrite{x}{5}$
precedes $\memwrite{x}{4}$, is just as likely a priori as the one that
was actually observed. Sequential consistency guarantees each program
is always consistent with one of the serializations that can be
expected a priori, before the program is executed and a real-time
external order of events is fixed.

\subsection{The CAP Theorem}
Real-world systems rarely function as a perfectly coherent, unified
system. One fundamental gap between idealized and real-world behavior
stems from a well-understood and fundamental tradeoff between
coherence and performance. The more ``coherence'' we demand from the
system, the more processes have to communicate over the network, whose
unpredictable delays impose overheads that degrade
performance. Conversely, the more we demand immediate answers from our
system, the less time a process has to communicate with other
processes, so the system as a whole does not seem as coherent and
unified to end users.

This tradeoff is made fully stark by considering the possibility that
the network suffers from a partition, which prevents some processes
from communicating with others.

\begin{definition}[Network partition] A \emph{network partition} is a
span of time where some nodes are unable to communicate with another
set of nodes on the network.
\end{definition}

In 1999, Fox and Brewer \cite{1999foxbrewer} articulated a formal
tradeoff between three desirable properties of distributed systems:
consistency, availability, and an ability to function during network
partitions. This observation was formalized and rigorously proven by
Gilbert and Lynch in 2002 \cite{2002gilbertlynchCAP}. Despite its
prominence at the heart of distributed systems, and the fact that its
proof is fairly straightforward, the CAP theorem is sometimes
misunderstood, so it is worth clarifying its key terms.

\begin{description}
\item[Consistency] Gilbert and Lynch define consistency as
  linearizability.
\item[Availability] A CAP-available system eventually responds to every client
  request (a read or write operation) after a finite time.
\item[Partition tolerance] A partition-tolerant system continues to
  function in the face of arbitrary partitions in the network.
\end{description}

In the last case, the possibility is allowed that a partition never
recovers. This could happen if a critical communications cable is
permanently severed, for instance.

The CAP theorem is the simple observation that a distributed system
cannot guarantee all three properties simultaneously: a system that
operates during network partitions cannot ensure both linearizability
and availability. We give only the informal sketch here, leaving the
interested reader to consult the more formal analysis by Gilbert and
Lynch. The key assumption in the proof is that a process's behavior is
only affected by the messages it receives. During a partition, its
behavior is the same regardless of what other processes do, since it
does not communicate with them and cannot be affected by them. Below,
we term this property \emph{behavioral invariance}.

\begin{figure}
  \input{images/pgf/dsm_cap_ex1.pgf}
  \caption{A history where linearizability cannot maintained during a network partition}
  \label{fig:dsm-cap-example-1}
\end{figure}

\begin{theorem}[The CAP Theorem]
  \label{thm:cap}
  If indefinite network partitions are possible, then a distributed
  system cannot guarantee both linearizability and
  eventual availability.
\end{theorem}
\begin{proof}
  Consider the history in Figure \ref{fig:dsm-cap-example-1}. Clearly
  there is only possible linearization of this history:
  $\memwrite{x}{1}$ executes, $\memwrite{x}{2}$ executes, and
  $\memread{x}$ reads the value $2$. Now suppose $P_1$ and $P_2$ are
  separated by a network partition, meaning $P_1$ does not receive any
  messages from $P_2$. This leaves two possibilities for how $P_1$
  might handle the $\memread{x}$ operation:
  \begin{enumerate}
  \item $P_1$ can proceed despite the network partition and return a
    value. By behavioral invariance, $P_1$ would have to return
    $\memreadVal{x}{1}$. This is value it would read (by
    linearizability) if $P_2$ did not write to $x$. Behavioral
    invariance means $P_1$'s reads the same value regardless of what
    $P_2$ does (since in either case, it receives no communication
    from $P_2$). Returning $1$ violates linearizability in the case
    above where $P_2$ \emph{does} write to $x$.
  \item $P_1$ might detect (or assume) the network suffers a partition
    and refuse to handle the $\memread{x}$ until it is able to
    communicate with $P_2$ again, whereupon it would learn the correct
    value of $x$ is $2$. However, we do not assume the partition has
    to recover, in which case $P_1$ waits forever, which violates
    availability.
  \end{enumerate}
  Thus, we cannot have both linearizable consistency and
  availability. More precisely, to ensure both of these properties, we
  would have to assume the network never suffers from partitions, but
  this is unrealistic. As discussed in Section
  \ref{sssec:ground-communication}, real-world examples of partitions
  in wildland firefighting environments are common.
\end{proof}

The proof above raises the question of whether the weaker notion of
sequential consistency can be used to avoid being subject to the CAP
theorem. The answer is negative: sequential consistency is also
CAP-unavailabile.

\begin{figure}[p]
  \setlength\belowcaptionskip{5ex}
  \begin{subfigure}{1\textwidth}
    \centering
  \input{images/pgf/dsm_cap_ex2.pgf}
  \caption{An execution that cannot maintain sequential consistency during a network partition}
  \label{fig:dsm-cap-example-2}
  \end{subfigure}
  \begin{subfigure}{1\textwidth}
    \centering
    \input{images/pgf/dsm_cap_ex2_results.pgf}
    \caption{By behavioral invariance, if there is a network partition, the values read for $x$ and $y$ must be their initial values of $0$}
    \label{fig:dsm-cap-example-2-results}
  \end{subfigure}
  \begin{subfigure}{1\textwidth}
    \centering
    \input{images/pgf/dsm_cap_ex2_seq1.pgf}
    \caption{A serial order where $\memreadVal{y}{0}$ precedes $\memwrite{y}{1}$ forces $\memwrite{x}{1}$ to precede $\memreadVal{x}{0}$, violating sequential consistency}
    \label{fig:dsm-cap-example-2-serial1}
  \end{subfigure}
  \begin{subfigure}{1\textwidth}
    \centering
    \input{images/pgf/dsm_cap_ex2_seq2.pgf}
    \caption{A serial order where $\memreadVal{x}{0}$ precedes $\memwrite{x}{1}$ forces $\memwrite{y}{1}$ to precede $\memreadVal{y}{0}$, violating sequential consistency}
    \label{fig:dsm-cap-example-2-serial2}
  \end{subfigure}
  \caption{}
  \label{}
\end{figure}

\begin{lemma}[CAP for sequential consistency]
  \label{thm:cap-sequential}
  An eventually-available system cannot provide sequential consistency
  in the presense of network partitions.
\end{lemma}
\begin{proof}
  Consider the history in Figure \ref{fig:dsm-cap-example-2} and
  suppose all memory locations are initialized to $0$. Following
  similar reasoning as above, behavioral invariance means that if
  $P_1$ and $P_2$ are separated by a partition and remain available,
  $P_1$ has to return $0$ to the request $\memread{y}$---that is the
  value it would return assuming $P_2$ does not write to
  $y$. Likewise, $P_2$ would read $\memreadVal{x}{0}$. However, the
  resulting history, shown in Figure
  \ref{fig:dsm-cap-example-2-results}, is not sequentially
  consistent. For $\memreadVal{y}{0}$ to be consistent, the sequential
  order of operations would have to arrange $\memreadVal{y}{0}$ before
  $\memwrite{y}{1}$---otherwise reading $0$ is incorrect. This results
  in the order shown in Figure \ref{fig:dsm-cap-example-2-serial1},
  which is incorrect because $\memreadVal{x}{0}$ occurs after
  $\memwrite{x}{1}$. The situation is symmetric: to order
  $\memreadVal{x}{0}$ before $\memwrite{x}{1}$ results in an order where
  $\memwrite{y}{1}$ precedes $\memreadVal{y}{0}$ (Figure \ref{fig:dsm-cap-example-2-serial2}).
\end{proof}

\subsubsection{Consequences of CAP}
\label{interpretation-of-the-cap-theorem}
While the CAP theorem is theoretically simple, its implications are
more nuanced than they may appear \cite{2012CAP12Years}. A common
oversimplification is that the CAP theorem is represents a ``choose 2
of 3'' scenario: a system designer can choose at most two of
consistency, availability, and partition resilience. In fact, real
systems may balance weaker forms of all three properties. The CAP
theorem only rules out the combination of all three properties when
each of them is defined in an idealized, rigid way.

In practice, applications often settle for weaker levels of
consistency than linearizability or sequential consistency. Resilience
to network partitions typically requires coping with intermittent,
rather than indefinite, communications failures. Finally, availability
is best measured in terms of actual response time as experienced by
the user, and not the mere assurance that a request will
``eventually'' be handled. Thus, each of these dimensions is actually
quantitative in nature, rather than an all-or-nothing proposition.

The locality principle is also highly relevant when considering
implications of the CAP theorem for a real system. The closer agents
are located, the more reliable their communications will be in
general, and the more applications can provide consistency and
availability for operations that only require coordinating with nearby
agents. At short time scales, operations that only require local
coordination are common.

\subsection{Causal Consistency}
\label{ssec:causal-memory}
\emph{Causal} consistency, defined by Ahamad et
al. \cite{1995:causal-memory}, is a weaker memory model than
sequential consistency. Whereas sequential consistency requires the
system as a whole to behave as if all write operations take place in
some total order (which must also respect program order), causal
consistency allows different processes to behave as if they witnessed
past write operations take effect in different orders. Only write
operations related by \emph{causal precedence} are required to take
effect in a common order across all processes: ``reads respect the
order of causally related writes.''  \cite{1995:causal-memory}

We have not defined what causal precedence means for memory
operations. The notion is similar in its motivation to causal
precedence in message-passing framework (Definition
\ref{def:causalprecedence}) and the idea of causal broadcast
(Definition \ref{def:causalorder-bcast}) but the definition of
causally related memory operations is not as simple as that of causal
precedence among messages. In message passing, a receive event is
always associated with a unique send event, but multiple processes can
write the same value to the same memory location, and for a later
operation that reads this value, it is not clear which write
``caused'' it. For this purpose we define a \emph{writes-into} order.

\begin{definition}[Writes-into order]
  Let $H$ be a history of memory operations.\footnote{We are treating
    $H$ as a \emph{multiset}, meaning for example that separation
    operations both of the form $\memwrite{x}{v}$ are considered
    distinct.} A ``writes into'' order $\writesinto$ is a binary
  relation where each read $\memreadVal{x}{v}$ is paired with the
  write operation that determined the value that operation read. That
  is, all relatedl pairs are of the form
  $\memwrite{x}{v} \writesinto \memreadVal{x}{v}$ and every read is
  paired with some write.
\end{definition}

  Ahamad et al. \cite{1995:causal-memory} give a slightly more complex
  definition to allow for operations that read uninitialized memory
  locations. For simplicity we assume each memory location is written
  to before it is read.

\begin{definition}[Causality order on memory operations]
  \label{def:memorycausalprecedence}
  For a given writes-into order $\writesinto$ on
  $H$, the associated \emph{causality order}
  $\causalityorder$ is the transitive closure of the union of
  $\writesinto$ and program order. That is, if $\Op \causalityorder
  \Op'$, then one of the following holds:
  \begin{itemize}
  \item $\Op \programorder{P} \Op'$ for $P$
  \item $\Op \writesinto \Op'$
  \item There is some $\Op''$ such that $\Op \causalityorder \Op''$ and $\Op'' \causalityorder \Op'$
  \end{itemize}
  By fiat, we also require that $\causalityorder$ must not be cyclic,
  meaning their are no ``causality loops'' like
  $\Op \causalityorder \Op' \causalityorder \Op$.  If
  $\Op \causalityorder \Op'$ in a causality order, we say $\Op$
  causally precedes $\Op'$.
\end{definition}

We can now define causally consistent executions of memory
operations. For a history $H$ and each process $P$, let $A|_{P}$ be
the union of $\localhistory{P}$ and the set of all writes in $H$.

\begin{definition}[Causal consistency]
  \label{def:causalconsistency}
  An execution is causally consistent if each $P$ behaves as if it
  observes some serialization of $A|_{P}$ consistent with $\causalityorder$.
\end{definition}

Notably, Definition \ref{def:causalconsistency} does not require that
all processes behave as if they are observing the \emph{same}
serialization.

\begin{example}
  \label{ex:dsm-causal-ex1}
  Figure \ref{fig:dsm-causal-ex1} depicts a causally consistent
  execution of the operations in Figure \ref{fig:dsm-example-2} that
  is not sequentially consistent. The writes-into order is depicted
  with dotted edges. The operations $\memwrite{x}{5}$ and
  $\memwrite{x}{3}$ are not related by $\causalityorder$ and therefore
  do not have to appear to take effect in the same order at all processes.

  Suppose this history were consistent with some serial order. By
  correctness, $\memwrite{x}{5}$ must be the last write before
  $\memreadVal{x}{5}$. By program order, $\memwrite{x}{5}$ comes
  before $\memreadVal{x}{3}$. By correctness, $\memwrite{x}{3}$ must
  come before $\memreadVal{x}{3}$, but not before
  $\memwrite{x}{5}$. But then it must be the last write before
  $\memreadVal{x}{5}$, a contradiction.
\end{example}

\begin{figure}
  \setlength\belowcaptionskip{5ex}
  \begin{subfigure}{1\textwidth}
    \centering
    \input{images/pgf/dsm_causal_ex1.pgf}
    \caption{A causally consistent history with a writes-into order
      shown with dotted edges}
    \label{fig:dsm-causal-ex1}
  \end{subfigure}
  \begin{subfigure}{1\textwidth}
    \centering
    \input{images/pgf/dsm_causal_ex1_serial1.pgf}
    \caption{A serialization consistent with the values read by $P_1$}
    \label{fig:dsm-causal-ex1-serial1}
  \end{subfigure}
  \begin{subfigure}{1\textwidth}
    \centering
    \input{images/pgf/dsm_causal_ex1_serial2.pgf}
    \caption{A serialization consistent with the values read by $P_2$}
    \label{fig:dsm-causal-ex1-serial2}
  \end{subfigure}
  \caption{A causally consistent but sequentially inconsistent history}
\end{figure}

Causal consistency is not subject to the limits of the CAP theorem.

\begin{lemma}[Causal consistency is CAP-available]
  \label{thm:cap-causal}
  A system can enforce causal consistency during network partitions.
\end{lemma}
\begin{proof}
  Consider two processes that execute read and write operations purely
  locally. That is, they never send messages to other processes, and
  they always read the most recent value they have written to a
  location, regardless of what other processes do.

  This situation is always causally consistent for the causality
  relation generated by the empty writes-into order. If the
  writes-into order is empty, each process only has to be consistent
  with some serial order of write operations, and processes do not
  have to agree on this order. In particular, each $P$ is
  consistent with a serialization where all write operations issued by
  other processes are executed \emph{after} all of $P$'s own read
  operations.
\end{proof}

The proof of the Lemma \ref{thm:cap-causal} speaks to the weakness of
causal consistency. In the proof, different processes are allowed to
deviate arbitrarily far from consistency with each other. At any
moment in time, they may diverge wildly, which violates our goal of
maintaining a common operating picture. Causal consistency is too weak
to place an upper bound on the divergence observed by users, which
suggests it is not the most appropriate model for the kinds of
safety-related applications we have in mind. Section
\ref{sec:continuous-consistency} will consider how a continuous
consistency model can provide these sorts of bounds.

\subsection{Section Summary}
\label{ssec:background-summary}
This discussion has explored the key challenges involved in building
distributed systems that connect geographically dispersed components
over unpredictable networks. The variability in message delays,
particularly in the context of broadcasts sent to multiple recipients,
can result in messages that arrive in different orders.  This
situation can lead to chaos if a message-ordering discipline is not
imposed.

To mitigate the effects described above, distributed systems must
track the causal precedence relation between events. Because physical
clocks are not generally reliable enough for this purpose, especially
at fine time scales, logical clocks---scalar, vector, and matrix
clocks---are typically used, each with a different tradeoff in terms
of precision of the information tracked and the administrative and
messaging overhead. If groups can change dynamically, as in our use
cases, then additional group membership protocols are needed to ensure
that all processes know which other processes are participating in the
system at any moment.

Programmers may find it easier to frame distributed applications in
terms of reading and writing from a shared pool of virtual memory,
rather than sending messages, using distributed shared memory
framework. The fact that processes can access the same virtual memory
locations at the same time makes it challenging to maintain systemwide
coherence. Strong consistency models like linearizability and
sequential consistency provide the illusion of a single source of
truth, but the CAP theorem makes it virtually impossible to guarantee
these properties over connection-challenged networks. Weaker models
like causal consistency are not subject to the same limitations, but
they do not enforce limits bounding how far apart data replicas can
diverge, rendering them potentially unsuitable for safety-related
applications.

% In summary, there is no free lunch in distributed systems. Designing
% resilient distributed systems for emergency response scenarios
% requires a careful balancing act between competing properties that
% is carefully calibrated to the use case and the operational
% environment.


\section{Timestamped Anti-Entropy}
\label{sec:tsae}
This section presents Golding's Timestamped Anti-Entropy (TSAE)
protocol \cite{1992:golding-thesis}, a \emph{weak consistency group
  communication} mechanism that provides a form of multicasting to
applications. We picture TSAE as a key driver of communication across
a distributed system supporting first responders across a region or
state during a wide-area event, like the kind discussed in Section
\ref{sec:disaster-response}. Such a system might track firefighters'
locations, monitor a wildfire's boundaries, disseminate weather data,
and orchestrate resource deployment. In this context, TSAE would have
responsibility for ensuring that every update eventually reaches each
of its intended recipients.

Golding's thesis \cite{1992:golding-thesis} presents TSAE as one
component of a broader toolkit for developing distributed
applications. Before laying out the protocol, it is useful to explain
what role it is meant to have in the context of a larger distributed
system. We imagine a group of cooperating processes, referred to by
Golding as ``principals,'' managing shared state and communicating
through message exchange. These messages are an application-specific
construct and might contain things like instructions to update a
database. These may not map one-to-one with low-level network messages
like those discussed in Section \ref{ssec:message-passing}, which we
think of as analogous to Internet Protocol (IP) packets. Note that an
IP packet might contain multiple application messages, and a large
application message may be split up into multiple packets during
network transit.

At a lower level, we assume the network supports point-to-point
(unicast) communication, but it does not have to guarantee FIFO
ordering and may duplicate network messages, though it does not
spontaneously create new ones. The network may not be reliable, but we
assume that persistent attempts to communicate with a distant node
will eventually succeed. The network might support native multicasting
of network-level messages, which could be used as part of an
implementation of TSAE, but the TSAE layer itself provides a form of
application-level multicasting regardless of the network
infrastructure.

A typical use case of TSAE is to maintain replicas of a shared
database at multiple sites, for which purpose its reliable eventual
delivery makes it well-suited as a fault-tolerant messaging
component. Here, ``database'' is used broadly to refer to any data
store updated according to some logical model, such as a relational
schema, a key-value store, or a document store. Processes update the
data by broadcasting update messages using TSAE. Updates are
eventually delivered to every process, at which point recipients apply
it their own replica. Of course, the semantics of updates are decided
by the application. If the underlying data model requires updates to
be applied in the same order across replicas to maintain consistency,
TSAE can be paired with a totally ordered delivery mechanism, which is
described in Section
\ref{ssec:tsae-message-ordering}. %and forms the basis of
                                  %Section\ref{sec:continuous-consistency}.

% The resulting system provides eventual, totally-ordered delivery,
% which offers a general mechanism for implementing replicated state
% machines.

Replication is an alternative to managing state in a central location,
such as a datacenter. This provides resiliency by removing single
points of failure. It supports scalability by allowing any process
with a replia to deliver services to clients, distributing the
load. Finally, replication exploits locality (introduced in Section
\ref{ssec:communication-patterns}) because by servicing user requests
at the nearest replica, we can rely on communications links that are
generally faster and more reliable.

One highly relevant use case of globally replicating state is to
support offline usage of applications. This represents taking locality
to its limit. Responders in the field may not be able to connect to a
datacenter, but in the meantime they can interact with the application
on their device backed by a local replica of the data. This is useful
for as long as the agents can be confident that their replica has not
diverged too far from the rest of the system. A key point is that this
is an application-level matter. How quickly shared state evolves, how
to quantify divergence, and how far apart is ``too'' divergent are
determined by user intent, usage patterns, and operational
environment, among other things. Because updates are merely delivered
\emph{eventually}, TSAE does not bound the divergence of each replica
from the ``ideal'' value of the shared state at any moment. Folling Yu
and Vahdat \cite{2002tact}, we show in Section
\ref{sec:continuous-consistency} how an implementation of the conit
(consistency unit) model on top of TSAE can be used to enforce,
through so-called compulsory anti-entropy sessions, quantitative
consistency requirements separately for each replica.

We now present TSAE in terms of its assumptions
(\ref{ssec:tsae-assumptions}), data structures and invariants
(\ref{ssec:tsae-message-log}--\ref{ssec:tsae-acknowledgement}),
ordering component (\ref{ssec:tsae-message-ordering}), and storage
recycling procedure (\ref{ssec:tsae-message-purging}). Finally, we
discuss how version matrices can be used instead of the loosely
synchronized physical clocks assumed below
(\ref{ssec:tsae-unsynchronized}).

\subsection{Assumptions}
\label{ssec:tsae-assumptions}
We assume each process is associated with an identifier. In practice,
these might be Uniform Resource Identifiers (URIs) \cite{rfc3986}
following a structured format like
\texttt{firefighter116@<agency>.<state>.us}. However, in the text we
will not make a distinction between a process $P$ and its
identifier. The first assumption we make is as follows:
\begin{quote}
  \textbf{Static process group}: The set of group members is a fixed
  set of processes $\AllProc = \{P_1, P_2, \ldots P_n\}$ where the
  identifier of each process is known to all of them.
\end{quote}
Golding's thesis describes how to connect TSAE with a dynamic group
membership management component, but we do not describe it here for
simplicity.

The second assumption we make is that processes have nearly
sychronized clocks. We write $\clock{P, t}$ for the clock value at $P$
at real time $t$.
\begin{quote}
  \textbf{Loose clock sychronization}: Processes' physical clocks are approximately
  synchronized, meaning the clocks of any two processes differ by at
  most some small constant $\delta$:
  \[ \forall t,\, \forall P, Q \in \AllProc, |\clock{P, t} - \clock{Q, t}| < \delta
  \]
  The clock resolution must be fine-grained enough for each principal
  to assign unique timestamps to all important events occurring in
  that process.
\end{quote}

Though physical clock synchronization is not a reliable assumption in
all contexts, we expect that for many useful purposes, sufficient
synchronization can be achieved with a protocol like NTP.  Section
\ref{ssec:tsae-unsynchronized} explains how synchronized physical
clocks can be replaced with version matrices at the cost of a
quadratic storage requirement. For the purpose of implementing
continuous consistency in Section \ref{sec:continuous-consistency},
timestamps from conventional scalar logical clocks can be used in
place of physical clocks, which is how Yu and Vahdat implement TSAE as
part of their TACT framework \cite{2002tact}. The real-time
consistency metric discussed in \ref{ssec:conit-real-time-consistency}
requires each process to keep a physical clock to measure elapsed
time, though synchronization does not appear to be strictly required
here either, assuming each clock runs at a consistent rate. We assume
synchronization here to follow Golding's presentation, highlighting
where the assumption is used.


\subsection{Message Log}
\label{ssec:tsae-message-log}
Each process $P$ manages a \emph{message log} containing all messages
sent or received by $P$.We denote $P$'s message log by $\WL{P}$. It
grows in linear order, as a stack, as new messages are sent and
received. Each message in $\WL{P}$ is tagged with the identifier of
the process that originally created it, with the subset of messages in
$\WL{P}$ that originated at $Q$ denoted by $\WLat{P}{Q}$. Thus,
$\WLat{P}{P}$ represents the set of messages created by $P$. The set
of global messages, $\W$, is defined as the (disjoint) union of
messages originating from every process, which by mild abuse of
notation we are treating as sets:
\[\W \equiv \bigcup_{X \in \AllProc} \WLat{X}{X}.\]

The application at $P$ broadcasts a message $m$ using TSAE by writing
$m$ into $\WL{P}$. We call this event the \emph{submission} of $m$ and
say that $m$ originates at $P$. At the time of submission, the message
is tagged with a pair $(P, \clock{P,t})$ containing $P$'s identifier
and current clock value. The timestamp of $m$ is denoted by
$\timestamp{m}$.

Submitted messages are not sent to other processes right away, but
propagate by a straightforward protocol. Periodically, $P$ contacts
some other process $Q$ and the pair conduct an \emph{anti-entropy
  session}. First, $P$ and $Q$ exchange summary vectors (defined
below). $P$ uses $Q$'s summary vector to quickly calculate which
messages in $\WL{P}$ are not already known to $Q$ and then transmits
these messages to $Q$. Symmetrically, $Q$ forwards messages in
$\WL{Q}$ that $P$ has not seen, resulting in $\WL{P}$ and $\WL{Q}$
containing the same messages afterwards, though typically not in the
same order.\footnote{In a multithreaded environment where $P$ and $Q$
  can engage in multiple anti-entropy sessions with different partners
  at once, the two sides may end in different states. For instance,
  $P$ may learn about new messages in an anti-entropy session with $R$
  during its session with $Q$. Later, we also consider one-sided
  anti-entropy sessions that only push or pull messages. }

TSAE provides reliable eventual delivery, meaning every message is
eventually received by every process, as long as no process fails and
no partition indefinitely separates any two nodes. In particular, if
processes stop originating new messages (so $\W$ remains fixed), if
processes remain able to communicate, then $\WL{P}$ is guaranteed to
eventually converge to $\W$. To achieve this, it is enough that each
pair of processes engage in anti-entropy periodically. This can
actually be relaxed by transitivity: it is enough that they
periodically engage in anti-entropy with a mutual partner, or with two
other processes that engage in anti-entropy with each other, and so
on. TSAE itself does not prescribe a particular policy regarding when
and with whom to engage in anti-entropy, however. Consequently, it
does not guarantee or estimate how far apart $\WL{P}$ is from
$\WL{Q}$, or from $\W$, at any moment. The framework in Section
\ref{sec:continuous-consistency} prescribes a policy for TSAE based on
conservative estimates of divergence in order to enforce these sorts
of limits.

Note that the guarantee of eventual delivery only ensures that
$\WL{P}$ converges to $\W$ as a \emph{set}. However, in general, we
think of $\WL{P}$ as a linear structure, with messages entered into
$\WL{P}$ in the order they were received by $P$. Thus, when $\WL{P}$
and $\WL{Q}$ eventually converge, it is more correct to say they will
be permutations of each other rather than equal. For applications that
require messages to be delivered in a particular order, like a total
order, an additional ordering component is used, as in
\ref{ssec:tsae-message-ordering}.

\begin{example}
  Figure \ref{fig:message-log} shows a message log for containing six
  messages from three processes $A$, $B$, and $C$. For readability it
  is convenient to depict logs whose contents are disaggregated by
  sender (Figure \ref{fig:message-log-b}). Disaggregation has the
  downside of obscuring the fact that message logs with the same
  contents may be arranged in different orders, as the reader should
  keep in mind.
\end{example}

\begin{figure}
  \setlength\belowcaptionskip{5ex}
  \begin{subfigure}{1\textwidth}
    \centering
    \includegraphics[width=0.6\textwidth]{images/MessageLog1.png}
    \caption{A message log with entries accumulating bottom-to-top in linear order}
    \label{fig:message-log-a}
  \end{subfigure}
  \begin{subfigure}{1\textwidth}
    \centering
    \includegraphics[width=0.6\textwidth]{images/MessageLog2.png}
    \caption{The same log shown with messages disaggregated by sender}
    \label{fig:message-log-b}
  \end{subfigure}
  \caption{A message log displayed linearly and disaggregated}
  \label{fig:message-log}
\end{figure}


\subsection{Summary Vector}
\label{ssec:tsae-summary-vectors}
Besides $\WL{P}$, the message propagation component maintains a
\emph{version vector} $\summaryVec{P}$ whose role is to quickly
communicate to other processes which messages $P$ has already
received. This structure is very similar to a vector clock (see
Section \ref{sssec:vector-clocks}) with a few differences. First, the
version of TSAE presented here happens to store physical, rather than
logical, clock values. A more essential distinction is that while a
vector clock tracks causality between events and increments the local
clock with each event, a version vector tracks the history of updates
to replicated data. The value $\summary{P}{P}$ tracks the set of
messages sent by $P$ and is incremented when $P$ sends a new message,
but not when $P$ receives a message from another process. As with
vector clocks, $\summary{P}{Q}$ can be thought of as $P$'s lower bound
view of $\summary{Q}{Q}$.

When $P$ submits a message to its own log, $\summary{P}{P}$ is
advanced to $\clock{P, t}$ and the message is timestamped with this
value. At some point in the future, $P$ contacts some other process
$Q$ to initiate an anti-entropy session. We conceptualize this process
as happening in three phases (setup, message exchange, and conclusion):
\begin{enumerate}
\item $\summary{P}{P}$ is advanced to $\clock{P, t}$. Symmetrically,
  $\summary{Q}{Q}$ is advanced to $\clock{Q, t}$. The parters exchange summary vectors.
\item For each process $X \in \AllProc$, $P$ sends to $Q$ the set of
  messages in $\WLat{P}{X}$ with timestamps greater than
  $\summary{Q}{X}$, if any. Likewise for each $X$ it receives all messages
  from $\WLat{Q}{X}$ with timestamps greater than $\summary{P}{X}$ and
  adds these to $\WLat{P}{X}$. The partners exchange signals to indicate
  when they are finished sending and receiving updates.
\item Much like a vector clock, $\summaryVec{P}$ is updated to the
  pointwise maximum of its current value and the value of
  $\summaryVec{Q}$ received from $Q$. Likewise $Q$ updates
  $\summaryVec{Q}$.
\end{enumerate}
The message log satisfies an invariant, termed the coverage property,
formalizing the role of $\summaryVec{P}$ as a summary of the contents
of $P$'s message log.
\begin{quote}
  \textbf{Coverage Property}: For all $Q$, $P$ has received
  all messages originating at $Q$ whose timestamps are less than the
  $Q^\textrm{th}$ entry in $P$'s summary timestamp vector.
  \[ \{m \in \WLat{Q}{Q} | \timestamp{m} \leq \summary{P}{Q} \} \subseteq \WLat{P}{Q} \]
\end{quote}
The previous subset relation is ``morally'' an equality---messages in
$\WLat{P}{Q}$ but not the subset are said to be ``early''---but only the
the subset property is required for correctness.\footnote{Section
  5.4.3 of Golding's thesis describes how a version of TSAE
  combined with an unreliable network-level multicast for optimization purposes can add
  messages to $\WLat{P}{Q}$ early.} Thus, besides forming a lower
bound of $\summary{Q}{Q}$, $\summary{P}{Q}$ can be thought of as an
\emph{upper} bound of the originating time, measured by $Q$'s clock,
of the last message $P$ received from $Q$. The coverage property
implies that $\summaryVec{P}$ provides complete information about the
(non-early) contents of $\WL{P}$.

The reader should convince themselves that the following inequalities
hold at all times for all $P$ and $Q$ (including when $P = Q$):
\[
  \max_{m \in \WLat{P}{Q}}\left({\timestamp{m}}\right) \leq \summary{P}{Q} \leq \summary{Q}{Q} \leq \clock{Q,t}.
\]

\begin{example}
  \label{ex:tsae}
  Figures \ref{fig:tsae1}---\ref{fig:tsae6} depict the evolution of
  TSAE executing across three distributed processes $A$, $B$, and
  $C$. The figures depict the following chain of events beginning at
  $t = 3$. Note that by $t = 9$, $A$ learns of writes submitted with
  $C$ without performing direct communication with $C$. The figures
  also depict acknowledgment vectors, shown in red, and what we later
  term the commit line, shown underlined. These are explained below.

  \begin{centering}
    \begin{tabular}{rl}\\
      \textbf{Time}    & \textbf{Action} \\
      $t = 1$   & $A$ submits a write                                            \\
      $t = 2$   & $B$ and $C$ submit writes                                      \\
      $t = 3$   & $B$ submits a write                                            \\
      $t = 4$ & $A$ and $B$ begin anti-entropy and swap summary vectors \\
      $t = 5$ & $B$ submits a new write  \\
      $t = 6$ & $A$ and $B$ finish anti-entropy, $C$ submits a write \\
      $t = 7$ & $B$ and $C$ begin anti-entropy, $A$ submits a write \\
      $t = 8$ & $B$ and $C$ finish anti-entropy, $A$ and $B$ begin anti-entropy \\
      $t = 9$ & $A$ and $B$ finish anti-entropy
    \end{tabular}
  \end{centering}
\end{example}

\begin{landscape}
  \begin{figure}%For some reason this empty figure adds vertical whitespace that makes the next figure positioned similarly to the ones that follow it.
  \end{figure}
  \begin{figure}[h]
    \centering
    \includegraphics[width=1.4\textwidth]{images/tsaenew/TSAE3.png}
    \caption{TSAE at time $t=3$.}
    \label{fig:tsae1}
  \end{figure}
  \begin{figure}[h]
    \centering
    \includegraphics[width=1.4\textwidth]{images/tsaenew/TSAE4.png}
    \caption{TSAE at time $t=4$. $A$ advances $\summary{A}{A}$ to $\clock{A, t} = 4$ and likewise for $B$. After exchanging summary vectors, the participants decide to exchange the shaded messages.}
    \label{fig:tsae2}
  \end{figure}
  \begin{figure}[h]
    \centering
    \includegraphics[width=1.4\textwidth]{images/tsaenew/TSAE5.png}
    \caption{TSAE at time $t=5$. $B$ has submitted a message with timestamp
      $t = 5$ while $A$ and $B$ are still engaged in an anti-entropy
      session in the background.}
    \label{fig:tsae3}
  \end{figure}
  \begin{figure}[h]
    \centering
    \includegraphics[width=1.4\textwidth]{images/tsaenew/TSAE6.png}
    \caption{TSAE at time $t=6$. $A$ and $B$ have finished their session and updated their summary vectors. Neither $A$ nor $B$ can update their commit line past $0$ because they both contain $\summary{}{C} = 0$, indicating they have not seen any messages from $C$. $C$ submits a message with timestamp $t = 6$.}
    \label{fig:tsae4}
  \end{figure}
  \begin{figure}[h]
    \centering
    \includegraphics[width=1.4\textwidth]{images/tsaenew/TSAE7.png}
    \caption{TSAE at time $t=7$. $B$ and $C$ update and exchange summary vectors before deciding to exchange the shaded messages. $A$ submits a message with timestamp $t = 7$.}
    \label{fig:tsae4}
  \end{figure}
  \begin{figure}[h]
    \centering
    \includegraphics[width=1.4\textwidth]{images/tsaenew/TSAE8.png}
    \caption{TSAE at time $t=8$. $B$ and $C$ finish their anti-entropy session. Both sides can update their commit line to $4$, since they have seen all messages $m$ such that $\timestamp{m} \leq 4$. $A$ and $B$ begin anti-entropy, exchanging updated summary vectors and exchanging the shaded boxes.}
    \label{fig:tsae6}
  \end{figure}
  \begin{figure}[h]
    \centering
    \includegraphics[width=1.4\textwidth]{images/tsaenew/TSAE9.png}
    \caption{TSAE at time $t=9$. $A$ and $B$ finish their anti-entropy
      session and have both seen all messages with timestampsless than
      or equal to $t = 7$. $B$ and $C$ can remove all messages with
      timestamps less than or equal to $4$ from their logs, since they
      both have minimum entries $\ack{}{C} = 4$. $C$ has received the
      same messages as $A$ and $B$, but has not witnessed $A$
      acknowledging them.}
    \label{fig:tsae6}
  \end{figure}
\end{landscape}

\subsection{Acknowledgement Vector}
\label{ssec:tsae-acknowledgement}
$P$'s summary vector enables another process to quickly learn which
messages $P$ has seen. Additionally, $P$ must keep tabs on which
messages \emph{other} processes have seen. This information is
critical for the message ordering and log recycling components of
TSAE. Golding presents two ways to maintain this information, which
make different efficiency tradeoffs.

Arguably the simplest approach is for $P$ to maintain a local (lower
bound) copy, called $P$'s \emph{view}, of the summary vector of every
other process. During anti-entropy sessions, $P$ exchanges views with
its partner alongside its $\summaryVec{P}$, taking their pointwise
maximums afterwards. This leads to the idea of a version matrix, but
it has the downside of a per-process space requirement this is
quadratic in the size of the process group. This approach is
considered further in \ref{ssec:tsae-unsynchronized}.

The other mechanism presented by Golding requires each process $P$
to maintain an \emph{acknowledgement vector} $\ackVec{P}$. The basic idea
is to coarsely summarize $P$'s knowledge of other processes not with
its summary vector, but the minimal element in this vector, a scalar,
which is stored in $\ack{P}{P}$. For reasons explained in
Section \ref{sec:continuous-consistency}, this value is called $P$'s \emph{commit
  line}. Periodically, $P$'s commit line is updated to the minimal
timestamp in its summary vector,
$\min_{X \in \AllProc} \left(\summary{P}{X}\right)$.  For correctness,
the invariant required of $\ack{P}{P}$ is that it is always a lower
bound of this minimum.
\begin{quote}
  \textbf{Acknowledgement Property}: $P$'s commit line is less than or
  equal to the minimum value in $P$'s summary vector.\footnote{The
    inequality here can be thought of as morally an equality. It may
    be a strict inequality while $P$ is updating $\summaryVec{P}$
    before recomputing its minimum. Recall $P$ may be multi-threaded
    with multiple anti-entropy sessions affecting $\summaryVec{P}$ at
    the same time.}
  \begin{equation*}
    \ack{P}{P} \leq \min_{X \in \AllProc} \left(\summary{P}{X}\right)
\end{equation*}
\end{quote}

Acknowledgment vectors are updated during anti-entropy sessions much
like vector clocks and version vectors.
\begin{enumerate}
\item At the beginning of the session, $\ack{P}{P}$ is updated to
  $\min_{X \in \AllProc} \left(\summary{P}{X}\right)$. Symmetrically
  for $Q$. $P$ and $Q$ exchange acknowledgement vectors alongside
  their summary vectors.
\item At the end of the session, $P$ sets $\ackVec{P}$ and to the
  pointwise maximum of its current value and the value of $\ackVec{Q}$
  received from $Q$. $Q$ updates $\ackVec{Q}$ symmetrically.
\end{enumerate}
Note that $\ack{P}{Q} \leq \ack{Q}{Q}$ at all times for all $P$ and
$Q$.

Slightly different from Golding's presentation of the protocol, the
figures discussed in Example \ref{ex:tsae} also demonstrate an
optimization that takes advantage of the fact that summary vectors are
updated at the end of anti-entropy sessions. After updating these
vectors to their pointwise maximum, we immediately recompute
$\ack{P}{P}$, setting it to
$\min_{X \in \AllProc} \left(\summary{P}{X}\right)$. Furthermore,
$\ack{P}{Q}$ is set to this value. Symmetrically, $Q$ updates
$\ack{Q}{Q}$ and $\ack{Q}{P}$. Note that this preserves the preserves
the invariant that $\ack{P}{Q}$ and $\ack{Q}{P}$ are lower bounds of
$\ack{Q}{Q}$ and $\ack{P}{P}$, respectively, provided the
implementation ensures $P$ increments $\ack{P}{Q}$ after confirming
$Q$ has received all messages from $P$ and vice versa. This
optimization advances acknowledgment vectors (and later, version
matrices) more often than Golding's presentation, which allows the
ordering and purging mechanisms (explained below) to progress more
quickly.

The following two lemmas explain the utility of $\ackVec{P}$ as a way
of estimating global state.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{lemma}
  \label{lem:commitline}
  $P$ has received all messages (from any sender) with timestamps less
  than or equal to its commit line $\ack{P}{P}$.
\end{lemma}
\begin{proof}
  Let message $m$ originate at $Q$ with timestamp
  $\timestamp{m} \leq \ack{P}{P}$. Then
  \[\timestamp{m} \leq \ack{P}{P} \leq \min_{X \in
      \AllProc}\left(\summary{P}{X}\right) \leq\summary{P}{Q}.\] The
  coverage property implies $m \in \WL{P}$.
\end{proof}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{lemma}
  \label{lem:ack-vector}
  For all messages $m$, if $\timestamp{m} \leq \ack{P}{Q}$, then $Q$
  has received $m$.
\end{lemma}
\begin{proof}
  Now $\timestamp{m} \leq \ack{P}{Q} \leq \ack{Q}{Q}$. By Lemma
  \ref{lem:commitline}, $Q$ has received $m$.
\end{proof}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


When a message $m$ in the write log satisfies
$\timestamp{m} \leq \ack{P}{P}$, $P$ is said to have
\emph{acknowledged} $m$. In light of Lemma \ref{lem:commitline}, all
acknowledged messages have been received. The converse does not hold,
since a received message will not be acknowledged until each entry in
$\summaryVec{P}$, not just the entry of its sender, is greater than
$\timestamp{m}$. We now explain how acknowledged messages can be
delivered to the application and ultimately purged from $\WL{P}$,
before considering the matrix-based alternative to acknowledgement
vectors.

\subsection{Message Ordering}
\label{ssec:tsae-message-ordering}
TSAE guarantees that messages will eventually be received by all other
processes, but ordering of these messages in different processes' logs
may vary. In some applications this is acceptable, but other
applications require more control over delivery order. Recall from
Section \ref{ssec:message-ordering} that to enforce ordering
guarantees, a distinction is made between message receipt and
delivery. A message ordering layer buffers incoming messages upon
receipt, giving time for slower messages to catch up to faster ones,
before delivering them to the application in a predictable
order. Below, we describe a simple implementation of totally ordered
delivery for TSAE. Golding also describes straightfoward mechanisms to
enforce causal and total-causal order in Section 5.5 in his
dissertation.

The message ordering component of TSAE periodically inspects $\WL{P}$
and delivers messages to $P$ when ready. To enforce total order, this
component delivers all messages whose timestamp is less than or equal
to $P$'s commit line. That is, the set of messages satisfying
\begin{equation}
  \label{eq:tsae-message-ordering-condition}
  \timestamp{m} \leq \ack{P}{P}.
\end{equation}
These messages are delivered to $P$ in order of their timestamps,
using the identifiers of their senders to resolve ties. We refer to
the combination of TSAE with total ordering as TSAE+TO. This protocol
has the following eventual convergence property.

\begin{lemma}
  TSAE+TO eventually delivers every message to every process in the
  same order.
\end{lemma}
\begin{proof}
  Assuming periodic anti-entropy sessions, the protocol in Section
  \ref{ssec:tsae-message-log} ensures that each message is eventually
  received by $P$ and each value of the form $\summary{P}{X}$
  eventually increases. Thus, the minimum entry in $\summaryVec{P}$,
  and thus $\ack{P}{P}$, will eventually increase beyond
  $\timestamp{m}$, so each $m$ will be delivered as long as the
  ordering component runs periodically. The fact that messages are
  delivered in the same order everywhere follows from Lemma
  \ref{lem:commitline}, since $P$ has seen all messages whose
  timestamp is less than $\ack{P}{P}$, and therefore the stream of
  deliverable messages does not have any ``gaps'' in the total order.
\end{proof}

The above mechanism for enforcing a total order dates back to
Lamport's introduction of scalar clocks \cite{1978:lamportclocks},
which Lamport used to implement a replicated state machine (RSM), a
conceptual device whose status is uniquely determined by a history of
transitions applied to a starting state. Two conditions ensure
replicas converge to the same state. First, the \emph{contents} of
their histories agree (they have seen all the same transitions), and
second, their \emph{orders} agree (they have applied transitions in
the same order). Using TSAE to announce transitions ensures the first
condition, since every process will eventually learn about each
transition. Combining TSAE with a TO delivery mechanism ensures the
second condition, if transitions are applied when announcements are
delivered. This provides a form of \emph{weak} consistency---it
ensures all replicas eventually reach the same state, which makes it
inherently fault-tolerant, though it does not provide guarantees about
the observed inconsistency of replicas in the meantime. We describe
using TSAE+TO for database replication in Section
\ref{sec:continuous-consistency}, using the ``conit'' framework of Yu
and Vahdat \cite{2002tact} to bound observed inconsistency.

Golding's assumption of loose clock synchronization is driven by a
practical need to ensure messages are delivered (and also purged,
described below) in a timely fashion instead of just eventually. For
example, if $Q$ has an exceptionally slow clock compared to other
processes, then $\summary{P}{Q}$ will remain the minimum element in
$\summaryVec{P}$ for a long time. During this time, only messages from
$Q$ will be delivered to $P$, as all other messages would have
timestamps greater than $Q$'s clock. Section
\ref{ssec:tsae-unsynchronized} explains one way, also introduced by
Golding, that the assumption of synchronization can be partially
mitigated. The TSAE-based protocol in Section
\ref{sec:continuous-consistency} does not require synchronized clocks
except to bound real-time staleness.

\subsection{Message Purging}
\label{ssec:tsae-message-purging}
A message simply cannot be removed from the write log after being
delivered to $P$, because some of the messages in $\WL{P}$ may be
propagated to new recipients during anti-entropy in the
future.\footnote{This implies the ordering component must perform
  bookkeeping to remember which messages in the log have already been
  delivered.} However, it is untenable to let the message log grow
without limit. Thus, a separate log recycling process can be used to
remove old entries when they are no longer required.

There are two requirements for a message to be safe to delete. First,
it clearly must have been delivered to $P$ already. Additionally, it
must have been received by all other processes---otherwise it might be
one of the messages $P$ should send in a future anti-entropy
session. $P$ will know a message $m$ has been received by all
other processes when its timestamp is less than or equal to $P$'s
\emph{purge line}, defined as the minimum entry in the acknowledgment
vector:
\begin{equation}
  \label{eq:tsae-message-purging-condition}
  \timestamp{m} \leq \min_{X \in \AllProc} \left( \ack{P}{X} \right)
\end{equation}

The safety of this deletion procedure is proven by the following
lemma.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{lemma}[Log purging]
  \label{lem:purge}
  $P$ can safely discard all messages in $\WL{P}$ with timestamps less
  than or equal to its purge line, after they have been delivered.
\end{lemma}
\begin{proof}
  Let message $m \in \WL{P}$ originate at $R$ with timestamp less than
  $P$'s purge line. Now the following inequalities hold for all $Q$:
  \[ \timestamp{m} \leq \min_{X \in \AllProc}\left(\ack{P}{X}\right)
    \leq \ack{P}{Q}.\] Therefore, by Lemma \ref{lem:ack-vector}, $m$
  has been delivered to $Q$. Since $Q$ is arbitrary, $m$ has been
  received everywhere (where it will eventually be delivered as well)
  and can be purged from $\WL{P}$ to reclaim storage space.
\end{proof}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{comment}
For contrast, suppose $m$ has a timestamp greater than $\ack{P}{Q}$.
Then without knowing the current value of $\summary{Q}{R}$ there is no
guarantee $\timestamp{m}$ is less than this value. In this case,
deleting $m$ from $\WL{P}$ might prevent $Q$ from ever receiving $m$.
\end{comment}

Because every message is eventually delivered and acknowledged by each
process, $P$ will eventually be able to remove each message from its
log. It is still possible for $\WL{P}$ to grow without bound if the
rate of message arrival exceeds the speed at which they are
purged. This might occur during periods of heavy usage or during a
network partition, as $P$ will eventually become unable to advance
$\ack{P}{Q}$ further if $Q$ is on the other side of a partition. The
storage requirements of the message log in a particular use case and
environment should be measured empirically as part of an application
optimization strategy.

\subsection{TSAE using Version Matrices}
\label{ssec:tsae-unsynchronized}
Using a single value,
$\min_{X \in \AllProc} \left(\summary{P}{X}\right)$, to concisely
estimate which messages $P$ has received has certain drawbacks. If
clocks are not approximately synchronized, the clock value at one
process may greatly exceed that of another. If $Q$ has a very fast
clock, this value will remain less than $\summary{P}{Q}$ for a long
time and messages from $Q$ will not be purged. If $Q$ has a slow
clock, only messages from $Q$ will be delivered and purged.

Rather than the minimum entry, the entire summary vector offers a more
precise measure of $\WL{P}$. For $P$ itself, that means tracking what
other process know about by keeping a copy of $\summaryVec{Q}$ for
each $Q$ in the system. Thus, we do away with $\ackVec{P}$ and track
what other processes know using a vector-of-vectors, to say a matrix,
that we call $\ackMatrix{}$. Since $\summaryVec{}$ is a version
vector, then $\ackMatrix{}$ can be called a version matrix.

With this implementation strategy, $\ackMatrix{P}[P]$ stores $P$'s
summary vector, while $\ackMatrix{P}\left[Q\right]$ represents $P$'s
lower bound estimate of $Q$'s summary vector. Thus,
$\ackMatrix{P}[Q][R]$ is $P$'s lower bound estimate of the upper bound
of the timestamp of any message $Q$ has received from $R$. Matrices
are exchange during anti-entropy sessions just as before, with both
sides taking the pointwise maximum after.

We additionally apply an optimization, similar to the one for
$\ackVec{P}$, of recomputing $\ackMatrix{P}$ after confirming $Q$ has
successfully received all messages during anti-entropy. Namely,
$\ackMatrix{P}[Q]$ is advanced to the pointwise maximum of the summary
vectors $\ackMatrix{P}[P]$ and $\ackMatrix{Q}[Q]$ exchanged at the
beginning of the anti-entropy session. This increases $P$'s estimated
knowledge of $Q$ to reflect any new messages $P$ just pushed during
anti-entropy. (The previous value reflected only the messages $Q$ knew
about when the session was initiated.) $Q$ updates $\ackMatrix{Q}[P]$
symmetrically. For purposes of continuous consistency, below, this
subroutine implements the ``view advance'' mechanism used to bound
numerical error.

For message ordering and log recycling, $P$ applies the following
(conservative) policies:
\begin{itemize}
\item A message with timestamp $m$ is ready to be delivered to $P$ by a total
  ordering component when the following analogue of \eqref{eq:tsae-message-ordering-condition} holds:
  \begin{equation}
    \timestamp{m} \leq \min_{X \in \AllProc} \left(\ackMatrix{P}[P][X]\right)
  \end{equation}
\item A message $m$ originating at $R$ has been
  received by $Q$ when the following condition holds:
  \begin{equation}
    \timestamp{m} \leq \ackMatrix{P}[Q][R]
  \end{equation}
\item A message $m$ originating at $R$, after being delivered to $P$,
  can be purged from the log when the following analogue of
  \eqref{eq:tsae-message-purging-condition} holds:
  \begin{equation}
    \timestamp{m} \leq \min_{X \in \AllProc}\ackMatrix{P}[X][R]
  \end{equation}
\end{itemize}
Note that the matrix-based version of TSAE does not necessarily solve
the delivery problem if clocks are unsynchronized: if physical
timestamps are used to totally order all messages in the system,
messages from a host with a slow clock will be delivered before any
other messages.

Of course, matrices require $\Theta(n^2)$ storage at each site for a
process group with $n$ members, which quickly becomes untenable for
systems where $n$ is on the order of 1,000 or greater.

\section{Continuous Consistency}
\label{sec:continuous-consistency}
This section outlines the idea of \emph{continuous} consistency, with
a particular focus on the conit (``consistency unit'') model proposed
by Yu and Vahdat
\cite{2000tact,2000tactalgorithms,10.5555/1251229.1251250,DBLP:conf/icdcs/YuV01,2002tact}.
Rather than viewing consistency as an all-or-nothing proposition, this
model makes the consistency/availability tradeoff first class by
quantifying (in)consistency in general terms and providing controls
that allow ``tuning'' the strength of the model by tightening or
relaxing allowable levels of divergence. These mechanisms support
trading consistency for availability, or vice versa, which might be
driven based on factors like application workload and network
capacity. Such an affordance has advantages for distributed shared
memory across a disruption-prone network, where some amount of
inconsistency is inevitable but also desirable to bound.

We describe the framework now in terms of its system model
(\ref{ssec:conit-system-model}), its three dimensions of consistency
(\ref{ssec:conit-numerical-consistency}---\ref{ssec:conit-real-time-consistency}),
and possible extensions (\ref{ssec:conit-extensions}).



\subsection{System Model}
\label{ssec:conit-system-model}
Continuous consistency is motivated by the observation that real-world
applications can generally tolerate divergence among replicas,
especially in exchange for greater performance, but only up to a
point. Intuitively, applications can operate with replicas that
deviate from their ``ideal'' value by at most some margin of error
$\epsilon \geq 0$. If the divergence is any greater, then the data is
unreliable and the application may become unavailable (or otherwise
alert the user to the unreliability of the data) until consistency is
re-established. In the implementation we focus on, consistency is
re-established through so-called \emph{compulsory} anti-entropy
sessions.

Unfortunately, applications have unique data models and measure error
in different ways, so building a general framework to enforce
quantitative error bounds on replicas without making assumptions about
the nature of the data is challenging. The conit framework strikes a
desirable balance between being general enough to work in applications
with different data models and practical enough to efficiently enforce
consistency with straightforward protocols.

% The model affords flexible methods for applications to tune their
% consistency guarantees, allowing these decisions to be encoded into
% the application logic on a per-process basis, perhaps based on
% feedback and observations from the network.

\subsubsection{Implementation}
The implementation we describe here is the one proposed by Yu and
Vahdat. The foundation of the system is TSAE+TO (see Section
\ref{ssec:tsae-message-ordering}), which is used to propagate updates
to all replicas. This mechanism provides a progress guarantee:
assuming no permanent host or network failures, all updates will
eventually be received, acknowledged, delivered in the same total
order, and purged from the write log of all processes. This implements
a \emph{weak consistency} model, where replicas are allowed to diverge
temporarily, but will eventually converge to an agreement if allowed
to communicate, at least if updates do not outpace the rate of
convergence. However, weak consistency does not say anything
about how far apart replicas might diverge in the meantime.

Viewing TSAE+TO as a way to replicate a state machine, recall that
machines beginning in the same initial state will agree on the final
state assuming all machines have applied the same transitions in the
same order. Thus, there are two reasons a state machine might diverge
from its ``ideal value,'' or the final state after applying all
updates. They are as follows:
\begin{itemize}
\item \textbf{Unseen messages} The error attributable to transitions
  the replica has not applied
\item \textbf{Out of order messages} The error attributable to
  transitions the machine has applied in the wrong order
\end{itemize}
Unseen messages are possible in this framework because an update is
not necessarily propagated to other processes immediately when it is
originated. Out of order messages are possible because replicas do not
(necessarily) wait for the total ordering component to decide the
final order of updates before applying them to their local
replica. Instead, updates are applied when they are received. This may
not match the final order, so the underlying data model must support a
mechanism to rollback changes and reapply them in a different
order. Updates that have been applied without knowing their final
order as said to be \emph{tentative}, and when they are applied in
their final order (possibly after rolling back tentative updates)
they are said to be \emph{committed}.

Thus, to bound the divergence between state machines, what we desire
is a way to place an upper bound on the set of updates that $P$ has
not seen, and an upper bound on how many updates $P$ applies without
knowing their final order. The conit framework actually measures
unseen updates in two ways (numerical error and real-time staleness),
so that consistency is treated as a three-dimensional concept:
\[\textrm{Consistency} = \langle \textrm{Numerical error,
    \textrm{Order error}, \textrm{Real-time staleness}} \rangle.\]

Furthermore, the consistency does not have to be specified on an
entire replica, but on the level of individual \emph{conits} or
``consistency units.'' Conits are a purely conceptual device defined
implicitly by the application. A conits can be defined for an
individual data item, a group of data items, a logical value
calculated from data items, or the whole database. The standard for
correctness of the conit framework is subtle, since it depends on fine
implementation details (such as locking policies) that are not in the
scope of this document. We defer a more detailed discussion until
\ref{ssec:conit-correctness}. Now we make the discussion more formal.

\subsubsection{Formal framework}
The system consists of processes in a group $\AllProc$ that replicate
a shared database or, more generally, any kind of replicated state
machine. The initial database state at each replica is $\Dinit$.

Processes perform two types actions, read or write requests, as in
Section \ref{ssec:shared-memory}. We call actions of both types
\emph{accesses}. Recall that a \emph{history} is an ordered sequence
of read and write accesses. The \emph{local history} of a process $P$,
denoted $\localhistory{P}$, is the sequence of accesses $P$ has
applied to $\Dinit$ since the start of the application.

Writes do not have to be simple instructions, but may be as complex as
database transactions, meaning containing multiple instructions that
might read values and make decisions based on the results. For
example, a write may read a value in the database and make a business
decision whether to increment it. The result of applying write action
$w$ to a database in state $D$ is denoted $D + w$. Read accesses can
be thought of as special write accesses that do not modify the
underlying database state, meaning $D + r = D$ if $r$ is a read. Let
$D + H$ denote the result of applying each of the writes in $H$ to $D$
in history order. Thus, the database state at a process $P$ at any
moment, written $D_P$, is defined
\begin{equation}
  D_P \equiv \Dinit + \localhistory{P}.
\end{equation}

The global history, as a set, is the collection of all local histories
in the system:
\begin{equation}
  \Hglobal \equiv \bigcup_{X \in \AllProc} \localhistory{X}
\end{equation}

\subsubsection{Write Log}
Reads and writes are handled by the protocol differently. Performing a
write access advances the state machine, while a read examines the
current state. Processes handle read and write access differently. A
read access submitted to $P$ by a user is always handled locally by
$P$ without involving other processes. The read action is immediately
performed (while enforcing consistency requirements using the
protocols below) and its value is returned to the user. When a read
request is submitted to $P$, the requested value is read immediately
from $D_P$ and returned to the caller.  With either type of access,
$P$ is said to be the \emph{originating replica}, and all other
processes are \emph{remote}. The value of $D_P$ is said to be the
\emph{observed state} of the access.



Each process $P$ maintains a write log, $\WL{P}$, containing the
history of all writes applied to its local database image in the order
they were applied. Note that this makes $\WL{P}$ equivalent to $\localhistory{P}$
restricted to writes.
\begin{equation}
  D_P = \Dinit + \WL{P} \label{eq:conit-DP}
\end{equation}
When a write request $w$ is submitted, the action is performed against
the local replica and $w$ is added to $\WL{P}$.

 The \emph{ideal result} of an access $a$ is the value
$\Dinit + \PH_\textrm{a}$.
\begin{gather}
  \Dideal = \Dinit + \WLideal \label{eq:conit-Dideal}
\end{gather}

TSAE is used to propagate writes to remote replicas, with $\WL{P}$
acting as the message log. When a remote replica $Q$ receives new
writes, they are added to $\WL{Q}$ and applied to $Q$'s local image as
well.

When a write is first received at any process and applied locally, it
is said to have been applied \emph{tentatively}. Tentative writes are
subject to being rolled back and reapplied in a different order later,
possibly with different results. Eventually, the write becomes
\emph{committed}, after which it is never rolled back. Committed
writes are applied in the same order at all replicas.

Write logs are propagated using TSAE with a total ordering message
ordering mechanism. Previously, we made a distinction between messages
in $\WL{P}$ that have been received but delivered. Here, we say that
messages in $\WL{P}$ are applied \emph{tentatively} until they are
committed. Note that we do \emph{not} wait for the total ordering
mechanism to deliver writes before applying them to $D$---they are
applied (tentatively) immediately. Thus, before $\WL{P}$ is
ordered. Thus, the data model should provide a mechanism for rolling
back (undoing) writes that have been applied to $D$.

\begin{comment}
Taking the place
of the message ordering component, a write committment component is
responsible for ultimately committing writes accesses. Commitment is
an adaptation of the total ordering mechanism in
\ref{ssec:tsae-message-ordering}.
\end{comment}

Push and pull-based anti-entropy. $P$ uses push-based anti-entropy to
send messages to $Q$. This consists of obtaining $\summaryVec{Q}$,
pushing unseen messages to $Q$, and updating $P$'s knowledge of $Q$
(either through $\ackVec{P}$ or $\ackMatrix{P}$) after receiving
confirmation of receipt. Pull-based anti-entropy is symmetric, with
$P$ contacting $Q$ to pull unseen messages, adding them to $\WL{P}$,
and updating $\summaryVec{P}$.


A (read or write) access has a dependency on a set of conits,
$\Conits$. For each process, each conit is associated with a triple
\begin{equation*}
  \langle \NE \left(P, F\right), \OErr(P, F), \RTE(P, F)\rangle
\end{equation*}
specifying $P$'s allowable numerical error, order error, and real-time
staleness error on $F$. These are bounded in different ways, some of
which require proactive cooperation from other processes.

\begin{comment}
\begin{itemize}
\item Numerical error bounds the application-defined ``weight'' of
  messages $P$ hasn't seen. Real-time staleness bounds the maximum
  real time that may elapse before $P$ receives a message. Order error
  bounds the maximum number of writes that are uncommitted and thus
  subject to being rolled back.

  Numerical error bounded by a \emph{push}-approach: $P$ may have to
  block during a request in order to proactively inform $Q$ about the
  update before it can be applied. This consists of $P$ obtaining
  $Q$'s current summary vector and then forwarding messages in $P$'s
  write log that $Q$ hasn't seen.

\item Real time. Take, say, an application for disseminating the most
  up-to-date visualization of the location of a fire front. A client
  may find this it acceptable for this information to appears 30
  seconds out of date to a client, but unacceptable if it is 30
  minutes out of date. For example, firefighters who are very close to
  a fire have a lower tolerance for stale information than a central
  client keeping only a birds-eye view of several fire fronts
  simultaneously. The \emph{real time staleness} metric allows each
  process $P$ to bound the maximum amount of time between an access
  affecting a conit being issued and $P$ seeing that access.

\item Real-time staleness and order error are both bounded by a
  \emph{pull}-based approach: $P$ may have to block during an
  operation while contacting other sites in order to request
  information from them.
\end{itemize}
\end{comment}

\begin{comment}
The initial database state at each replica is $\Dinit$.  Write
accesses encode an action to be performed against a database image
(rather than the final value the data after performing that
action). The result of applying write action $w$ to a database state
$D$ is denoted $D + w$. Let $D + H$ denote the result of applying each
of the writes in $H$ to $D$ in order.  Each process $P$ maintains a
write log, $\WL{P}$, containing the history of all writes applied to
its local database image in the order they were applied (making
$\WL{P}$ equivalent to the local history of $P$ restricted to write
requests). Note that the current state of $P$ is uniquely determined
by a combination of the starting state ($\Dinit$) and $\WL{P}$. In the
above notation, the current value $D_P$ of $P$'s replia is
$\Dinit + \WL{P}$. When a read request is submitted to $P$, the
requested value is read immediately from $D_P$ and returned to the
caller. When a write request $w$ is submitted, the action is performed
against the local replica and $w$ is added to $\WL{P}$. With either
type of access, $P$ is said to be the \emph{originating replica}, and
all other processes are \emph{remote}. The value of $D_P$ is said to
be the \emph{observed state} of the access.  We assume some ECG
history as a reference point. The \emph{ideal result} of an access $a$
is the value $\Dinit + \PH_\textrm{a}$.  The following equations hold.
\end{comment}



\subsection{Numerical consistency}
\label{ssec:conit-numerical-consistency}
We assume each conit $F$ is associated with a valuation function,
\[
  \Val^F\colon \mathsf{Database\ Image} \to \mathbb{R}
\]
that maps a database state to some real number representing the value
of that conit. Each write $w$ is associated with a value,
$\NumWeight(w, F)$, for each conit $F$, codifying the effect of $F$ of
applying $w$ to a database (we assume this value is independent of the
current state $D$):
\[ \NumWeight\left(w, F\right) \equiv \Val^F(D + w) - \Val^F(D). \] Yu
and Vahdat actually \emph{define} a conit as what we have called a
valuation function. We adopt a slightly more abstract terminology and
say that a conit is just an identifier such that each write is
associated with a well-defined weight for each conit. A weight of $0$
indicates that a conit is not affected by $w$.

Let $V_P$ denote the current value of $F$ at $P$'s replica $D_P$. Let
$\Videal$ represents the value of $F$ at $\Dideal$.  Now $F$
distributes over $+$ in the sense that the following holds:
\begin{equation}
  \Val^F(D + w) = \Val^F(D) + \NumWeight(w, F).
\end{equation}
Combining this with Equations \eqref{eq:conit-DP} and
\eqref{eq:conit-Dideal} gives
\begin{gather}
  V_P = \Vinit + \sum \{\NumWeight(w, F) | w \in \WL{P}\} \\
  \Videal = \Vinit + \sum \{\NumWeight(w, F) | w \in \WLideal\}
\end{gather}

The correctness condition required of numerical error is that the
difference between these values is bounded at all times. Let
$\NE(P, F)$ be the allowable numerical error of $P$ for conit $F$. We
seek to enforce the following inequality as an invariant:
\begin{equation}
  | \Videal - V_P | < \NE\left(P, F \right)
\end{equation}
Enforcing this condition requires bounding the set of writes that $P$
has not seen:
\[
  \WLideal \setminus \WL{P}
\]
Loosely speaking, this is accomplished by requiring each process to
maintain approximate knowledge of the contents $\WL{P}$, and used this
estimate to bound its own ``contribution'' towards the writes in
$\WLideal$ that are not in $\WL{P}$.

\subsubsection{Assumptions}
Bounding numerical error requires a cooperation between all
processes. In particular, processes need to know every processes'
numerical error bounds for each conit.
\begin{quote}
  \textbf{Global knowledge of error bounds}: Every process knows
  $\NE\left(P, F \right)$ for each conit $F$ and each process $P$.
\end{quote}
If $P$ wishes to dynamically update its numerical error bounds for a
conit $F$, it must invoke some mechanism to inform all other processes
of this change.
\begin{comment}
  For this reason, though theoretically possible,
individual accesses at $P$ cannot easily specify their own tailored
numerical consistency requirements for each access. Instead, $P$
maintains a requirement per conit.
\end{comment}
We also assume each process $P$ maintains a conservative estimate,
called its \emph{view}, of which writes originating at $P$ have been
received by each other process. $P$'s view of $Q$ is denoted by
$\View{P}{Q}$.
\begin{quote}
  \textbf{Approximate knowledge of remote knowledge}: For each $Q$,
  $P$ can compute a set $\View{P}{Q}$ subject to the following
  invariant:
  \begin{equation*}
    \View{P}{Q} \subseteq \WL{Q}(P)
  \end{equation*}
  $P$ must also implement a mechanism to advance its view by
  synchronizing with $Q$.
\end{quote}
$\View{P}{Q}$ provides $P$ with an estimate of the messages it has
originated that $Q$ has not received yet, namely
$\WLat{P}{P} \setminus \View{P}{Q}$. We describe two ways of
to implement views for the $\ackVec{}$ and $\ackMatrix{}$
implementations of TSAE.
\begin{itemize}
\item If TSAE is implemented with $\ackVec{}{}$, then $P$ maintains an
  additional vector $\lastVec{P}$ where $\last{P}{Q}$ stores the value
  of $\clock{P,t}$ at the time of the last push to $Q$. $P$
  conservatively estimates that $Q$ has not received any messages from $P$
  (via a third party) since $\last{P}{Q}$:
  \begin{equation}
    \View{P}{Q} \equiv \{ m \in \WLat{P}{P} | \timestamp{m} \leq
    \last{P}{Q} \}
  \end{equation}
  While handling a write with timestamp $\clock{P, t}$, after pushing
  messages to $Q$, $\last{P}{Q}$ is advanced to $\clock{P, t}$.
\item If TSAE is implemented with $\ackMatrix{}$, $P$ estimates that $Q$ has
  not seen any messages from $P$ with timestamp newer than
  $\ackMatrix{P}[Q][P]$:
  \begin{equation}
    \View{P}{Q} \equiv \{ m \in \WLat{P}{P} | \timestamp{m} \leq
    \ackMatrix{P}[Q][P] \}
  \end{equation}
  While handling a write update with timestamp $\clock{P, t}$, if $P$
  engages in push-based anti-entropy, $P$'s view is advanced because
  $\ackMatrix{P}[Q][P]$ will have value $\clock{P, t}$.
\end{itemize}
Note that matrices allow $P$ to maintain a view of which updates $Q$
has seen from any $X$, but for the protocol, only an estimate of
$\WLat{Q}{P}$ is required. For the $\ackVec{}$-based implementation of
TSAE, $\ack{P}{Q}$ itself provides a conservative view of $\WL{Q}$,
but it is too coarse: there is no guarantee it will advance after
pushing updates to $Q$, since it is a lower bound of the minimum value
of $\summary{Q}{X}$ for \emph{any} $X$. The advantage of using
$\ackMatrix{}$ over $\lastVec{}$ is that, for example, it allows $P$
to learn from some process $R$ that $Q$ has received messages that
originated at $P$. Entries in $\lastVec{P}$ can only be updated by $P$
itself, making it a coarser estimate of $Q$'s write log, potentially
causing $P$ to push updates more often.

\subsubsection{Split Weight Absolute Error}
We describe \emph{split-weight AE} (absolute error) algorithm, which
Yu and Vahdat explicate most at length in a 2000 paper
\cite{2000tactalgorithms}. We denote the set of writes originating at
$P$ estimated to be unseen by $Q$ as follows:
\begin{equation*}
  \EstUnseen{\left(P,Q\right)} \equiv \WLat{P}{P} \setminus \View{P}{Q}
\end{equation*}
For reasons explained below, we consider writes with positive and negative weights separately, defining the positive and negative unseen weights %TODO
\begin{gather}
  \EstUnseen{\left(P,Q\right)} \equiv \{ w \in  \EstUnseen{\left(P,Q\right)} \}
\end{gather}
For each $F$, define the following values totalling the positive and
negative weights to conit $F$ of writes from $P$ thought to be unseen by $Q$.
\begin{gather}
  \twp{P}{Q, F} \equiv \sum_{\substack{
    w \in \EstUnseen{\left(P,Q\right)} \\
    \NumWeight\left(w, F\right) > 0}} \NumWeight\left(w, F\right)\\
  \twn{P}{Q, F} \equiv \sum_{\substack{
    w \in \EstUnseen{\left(P,Q\right)} \\
    \NumWeight\left(w, F\right) < 0}} \NumWeight\left(w, F\right)
\end{gather}

To bound numerical error, the intuition is that $P$'s estimate of
unseen numerical weight must not exceed $Q$'s allowable error. Since
there are $|\AllProc| - 1$ processes that can accept writes without
immediately adding them to $\WL{P}$, this means the total weight of
messages accepted by each process locally and not seen by $P$ must not
exceed $\frac{\NE\left(P, F \right)}{|\AllProc| - 1}$.  This value
represents $P$'s share of allowable weight of writes accepted by
processes other than $Q$ that are unseen by $Q$. When $P$ submits a
new write, $P$ first checks for whether the following conditions hold:
\begin{align*}
  \twp{P}{Q, F} + \NumWeight(w, F) &< \frac{\NE\left(Q, F\right)}{|\AllProc| - 1} & \textrm{if $\NumWeight(w, F) > 0$} \\
  \twn{P}{Q, F} + \NumWeight(w, F) &> - \frac{\NE\left(Q, F\right)}{|\AllProc| - 1} & \textrm{if $\NumWeight(w, F) < 0$}
\end{align*}
If either of these conditions are violated, then $P$ updates its view
of $Q$, which might involve sending updates that $Q$ has not seen.

\begin{lemma}[Numerical weight correctness]
  When following the above protocol, then each value of each conit at process will satisfy
  \[ | \Videal - V_{P} | < \NE{(P, F)}. \]
\end{lemma}
\begin{proof}
  \begin{align*}
    \Videal - V_{P} &= \\
                    &= \\
                    &> \\
                    &>
  \end{align*}
\end{proof}

Because $P$'s view is conservative, positive and negative weights must
be totalled separately, without letting writes with positive weight
``cancel out'' writes with negative weight. For example, suppose $P$
has accepted writes with positive and negative weights that exactly
cancel out, and suppose $Q$ has actually seen, unbeknownst to $P$, all
of the writes with negative weight. If $P$ totalled all weights
together, it would wrongly estimate that the weight of writes unseen
by $Q$ is $0$, when it is actually a positive value that may exceed
$Q$'s error bounds.

\paragraph{Variations} Yu and Vahdat also describe two other schemes
for bounding numerical error. \emph{Compound-weight absolute error} is
similar to split-weight AE, but allows positive and negative weights
to cancel out. This method represents a potential tradeoff between
storage space and communication, but they did not find the savings
particularly compelling in their performance analysis. They also
consider a scheme, \emph{inductive relative error}, which bounds the
relative error $|1 - \frac{V_i}{V_{\textrm{final}}}|$. Notably, this
algorithm only relies on $P$'s local knowledge. This is particularly
interesting because it is not obvious how to efficiently bound
relative error without knowing the ideal value $V_{\textrm{final}}$.

\begin{figure}
  \centering
  \includegraphics[width=0.6\textwidth]{images/conit/Numerical1.png}
  \caption{Write log $\WL{A}$ for Example \ref{ex:conit-numerical}, where
    messages with a grid background represent $A$'s view of $\WL{C}$.}
  \label{fig:conit-numerical}
\end{figure}

\begin{example}
  \label{ex:conit-numerical}
  Figure \ref{fig:conit-numerical} depicts the role of numerical
  weight for a process $A$.  Messages with a grid background represent
  $A$'s view of $\WL{C}$. We are assuming here that matrices are used,
  so $A$ has a view of $\WLat{C}{B}$, this does not play a role as
  $A$ only works to bound the weight of its own writes unseen by $C$.

  $A$ estimates there are two messages, with timestamps $(A, 6)$ and
  $(A, 9)$, in $\WLat{A}{A}$ that are not in $\WLat{C}{A}$. We assume
  both of these updates apply positive weight to a conit $F$. While
  handling access $(A, 9)$, before returning to the user, the combined
  weight of the updates must be less than
  \mbox{$\NE\left(C, F \right)/ 2$}, reflecting $A$'s ``share'' of the
  allowable error. If this limit is exceeded, $A$ would become
  unavailable and engage in compulsory anti-entropy with $C$. Note
  that if $A$ is multithreaded and can handle multiple accesses at
  once, then $A$ only has to be unavailable for further access
  involving the same underlying conits. Reads and writes only
  affecting other conits are not blocked.
\end{example}

\subsection{Order consistency}
\label{ssec:conit-order-consistency}
The write log $\WL{P}$ is a linearly ordered structure, and the local
replica state $D_P = \Dinit + \WL{P}$ reflects this order. Writes are
added to $\WL{P}$ and applied to $D_P$ as they are received, instead
of waiting for the totally-ordered delivery component to execute, so
at any moment it is possible that $P$'s state machine has diverged
from its ideal value, beginning at the first access $P$ applied that
wasn't in the final order.

\begin{comment}
TSAE+TO guarantees that eventually writes will be committed, and the
order of committed writes in the same everywhere. Until they are
committed, the tentative writes might have been applied in the wrong
order. Writes applied in the wrong order will have to be rolled
back. The rolling back process may involve application-specified logic
and be associated with a cost, so tentative writes represent a
liability. The goal of ordered consistency to is to bound this
liability.

Messages are initially received and applied tentatively to
$D_P$ in an uncommitted state. Eventually they will become committed
in a common global order, but the relative position of each tentative
write in the final order is not fixed.
\end{comment}

\begin{example}
  \label{ex:conit-booking}
  Suppose $P$ and $Q$ are two web servers that allow clients to
  reserve individual seats for a flight, and suppose both servers
  independently process requests from clients to reserve a ticket for
  seat \#1 at the same time. For efficiency, $P$ and $Q$ do not
  maintain linearizability, so they both issue tickets and double book
  the seat. Let these reservations correspond to two write updates
  $w_P$ and $w_Q$. The local replicas are left in state $\Dinit + w_P$
  and $\Dinit + w_Q$.

  TSAE promises that eventually $P$ and $Q$ learn about each
  other's updates and commit them in some agreed upon order. Let us
  arbitrarily suppose $P$'s update is ordered first.  Thus, $P$
  advances its state to \mbox{$\Dinit + w_P + w_Q$}, while $Q$ rolls
  back its state and reapplies its write, which we represent a series
  of transitions:
  \begin{equation*}
    \Dinit + w_Q \xmapsto{\textrm{rollback}} \Dinit \xmapsto{\textrm{reapply}} \Dinit + w_P +
    w_Q
  \end{equation*}

  Recall that $w_P$ and $w_Q$ represent transactions and may contain
  business logic.  Thus, the final database state $\Dinit + w_P + w_Q$
  might have the interpretation, ``Seat \#1 is reserved to $P$'s
  client, while $Q$'s client attempts to double book the seat and is
  issued a refund.'' The rollback process at $Q$ may be associated
  with business rules triggering a notification to $Q$'s client that
  their reservation has been cancelled, alongside their refund and
  compensation fee.
\end{example}

In Example \ref{ex:conit-booking}, accepting $w_Q$ in a tentative
state represented a liability for $Q$. An indirect way this liability
might have been prevented is if $Q$ had enforced $0$ numerical error,
but this is not a full solution. For instance, another reservation
server $R$ might be notified about $w_Q$ and $w_P$ but apply them in
the wrong order to end up in state $D_R = \Dinit + w_Q + w_P$ where
$Q$'s client appears to get the reservation first. This state, which
has $0$ numerical error, would persist at $R$ until the total ordering
component discovers that $w_P$ is first in the final order. Numerical
error only controls the updates a process sees, but not their order.

A more direct approach to controlling the potential cost of rollbacks
is to bound the order error of the local database state. For each
conit $F$, each write is associated with a real value
$\OrderWeight{(w, F)}$. In the example above, this value might be the
total cost of refunding the ticket in case $w$ has to be reverted. The
allowable order error acts as an upper bound on the sum of the order
weight of writes that appear locally in the wrong order.

Unlike numerical error, order error can be bound without direct
cooperation from other processes. We only need to assume that
processes, when required, can invoke a write commitment mechanism to
learn the final order of their tentatively accepted updates.
\begin{quote}
  \textbf{Ability to commit writes}: While handling an access, each
  process has a mechanism to determine the final order of any
  uncommitted writes in its write log.
\end{quote}
We will describe how to implement write commitment with TSAE below.

An access received by $P$ observes the state, called its
\emph{observed prefix history}, of $\WL{P}$ at the time the access is
accepted. Let this value be denoted $\PHobs$. The same access occurs
in an ideal history, where the set of writes preceding it consitutes
its \emph{ideal prefix history}, $\PHideal$. Given two histories $H$
and $H'$, let their \emph{intersection} or \emph{greatest common
  prefix}, $H \cap H'$, be the set of all accesses they agree on since
their beginning. The \emph{errant history} of an access is defined as
all of the writes at the tail end of $\PHobs$, beginning at the first
write where $\PHobs$ does not agree with $\PHideal$:
\begin{equation*}
 \PHobs - \left(\PHobs \cap
   \PHideal\right)
\end{equation*}

\begin{figure}
  \begin{subfigure}{0.5\textwidth}
    \centering
    \includegraphics[width=0.5\textwidth]{images/conit/Order1.png}
    \caption{$\WL{A}$ shown with $5$ uncommitted writes}
    \label{fig:conit-order-a}
  \end{subfigure}%
  \begin{subfigure}{0.5\textwidth}
    \centering
    \includegraphics[width=0.5\textwidth]{images/conit/Order2.png}
    \caption{$\WL{A}$ shown at a later time}
    \label{fig:conit-order-b}
  \end{subfigure}
  \caption{Logs for Example \ref{ex:conit-order} where committed writes are drawn with a dotted background}
  \label{fig:conit-order}
\end{figure}

\begin{example}
  \label{ex:conit-order}
  Figure \ref{fig:conit-order} depicts two write logs where committed
  writes are shown with a dotted grid pattern. In Figure
  \ref{fig:conit-order-a}, five writes are uncommitted. No write with
  timestamp greater than or equal to $3$ can be committed, since the
  minimum entry in the log, $\summary{A}{C}$, is only $2$. The final order
  of these accesses is shown in Figure \ref{fig:conit-order-b}. For
  the next access, $(A, 11)$, submitted to $A$, the errant history
  consists of all accesses after and including $(A, 6)$, the point at
  which the write log deviates from the ideal history.
  \begin{align*}
    &\PHobs \cap \PHideal &&= \{ (B, 1) \ldots (A, 3) \} \\
    &\PHobs - \left(\PHobs \cap \PHideal\right) &&= \{ (A, 6), (B, 4), (B, 5), (A, 9) \}
  \end{align*}
\end{example}

Let $r$ be a read access submitted to $P$ with a dependency on conit
set $\mathcal{F}$.  Let $\Uncommitted{P}$ represent the uncommitted
messages in $\WL{P}$.  Since committed writes are a subset of the
ideal prefix history, the writes in the errant history are a subset of
the uncommitted ones:
\begin{equation}
  \PHobs - \left(\PHobs \cap \PHideal\right) \subseteq \Uncommitted{P}
\end{equation}


For a read access that depends on conits in a set $\Conits$, the
relative order of updates applied to $D_P$ only matters if they affect
(i.e. apply non-zero order weight to) one or more of the conits in
$\Conits$. In fact, this can be considered an axiomatic constraint on
how applications should define conits and order weight. The errant
history overstates the difference between $\PHobs$ and $\PHideal$. For
instance, suppose they only differ in the relative order of updates
applied to conits that do not affect $\Conits$; in this case, a read
depending on $\Conits$ should have $0$ order error. Let $H|_\Conits$
denote the \emph{write-order projection} of $H$ to $\Conits$, which
consists of just the writes $w$ such that $\OrderWeight(w, F) \neq 0$
for at least one $F$ in $\Conits$. We the define the
\emph{out-of-order} accesses to $\Conits$,
$\mathsf{OOO}\left(\Conits\right)$, as the set of all writes affecting
any conit in $\Conits$ that have been applied in an order deviating
from their final order relative to each other:
\begin{equation}
  \mathsf{OOO}\left(\Conits\right)  = \PHobs|_\Conits - \left(\PHobs|_\Conits \cap \PHideal|_\Conits \right)
\end{equation}
The reader may want to verify with an example that computing the
errant history and then restricting to writes that affect $\Conits$ is
not correct, since the errant history is affected by conits that do
not affect $\Conits$.

The order error of an access is the total order weight of out of order
writes to any of the conits it depends on.
\begin{definition}[Order error]
  The \emph{order error} of an access $r$ depending on a conit set
  $\mathcal{F}$ is the sum of $\OrderWeight\left(w, F\right)$ for
  each $F$ in $\Conits$, for all writes $w$ that are in the out-of-order
  set for $\mathcal{F}$.
  \begin{equation}
    \OrderError\left(r, \mathcal{F}\right) \equiv \sum_{F \in \Conits} \{ \OrderWeight\left(w, F\right) | w \in \mathsf{OOO}\left(\Conits\right) \}
  \end{equation}
\end{definition}


Because the committed writes are never out of order, the out of order
writes form a subset of the uncommitted writes. Thus, $P$ can estimate
the order error of an access by totalling the order weight of accesses
to $\Conits$ that have not been committed:
\begin{equation}
  \EstOrderError\left(r, \mathcal{F}\right) \equiv \sum_{F \in \Conits} \{ \OrderWeight\left(w, F\right) | w \in \Uncommitted{P}|_\Conits \}
\end{equation}
Note that $\mathsf{OOO}\left(\Conits\right) \subseteq \Uncommitted{P}|_\Conits$ implies $\OrderError\left(r, \mathcal{F}\right) \leq \EstOrderError\left(r, \mathcal{F}\right)$.

Let $\OErr$ be the allowable upper bound of
$\mathsf{OrderError}\left(r,\Conits\right)$ for an access $r$. While
handling this access, if the value above exceeds $\OErr(P, \Conits)$,
the $P$ engages the write committment procedure assumed earlier to
commit enough writes in its log to reduce the size of estimated order
error.

\begin{comment}
et $\PHobs$ be the observed
prefix history. Now $\PHobs \cap \PHideal$,
the common prefix of the observed and ideal history, represents the
longest part of $\PHobs$ that is correct (i.e. agrees with
the ideal history). Therefore,
$\PHobs \setminus \left(\PHobs \cap
  \PHideal\right)$ represents all of the accesses that
have been applied to $P$'s local replica that will have to be rolled
back during the eventual write committment process.
\end{comment}



This is implemented with a \emph{pull-based} approach, to decide the
final order of enough writes until the order error is within
bounds. For the TSAE-based implementation, $P$ can commit writes by
engaging in pull-based anti-entropy to advance its commit line
($\min_{X \in \AllProc} \left(\summary{P}{X}\right)$) far enough for
the total ordering component to decide the final value of all writes
in its log, at which point any errant updates can be rolled back and
reapplied in their final order. Let $t$ be the greatest timestamp of
any message in its write log. To guarantee that $P$ can advance its
commit line enough to commit the weight, it is enough to ensure that
$\summary{P}{Q}$ is greater than $t$ after engaging in anti-entropy
with each $Q$. This is true of physically synchronized clocks, since
$\summary{P}{Q}$ will reflect the time at which $Q$ was contacted by
$P$. This also works with scalar logical clocks, since $Q$ would
increment its clock to be greater than $P$'s clock.

\begin{example}
  Suppose in Figure \ref{fig:conit-order-a} that the next access
  submitted to $A$ is timestamped $(A, 11)$. Suppose all writes apply
  unit order weight to a conit $F$. If $(A, 11)$ depends on $F$ with
  an allowable order error of less than $5$, the number of uncommitted
  writes, then the submission of $(A, 11)$ will not return until $A$
  pulls writes from $B$ and $C$ so that all messages up to $(A, 9)$
  can be committed. Note that the actual order error is $4$, less than
  the estimate of $5$.
\end{example}

\subsection{Real time staleness}
\label{ssec:conit-real-time-consistency}
The \emph{real time staleness} metric allows each process $P$ to bound
the maximum amount of time between an access affecting a conit being
issued and $P$ seeing that access. Here we rely on the assumption that
$\summaryVec{P}$ stores physical timestamps from loosely synchronized
physical clocks, though this assumption can be weakened. Let $P$ have
an upper bound of $\eprt \geq 0$ on the real-time staleness of a conit
$F$. Here, $\eprt$ should be greater than the time it takes for $P$ to
engage in a typical anti-entropy session. The rule for enforcing
real-time bounds is very simple.

While handling an access with a read dependency on $F$, submitted at
time $t$, $P$ checks for each $X \in \AllProc$ whether
$|\clock{P, t} - \summary{P}{X}| < \eprt$ is true. If it is not true,
then $P$ engages in a pull-based anti-entropy session with $X$, at
which point $\summary{P}{X}$ has value $\clock{X, t'}$ for some
$t' > t$. Note that the assumption of loose synchronization implies
$\clock{X, t'} \approx \clock{P, t}$ if $t \approx t'$. This protocol
ensures that the original access, timestamped with value
$\clock{P,t}$, will observe the effect of all writes affecting $F$
with timestamps less than $\clock{P,t} - \eprt$.

A pull-based protocol may seem wasteful because $P$ may poll $X$ for
updates even if $X$ does not have any new writes. However, an approach
where $X$ pushes updates to $P$ cannot bound real-time staleness
without an upper bound on the time it takes to push messages across
the network to $P$.

If clocks are not loosely synchronized, an alternative implementation
strategy is for $P$ to maintain a vector $\vtphys{P}{}$ where
$\vtphys{P}[X]$ stores the value $\clock{P,t}$ of $P$'s clock at the
last time $P$ was on the receiving end of an anti-entropy session
directly with $X$. Then $P$ enforces consistency by ensuring that
$|\clock{P, t} - \vtphys{P}{[X]}| < \eprt$ is true before handling an
access submitted at time $t$. Since this approach only compares values
from $P$'s clock, synchronization is not required assuming $\clock{P}$
runs at a constant rate. However, this approach has the downside that,
unlike $\summary{P}{Q}$, $\vtphys{P}[Q]$ cannot be updated based on
information indirectly learned during anti-entropy with a third
party.

\subsection{Correctness Properties}
\label{ssec:conit-correctness}
To define quantify deviation from strong consistency, we must give a
few definitions. Recall the notion of external order (Definition
\ref{def:external-order}): an access $A$ externally precedes $A'$ if
$A$ has returned to the caller before $A'$ is invoked. Yu and Vahdat
also define a notion of causality:
\begin{definition}[Causal precedence of accesses]
  \label{def:conit-causal-precedence}
  $A$ \emph{causally precedes} $A'$ if $A$ was already in the local
  history of the originating replica when $A'$ was submitted, and so
  could have influenced how $A'$ executed.\footnote{At face value it
    may seem that if $A$ is in a replica's write log before $A'$, then
    it must have completed before $A'$ started and thus already
    precedes $A'$ externally. However, if these represent concurrent
    but isolated database transactions, it is possible that $A$
    logically takes effect before $A'$, although the transactions have
    no external order relation.}
\end{definition}

With these definitions, we can define a strongly consistent execution
as a reference point for measuring deviation. An ECG history is
essentially a linearization of all accesses in the system.

\begin{definition}[Ideal image]
  An ECG (externally consistent, causally consistent, global) history
  is the set of accesses across the system arranged in some total
  order that respects both external and causal order as defined above.
\end{definition}

\begin{comment}
Definitions below are given with respect to some ECG
history. Intuitively, the condition expected from the conit framework
is that at any moment in time, there is some ECG history such that all
replicas diverge from the ECG history by at most some error bound.
We assume some ECG history as a reference
point.
\end{comment}
Yu and Vahdat describe two implementations of write propagation that
differ in their guarantees. The first involves a two-phase locking
procedure where, during write propagation, remote locks covering all
of the affected data items are first acquired from all hosts where one
intends to send write updates. Once all locks have been acquired, all
necessary writes are sent. While locks are held, reads from affected
conits are blocked on those remotes. When the writes are propagated,
locks are released. This two-phase locking mechanism, which is also
used in a conventional implementation of linearizability, ensures that
the propagated write operations appear to take effect at all
destinations at the same time. When writes are propagated this way,
the numerical and order weights of all accesses are properly bounded
with respect to their ideal values in a strongly consistent history.
\begin{theorem}[Yu and Vahdat]
  Assuming the two-phase locking policy is used during compulsory
  write propgation, there is an ECG history such observed numerical
  and order error of every access is correctly bounded with respect to
  its ideal return value in the ECG history.
\end{theorem}

The following theorems are Theorems 4.1 and 4.3 of their 2002 paper
\cite{2002tact}. Note that ``strict serializability'' here is
essentially synonymous with linearizability.\footnote{Strict
  serializability is essentially linearizability for replicated
  databases. The condition also implies serializable isolation of
  transactions, which is a concept specific to the database context.}
\begin{theorem}
  If a conit is defined for each data item, if reads have 0 allowable
  numerical error and order error on all read conits, and all writes
  have unit weight on all affected conits, then the system provides
  strict serializability.
\end{theorem}

Allowing unbounded numerical error provides one-copy serializability
(1SR) \citationneeded. The condition roughly states that the history
is consistent with some totally ordered history, but the history does
not have to respect real-time order.\footnote{Thus, one-copy
  serializability is something of an analogue of sequential
  consistency for replicated databases, where the primitive unit of
  operations is a database transaction.}
\begin{theorem}
  If reads have 0 allowable order error and no bounds on their
  numerical error, and all writes have unit weight on all affected
  conits, then the systems provides one-copy serializability.
\end{theorem}

Yu and Vahdat also consider a lazy propagation method where remote
locks are not acquired before pushing writes. This allows some
replicas to seemingly learn about writes (meaning it is visible by
read accesses) before others. That is, this policy does not enforce
atomic visibility (updates appear to multiple items, in this case
multiple replicas, appear to occur at different physical times instead
of all at once). This is a weaker consistency model but may be
valuable for certain kinds of information, particularly for
applications such that accepting new updates is always valuable, even
if the update has not propagated to others yet. In this case, the
guarantee provided by bounding numerical error is that the weight of
unseen \emph{finished} writes (meaning writes that returned to the
client at their originating replica) never exceeds the allowable
numerical error for each conit at each replica.

\begin{comment}
  Yu and Vahdat also consider a one-round Read-One-Write-All (ROWA)
  policy where writes are pushed lazily to all compulsory remote
  destinations, without acquiring locks on the data items first. The
  originally submitted write access does not return to the client until
  the replica has finished all compulsory push-based entropy
  sessions. However, this policy allows some replicas to reflect the
  newly seen writes before others.

The intention of the conit framework is this: at any moment in time,
there exists some ECG history of system accesses such that no
individual replica deviates from too far this history. To define what
it means to deviate, we now define conceptual units of data whose
relative (in)consistency can be quantified.

A read request depends on a set of conits $\mathcal{F}$. That is to
say, it assumed that the read will return the same value for any two
database images $D$ and $D'$ such that $F(D) - F(D') = 0$ for each
conit $F$ in $\mathcal{F}$. This access will be associated with a
consistency requirement for each conit in $\mathcal{F}$.

We write $\WL{P}|_\mathcal{F}$ to indicate the history of $P$
restricted to just the messages with non-zero weight on any of the
conits in the set $\mathcal{F}$.

Anti-entropy can be push- or pull-based. To push updates from $P$ to
$Q$, $P$ obtains $Q$'s summary vector $\summaryVec{Q}$ and pushes all
unseen messages in $\WL{P}$ to $Q$. To pull updates, $P$ sends its
version vector and requests any messages in $\WL{Q}$ that $P$ has not
seen.
\end{comment}

\subsection{Extensions}
\label{ssec:conit-extensions}
One can imagine various ways that the conit model can be augmented
with additional capabilities. These topics are outside the scope of
this memo but offered for future consideration.

\paragraph{Dynamic bounds}
Because real-time staleness and order error are bounded by pull-based
anti-entropy sessions, it is straightforward to allow the user to
dynamically change the error bounds at each site. However, numerical
error is bounded by a push-based approach that requires every process
to be track all other processes' error bounds and cooperate to enforce
it. Therefore, dynamically tuning numerical $P$'s numerical error
bounds requires a consensus mechanism so that $P$ can inform other
processes any changes to its error bounds. However, $P$ cannot be sure
its new bounds will be respected until every process has acknowledged
the update.

Yu and Vahdat do not propose a particular mechanism for consensus. One
possible approach may be to reuse the existing message-propagating
mechanism to announce changes to error bounds, similar to Golding's
approach for handling dynamic group membership.

\paragraph{Dynamic conits}
The framework we have described above assumes the set of conits is
fixed in advance. Besides tuning bounds dynamically, we can imagine
situations where new conits need to be created on-the-fly. For
instance, data related to a new wildfire that has emerged may require
forming new conits to set consistency bounds.

Donkervliet's master's thesis \cite{dyconits} explored the subject of
dynamic conit creation in the context of massive multiplayer online
games, particularly Minecraft. In that work, new conits may be
associated with newly encountered objects in an area, and their
consistency bounds tuned as the user approaches them, exploiting a
form of locality to allocate network resources for information the
where inconsistency would most readily be perceived by the
player. Adapting their \emph{dyconits} (dynamic conits) framework to
the wide-area tactical environment may be worthwhile.

\paragraph{Interaction with the Network}
We mentioned in Section \ref{sssec:allocation-of-network-resource} our
expectation of a tighter, more complex interaction between the network
and application layers in this environment because of the need to
optimize scarce network resources for the most important
information. Because conits can quantify divergence, and therefore
indirectly the relative importance of an update, one way this might be
realized is by making the network conit-aware.

Network packets, or DTN bundles, could be specially marked as
containing database updates alongside any metadata (such as the weight
of an update to various conits) that could be used by the network for
quality-of-service purposes. Such usage may run contrary to the
conventional wisdom that networking protocols should be agnostic to
the actual content of a message, e.g. routers should be concerned only
with the data in IP packet headers but not the data contained in the
packet. This sort of atypical usage is potentially justified in our
setting because of a heightened requirement to optimize the user of
very scarce networking resources, even at the cost of blurring the
line between the network and application layers.

Modifying a network protocol to optimize a particular middleware or
application is not a lightweight task, particularly since network
drivers are often embedded into an operating system kernel or into
hardware---this makes their modification difficult or at least
fraught. We conjecture that SDN would be particularly suitable because
it is easier to modify or customize software-defined networking
protocols, so that custom hardware is not required even for extremely
specialized networking needs. The ability to design network protocols
at the level of conventional application software, rather than baking
them into hardware, might offer the flexibility to experiment with
variations of protocols.

We previously mentioned an example where a UAV or a message ferry
could be deployed dynamically to provide greater throughput in a
particular geographical area. Such a resource could be dispatched if
the application signals to a network controller that it is struggling
to enforce conit bounds in a timely manner.

Yu and Vahdat discuss quality of service as one of the features that
might itself use conits.



%\section{Conclusion and Summary}
%\label{sec:conclusion}

\section*{Bibliography}\label{bibliography}
\addcontentsline{toc}{section}{Bibliography}

\bibliographystyle{abbrv}
\bibliography{bibliography}
\end{document}

%%% Local Variables:
%%% mode: LaTeX
%%% TeX-master: t
%%% End:
