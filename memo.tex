% !TeX document-id = {beb7ced9-b3cd-42b2-b16a-3ed3c633a1d9}
\documentclass[]             % options: RDPonly, coveronly, nocover
{NASA}                       %   plus standard article class options
%\DeclareRobustCommand{\mmodels}{\mathrel{|}\joinrel\Relbar}

\usepackage[utf8]{inputenc}
\usepackage{csquotes}
\usepackage{setspace}
\usepackage{hyperref}
\usepackage{amsmath, amssymb, amscd, amsthm, amsfonts}
\usepackage{mathtools}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{amsthm}
\usepackage[english]{babel}
\usepackage{stmaryrd}
\usepackage{proof}
\usepackage{tikz-cd}
\tikzcdset{scale cd/.style={every label/.append style={scale=#1},
    cells={nodes={scale=#1}}}}
% Added for subfigures
\usepackage{caption}
\usepackage{subcaption}
\usepackage{afterpage}
\usepackage{comment}

\newtheorem{theorem}{Theorem}[section]
\newtheorem{corollary}{Corollary}[theorem]
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{example}{Example}
\theoremstyle{definition}
\newtheorem{definition}{Definition}[section]

\newcommand{\B}{\mathbf{B}}
\newcommand{\w}{\mathbf{w}}
\newcommand{\citationneeded}{\textbf{[CITATION NEEDED]}}

% Globally redefine pgfpicture to use \Large fonts
\let\origpgfpicture=\pgfpicture
\def\pgfpicture{\origpgfpicture\small}

% Try loading this package to prevent so much hyphenation
% as recommended by https://stackoverflow.com/questions/1609837/latex-breaking-up-too-many-words
\usepackage{microtype}

% Try setting matrix columns closer
\setlength\arraycolsep{2pt}

\title{A Survey of Distributed Systems Challenges for Wildland
  Firefighting and Disaster Response}

\author{Lawrence Dunn and Alwyn E. Goodloe}

\AuthorAffiliation{Lawrence Dunn \\ Department of Computer and Information
  Science \\ University of Pennsylvania \\ Philadelphia, PA \\ Alwyn Goodloe\\                                          % for cover page
  NASA Langley Research Center, Hampton, Virginia
}
\NasaCenter{Langley Research Center\\Hampton, Virginia 23681-2199}
\Type{TM}                    % TM, TP, CR, CP, SP, TT
\SubjectCategory{64}         % two digit number
\LNumber{XXXXX}              % Langley L-number
\Number{XXXXXX}              % Report number
\Month{12}                   % two digit number
\Year{2022}                  % four digit number
\SubjectTerms{Distributed Systems, Formal Methods, Logic, }     % 4-5 comma separated words
\Pages{46}                   % all the pages from the front to back covers
\DatesCovered{}              % 10/2000--9/2002
\ContractNumber{}            % NAS1-12345
\GrantNumber{}               % NAG1-1234
\ProgramElementNumber{}
\ProjectNumber{}             % NCC1-123
\TaskNumber{}                % Task 123
\WorkUnitNumber{}            % 123-45-67-89
\SupplementaryNotes{}
\Acknowledgment{The work was conducted during a summer internship at the NASA Langley Research Center in the Safety-Critical Avionics Systems Branch focusing on distributed computing  issues arising in the Safety Demonstrator challenge in the NASA Aeronautics System Wide Safety (SWS) program.}

%Added for Pandoc
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}


\abstract{The System Wide Safety (SWS) program has been investigating
  how crewed and uncrewed aircraft can safely operate in shared
  airspace. Enforcing safety requirements for distributed agents
  requires coordination by passing messages over a communication
  network. Unfortunately, the operational environment will not admit
  reliable high-bandwidth communication between all agents,
  introducing theoretical and practical obstructions to global
  consistency that make it more difficult to maintain safety-related
  invariants. Taking disaster response scenarios, particularly
  wildfire suppression, as a motivating use case, this self-contained
  memo discusses some of the distributed systems challenges involved
  in system-wide safety through a pragmatic lens. We survey topics
  ranging from consistency models and network architectures to data
  replication and data fusion, in each case focusing on the practical
  relevance of topics in the literature to the sorts of scenarios and
  challenges we expect from our use case.  }

\begin{document}
\newpage
\setcounter{tocdepth}{2}
\tableofcontents
\newpage

\section{Introduction}
\label{introduction}
Civil aviation has traditionally focused primarily on the efficient
and safe transportation of people and goods via the airspace. Despite
the inherent risks, the application of sound engineering practices and
conservative operating procedures has made flying the safest mode of
transport today. Now the desire not to compromise this safety makes it
difficult to integrate unmanned vehicles into the airspace, accomodate
emerging applications, and keep pace with unprecedented recent growth
in commercial aviation. To that end, the System Wide Safety (SWS)
project of the NASA Aeronautics' Airspace Operations and Safety
Program (AOSP) has been investigating technologies and methods by
which crewed and uncrewed aircraft may safely operate in shared
airspace.

This memo surveys topics in computing that are relevant to maintaining
system-wide safety across large, physically distributed data and
communication systems. It aims to be self-contained and accessible to
a technical audience without a deep background in distributed
systems. Our primary motivating use cases have been taken from civil
emergency response scenarios, especially wildfire suppression and
hurricane relief, primarily for three reasons. First, improved
technology for wildfire suppression, especially related to
communications and data sharing, is frequently cited as a national
priority \cite{pcast2023}.  Second, the rules for operating in the US
national airspace are typically relaxed during natural disasters and
relief efforts, so this is a suitable setting for testing new
technologies. Finally, this setting is an excellent microcosm for the
sorts of general challenges faced by other, non-emergency
applications.

If there is a central theme uniting the sections of this manuscript,
it is \emph{continuity} in the sense considered by
topology.\footnote{For a typical introductory textbook see
  \cite{mendelson2012introduction}.} The systems we consider will be
subject to harsh operating conditions that limit how well they can
perform---for example, wireless communication is typically less
reliable during inclement weather. To build a system that is
predictable (clearly a prerequisite for safety), one must ensure the
system is flexible enough to perform reasonably well under a wide
variety of adverse conditions. In other words, the behavior of a safe
system should in some sense be a \emph{continuous} function of its
inputs and environment. This sort of robust design is particularly
challenging because distributed systems designers are forced to make
delicate tradeoffs between competing objectives, most famously between
performance and consistency, the topic of Section
\ref{sec:background}.

\subsection{Summaries of the sections}
\label{summaries-of-the-sections}

Sections \ref{sec:disaster-response}--\ref{sec:desiderata} contain
background material on disaster response, distributed systems, and the
specifics of our use case. The critical takeaway of these sections is
that system-wide safety is, at least in part, a computer science
problem, indeed a software problem, and not ``just'' a matter of
engineering better hardware. Sections
\ref{sec:networking}--\ref{sec:data-fusion} survey particular topics
from the distributed systems literature, proceeding from lower-level
considerations to higher-level ones; these sections may be read
independently of each other. Below we summarize each section.

Section \ref{sec:disaster-response} starts with a pragmatic summary of
disaster response and some of the relevant computing challenges in
that setting. We aim to justify and explain the role of distributed
systems theory in system-wide safety by citing real examples
encountered in disaster response scenarios.

Section \ref{sec:background} is an introduction to distributed
systems, culminating in a illustrative result: the ``CAP'' theorem(s)
for the atomic and sequential consistency models (Theorems
\ref{thm:cap} and \ref{thm:cap-sequential}, respectively). CAP is
considered a ``negative'' result, meaning it proves something cannot
be done. The CAP theorem proves that strong consistency for a
distributed system makes systemwide network performance an upper bound
on the availability of a system to do useful work for clients, which
for our purposes is an unacceptable restriction. The practical
significance of CAP is that in emergency response environments, agents
will always act with incomplete information about the global system, a
key motivation for Section \ref{sec:continuous-consistency}.

Section \ref{sec:desiderata} refines our assumptions and identifies
desirable properties of systems for our use case. We use these points to
frame the discussion of systems and protocols in subsequent sections.

Section \ref{sec:networking} examines networking considerations. Our
vision of future emergency communication networks integrates concepts
from delay/disruption-tolerant networks (DTN) and mobile ad-hoc
networks (MANET) to provide digital communications that are robust to
a turbulent operational environment. We also examine the state of
software-defined networking (SDN). SDN is an emerging field that puts
networking protocols on the same footing as ordinary computer
programs. In theory, this should furnish computer networking with all
the benefits of modern software engineering, such as reprogrammable
hardware, rapid iteration, version control, and especially formal
verification.

Section \ref{sec:continuous-consistency} describes a hypothetical
application that might be used in a disruption-heavy network: a data
replication service built on Yu and Vahdat's theory of ``conits"
(short for ``consistency unit'') \cite{2002tact}. This framework
realizes a \emph{continuous} consistency model in the sense that, as
typically configured, it provides neither strong consistency nor
guaranteed high-availability, but rather a quantifiable and
controllable tradeoff between the two. The key idea is that many
applications can tolerate inconsistency among replicas of a data item
if an upper bound on the divergence between replicas is enforced. A
conit-based database replication framework would allow system
designers to define units of replicated data whose consistency is of
interest, enforce policies bounding inconsistency between replicas of
these items, and even dynamically tune these policies on the fly. We
believe that only a conit-based replication infrastructure can provide
the strict guarantees required for safety-related systems while also
tolerating the adverse environments and real-world limitations of the
systems we have in mind.

Section \ref{sec:data-fusion} concerns data fusion. Now and in the
future, agents in disaster scenarios will make decisions informed by
many different kinds of information. Efficient integration,
processing, filtering, and dissemination of this information will be
necessary to avoid ``swimming in sensors and drowning in data''
\cite{2010:magnuson}.  This task is especially challenging because
agents will often work with incomplete or out of date information, and
different sources of the same data may be contradictory, e.g. first
responders may receive contradictory reports about whether a structure
is occupied. One promising trend in this space, which we briefly
introduce in this section, is the development of sheaf theory as a
natural mathematical model for data fusion
\cite{2017robinsonCanonical}. Sheaf theory provides a rigorous
framework for discussing how heterogeneous sources of noisy data can
be integrated into a coherent picture, and can formally measure how
well this task has been achieved.

We conclude in Section \ref{sec:conclusion} by recapping some of the
main themes in this document and highlighting areas where design
decisions at various levels must be made to build a system that is
tuned to the exact conditions we can expect from real-world
scenarios. Such decisions might be informed by a combination of
simulation and experimentation in the field.

\section{Coordination Challenges in Disaster Response}
\label{sec:disaster-response}
This section explains aspects of disaster response, particularly
firefighting, that motivate the remainder of this document. We
describe how real-world environments give rise to foundational
challenges that must be addressed through the application of
distributed computing principles. Even using the best communications
equipment cannot avoid the fundamental computer scientific problems
raised when distributed agents must coordinate their actions over a
widespread area.

The operational environment of wildfire suppression, hurricane relief,
and other disaster settings is generally characterized by systemic
communications challenges. A 2023 report on wildland firefighting
modernization by the Presidentâ€™s Council of Advisors on Science and
Technology (PCAST) cites ``the vulnerabilities and shortfalls in
wildland firefighter communications, connectivity, and technology
interoperability'' in their first of five recommendations
\cite{pcast2023}. These shortfalls can be partly attributed to factors
that are simply inherent to disaster response: remote locations,
difficult terrain, damaged infrastructure, harsh weather, and limited
battery power, to name a few.

Agents in the field generally experience high packet loss, garbled
transmissions, and unpredictable latencies when passing information
over the communication network(s). A conservative view suggests
expecting the worst performance at the most inopportune times, because
the conditions that prompt urgent communication tend to correlate with
those that make communication difficult. It could be that the disaster
itself damages the communications infrastructure. A more basic
observation---essentially a tautology---is that a network is the most
congested, and therefore the least available, precisely when everyone
needs to use it. Both of these phenomena were famously exhibited in
the immediate aftermath of the September $11^\textrm{th}$ terrorist
attacks, when sudden user demand and severed trunk cables brought New
York public cell phone networks virtually to a halt
\cite{2011:Reardon}. Other networks along the East Coast, including
dedicated networks for first responders, experienced similar
effects. The communication failures of 9/11 eventually became the
impetus for the creation of a nationwide public safety broadband
network, FirstNet \cite{2021:firstnet, 2021:firstnet2}.

From a systems perspective, an unreliable network presents a challenge
for coordinating distributed agents. Coherent decision-making and
coordinated action require consistency, i.e.~agreement, among data
shared between agents. We shall make this somewhat vague notion more
precise in Section \ref{sec:background}, but the intuition is clear:
it is very important for everyone to agree which firetrucks should
respond to which scenes, which tasks should be prioritized, or which
radios and radio frequencies have been reserved for whom. A general
observation is that stronger standards for consistency are more
difficult to maintain than relaxed ones because they require
exchanging more information in less time, putting a heavier burden on
the network. When a communications link is slow, system components
that need to coordinate may have to pause and wait, diminishing the
efficacy of the system. When this delay is unacceptable, the most
realistic alternative is to relax the standards for coordination. In
the next section we demonstrate that such a tradeoff has implications
for system safety.

\subsection{Communication and Safety}
\label{communication-and-safety}
Operational safety requires agents to gather information about their
environment and react to it quickly and systematically. This
information is often transmitted over networks, so a poor
communications environment is a safety problem. Poor communication
forces agents to choose between delays in sending and receiving
information or acting with only limited knowledge, but note that
inaction and uninformed action are both problematic for their own
reasons. This turns out to be related to a computer science phenomenon
generally known as the safety/liveness tradeoff.

As a running example, we consider the use of firefighting airtankers,
the largest of which are the Very Large Airtankers (VLATs), defined as
those carrying more than 8,000 gallons of water or fire retardant
\cite{2019:airtankerops}. The largest VLATs can deposit more than
20,000 gallons, weighing about 170,000 pounds, in a single ``drop.''
In the United States today, a typical policy is to perform drops from
a mere 250 feet above the tree canopy \cite{2019:airtankerops}, though
the complexity of the maneuver means errant drops are sometimes
performed even lower than this, easily crushing a ground vehicle
\cite{2019:stickney}. A 2018 accident led to the death of one
firefighter and the injury of three others when an 87-foot Douglas Fir
tree was knocked down by an unexpectedly forceful drop from a Boeing
747-400 Supertanker \cite{2018:calfire}.

\begin{figure}[h]
  \label{fig:airtanker}
  \centering
  \includegraphics[scale=0.4]{images/dc10.jpg}
  \caption{A DC-10 airtanker, rated for 9,400 gallons, drops retardant above Greer, Arizona. \citationneeded.}
\end{figure}
% https://www.flickr.com/photos/apachesitgreavesnf/5837741382
% Also appears at https://www.nifc.gov/resources/aircraft/airtankers

Suppose that in the future, firefighters are equipped with GPS sensors
and digital transmitters that are integrated into an environment-wide
communication system. A reasonable policy would be to prohibit VLATs
from performing a drop if its computers do not have up-to-date
information about the location of firefighters on the
ground. Unfortunately, this information may be difficult or impossible
to share: perhaps heavy smoke, a damaged radio tower, or a tall ridge
prevents communications between the air and ground. In these
scenarios, rigid enforcement of the safety policy would prevent
airtankers from operating.

This scenario exemplifies a classic tradeoff between opposing goals:
system \emph{safety} and system \emph{availability} (or
\emph{liveness}), elaborated on in Section \ref{sec:background}. In
the distributed computing context, safety properties guarantee that a
system will not perform an action that violates a constraint. An
exemplary safety property could look like the following:
\begin{quote}
  \interlinepenalty=10000 % Punish pagebreaks inside this quote!
  $\textbf{P}_\textrm{safe}$: Ground agents are known to be at least
  100 feet outside the drop zone, and this information is current to
  within 30 seconds, or airtankers will not perform a drop.
\end{quote}
By contrast, liveness properties stipulate that the system will
certainly perform requested actions, typically within some time
bound. An exemplary liveness property might be the following:
\begin{quote}
  $\textbf{P}_\textrm{live}$: A VLAT on the ground will takeoff and
  perform a drop within 20 minutes \footnote{In an interview with
  PBS, the Chief of Flight Operations for Cal Fire cited 20
  \mbox{minutes} as the response time for aerial firefighting units
  within designated responsibility areas
  \cite{2021:aerialfirefighting}} of receiving a request from an
  incident commander.
\end{quote}

Safety and liveness are frequently dual mandates: safety, in the sense
used here, requires a system \textbf{never} to perform certain
actions, while liveness requires a system to \textbf{always} perform
certain actions. The tension between these ideals means the two often
cannot be guaranteed simultaneously. Such is the case in our example:
if firefighters are unable to broadcast their locations to the pilot,
then the pilot's actions are impeded to maintain
\(\textbf{P}_\textrm{safe}\) at the cost of
\(\textbf{P}_\textrm{live}\), allowing the fire to spread in the
meantime.\footnote{A slight linguistic idiosyncrasy exhibited here is
that liveness properties---not just ``safety'' properties---can also
be relevant to human safety. Thus, the narrow technical meaning of
safety properties for distributed systems does not capture the whole
meaning of System Wide Safety.}

Besides the safety/liveness tradeoff, the previous example exhibits
two other aspects of reasoning about distributed systems. We pause to
draw attention to them.

\paragraph{Epistemology}
Observe that the issue in the VLAT example does not simply disappear
if no ground personnel are actually within 100 feet of a drop
zone. That is, it is not simply a matter of whether a danger is
factually present. To guarantee \(\textbf{P}_\textrm{safe}\), an
airtanker's actions must be restricted when its computers do not
\emph{know} whether an action would violate
\(\textbf{P}_\textrm{safe}\)---knowledge of the fact, and not merely
the fact of it, is the crucial part. In philosophical terms, the logic
of distributed agents is inherently an \emph{epistemic} one, meaning
it must take into account not just what is true but what is known. The
need to share knowledge is what drives communication and puts a burden
on the network.

\paragraph{Discontinuity}
The properties $\mathbf{P}_\textrm{safe}$ and
$\mathbf{P}_\textrm{live}$ are inflexible, all-or-nothing
propositions. The complexity of the operational environment demands
considering more flexible kinds of properties. Suppose that agents are
known to be $500$ feet outside the drop zone, the extra margin meaning
they are well away from any danger, but the information is only
current to within 35 seconds. Clearly this is good enough information
to authorize a drop, but strictly speaking the 5-second difference is
a violation of $\mathbf{P}_\textrm{safe}$. When safety properties are
this rigid, the system's behavior becomes overly sensitive to the
particulars of the environment and therefore difficult to predict,
which is precisely the kind of \emph{discontinuity} that we aim to
prevent. Our goal is to build reliable systems that perform well in a
wide range of circumstances.

\subsection{Communication Patterns in the Field}
\label{communication-in-practice}

We now consider some of the communication patterns that occur in
wildland firefighting. The layman reader may be surprised to learn
that the state of the art is somewhat primitive, which is partly
attributable to the fact that very little permanent infrastructure
exists in this setting. This fact is also what makes wildfires an
interesting and generalizable example for other kinds of civil
disaster environments.

One trend we will draw attention to is a kind of ``geospatial locality
of reference'' principle that system designers should take into
account. By this, we mean the happy coincidence of two observations
which, if not exactly guaranteed rules, are at least approximately
true in many circumstances. The first observation is simple:
\begin{quote}
  $\textbf{O}_1$: Agents with the most urgent need to
  coordinate their actions will tend to be located closer to each
  other and require the same kinds of information.
\end{quote}
The second observation is as follows:
\begin{quote}
  $\textbf{O}_2$: Agents that are located closer together
  will tend to have more reliable communications between them than
  agents that are far apart. Conversely, information that must travel
  a long distance tends to be delayed or degrade in quality.
\end{quote}

We will refer to the concomitance of these two facts as simply the
``locality'' principle. The reason the locality principle is crucial
is that, as we see in Section \ref{sec:background}, there are major
theoretical and practical limits to how well \emph{all} agents in the
system can share \emph{all} information with each other, i.e. how well
a system can achieve global consistency. As luck would have it, in
many cases this will not be required: it will be often be enough for
\emph{some} agents to share \emph{some} information with each other, a
fact that raises opportunities to optimize scare network resources. Of
course, this does raise the question of how to decide which
information must be shared with whom, and how to use this knowledge to
best exploit the communication network. We will revisit this question,
without the pretense of answering it conclusively, throughout Sections
\ref{sec:networking}, \ref{sec:continuous-consistency} and
\ref{sec:data-fusion}. For now, we resume our examination of what
communication patterns look like today.

\paragraph{Communication on the ground}
In the field, communication between firefighters and other agents is
often facilitated by handheld (analog) land-mobile radios, which are
inherently limited in their battery life, bandwidth, effective range,
and ability to work around environmental factors like foliage and
smoke.

As an alternative to using a radio, it is common for wildland
firefighters in the field simply to shout commands and notifications
to nearby personnel. This is a clear manifestation of the locality
principle: a substantial amount of communication occurs directly
between nearby firefighters working on the same or closely related
tasks, and in some cases they are so nearby they can communicate
without any network infrastructure at all. In a future environment
where agents might be equipped with body-worn sensors and heads-up
displays (HUDs) \citationneeded, this sort of local communication
might be facilitated by simple low-power technologies such as
Bluetooth, without the need for more sophisticated (and heavy)
equipment.

Communication over a long distance requires infrastructural support,
such as the use of cell towers and repeater stations. Typically,
disaster response environments have scarce permanent infrastructure:
in a wildland fire setting, perhaps a few repeaters mounted to a
nearby watch tower. Ad-hoc infrastructure, such as a cell on wheels
(COW), can sometimes be deployed on an as-needed basis if the location
allows for it. An extremely common issue is making sure that all
equipment is properly configured, for instance that all radios are
listening on the correct frequencies, particularly when different
agencies and groups need to interoperate (another problem highlighted
during the September $11^\textrm{th}$ attacks).

\begin{figure}[t]
  \centering
  \includegraphics[scale=0.085]{images/ironside.jpg}
  \caption{The Ironside Mountain lookout and radio repeater station, destroyed in the 2021
    Monument fire, shown with protective foil on August
    $10^\textrm{th}$, 2015 during the 2015 River Complex fire. This
    particular fire burned 77,077 acres over 77 days. \citationneeded}
  \label{fig:ironside}
\end{figure}
%https://web.archive.org/web/20150923190323/http://inciweb.nwcg.gov/incident/photograph/4431/44/45122/

Use of centralized infrastructure comes with the potential for
widespread failure when the infrastructure breaks down. For example,
in California, the Ironside Mountain lookout/repeater station, seen in
Figure \ref{fig:ironside}, was destroyed during the 2021 Monument
Fire, which burned approximately 223,124 acres over 88 days
\cite{2021:monumentfire}. The Ironside Mountain station had strategic
importance, being located on a tall ridge. According to a video blog
from a volunteer firefighter involved in the incident \cite{2022:mechfire},
% See also https://web.archive.org/web/20220809061927/https://www.youtube.com/watch?v=4F2dDKMgAME
its loss prevented communication between operators on different sides
of the ridge, in networking parlance creating a \emph{partition} that
lasted until crews could ascend the ridge to deploy a temporary
station:
\begin{quote}
  ``When {[}the Ironside Mountain lookout station{]} burned down the
  radio repeater went with it. And so communications were lost across
  the fire\ldots{} one side of the fire couldn't talk to the other
  side\ldots.  So it was kind of a critical job to get that road
  cleared so that the radio crews could go back up there and set up a
  temporary radio tower.''
\end{quote}
A scenario where communication between two groups is completely
severed is exactly the sort of thing considered by the CAP theorem in
Section \ref{sec:background}.

\paragraph{Vehicles on the ground}
Large numbers of ground vehicles are involved in wildfire
suppression. A large wildfire response can involve up to 100
firetrucks distributed over a large geographical area. Bulldozers and
similar vehicles are also commonly used to control the landscape and
perimeter of the fire. An advantage of vehicles is that they can carry
heavier, which is to say better, communications equipment than a
human. For instance, a vehicle could be equipped with a satellite link
as well as a local wireless area network (WLAN) base station, serving
as a bridge between agents in the field and central coordinators
(e.g. incident commanders).

\paragraph{Communication in the air}
Wildland firefighting increasingly involves the use of helicopters and
fixed wing aircraft. Civil aviation has traditionally employed simpler
communication patterns than this use case demands. For instance,
aircraft equipped with Automatic Dependent Surveillance-Broadcast
(ADS-B) monitor their location using GPS and periodically broadcast
this information to air traffic controllers and nearby aircraft. This
sort of scheme has worked well in traditional applications, where
pilots typically only monitor the general locations of a few nearby
aircraft. The locality principle is exhibited here, too: aircraft have
the highest need to coordinate when they are physically close and
therefore in range of each other's ADS-B broadcasts.

In our setting, a large number or aircraft, easily a half dozen or
more, may need to operate in a small area, near complex terrain,
during adverse conditions, often at low altitude. In other words, the
demands are many and the margins for error are small. This sort of use
case calls for more sophisticated coordination schemes between
airborne and ground-based elements than solutions like ADS-B provide
by themselves.

\paragraph{Message relaying}
As aircraft generally have better line-of-site to ground crews than
ground crews have to each other, firefighters sometimes relay messages
to air-based units over the radio, which in turn is relayed back down
to other ground units. The locality principle comes into play for this
sort of message relaying scheme, but in the negative direction:
relaying allows knowledge to travel farther but requires more effort,
and the extended reach comes at the cost of introducing delays and
possible degradation of message quality, as in the classic game of
telephone. Hence, this mode of communication should be reserved for
more critical information.

The Communications Program of the Civil Air Patrol (a civilian
auxiliary of the U.S. Air Force) is sometimes deployed to provide
communications for firefighters on the ground using airplane-mounted
radio repeaters. Air-based repeaters are better than scheme in the
previous paragraph as they do not require a human to receive and then
manually re-transmit information. That is, this form of relaying is
\emph{transparent}. In this future, this sort of service could be
provided autonomously by base stations mounted to unmanned aerial
vehicles (UAVs), which might perform additional functions such as
tracking the fire perimeter.

In Section \ref{sec:networking} we imagine a resilient ad-hoc digital
network built from handheld and ground- and air-vehicle-mounted
devices, permanent base stations, portable temporary infrastructure,
and so on. In effect, this would be a high-tech modernization of the
sort of informal relay schemes operating today over traditional radio
channels.

%More generally, future systems should transparently facilitate
%exchanging information between agents in a decentralized fashion that
%is robust to the failure of any one component.

\subsection{Towards the Future}
\label{towards-the-future}
So far we have said a lot about the state of disaster response
today. A distributed system for this sort of challenge should be
designed for the kinds of environments and conditions expected in the
near- to medium-term future, so we briefly turn our attention to some
of our expectations for this topic.

Perhaps the most prominent expectation for future disaster response
events is a heavy reliance on \emph{data}. Besides improvements to
communications that facilitate information sharing, we expect advances
in machine intelligence to greatly influence how this data is
handled. Agents in disaster response environments will be both
producers and consumers of data, and this data will need to processed
by humans and machines in ways that agents can readily make sense of
to support their decision-making. Our background research indicated
many different kinds of data that could be valuable for
responders. Just some of the kinds of information and communication we
expect include the following:
\begin{itemize}
  \tightlist
\item
  Free-form communication, especially recorded voice messages
  broadcast to many agents at once, which may need to be processed by
  machines to extract the most pertinent information into a more
  actionable format
\item
  The exact or estimated location of victims, firefighters, vehicles,
  hazards, etc.
\item
  Medical information gathered from victims, perhaps stored in and
  collected from digital triage tags\citationneeded
\item
  Data about current and predicted fire or weather patterns
\item
  Topographic information about the terrain, highlighting for instance
  the location of rivers and roads that could form a fire control line
\item
  Planned escape routes, rendezvous points, safety zones, and landing
  zones
\item
  Availability and dispatching of assets, e.g.~ambulances, airtankers,
  or crews on standby\footnote{NOTE Need to mention Monares et al 2011}
\end{itemize}
In a perfect environment, such information would be shared with all
necessary agents in whole and instantly. In reality, agents will be
presented with information that is sometimes incomplete, out of date,
or contradictory---all problems that are further exacerbated by an
unreliable network. A competing concern is that the information
presented will be \emph{overcomplete}, filled with petty details that
distract agents from their important tasks.

In some ways, future systems for disaster response will bear
resemblence to future systems for warfighting, such as the conceptual
\emph{Internet of Battle Things} (IoBT) \cite{2016:iobt}. Chiefly,
agents ``under extreme cognitive and physical stress'' will be subject
to a highly dynamic and dangerous environment. Various kinds of
technology will assist humans by providing data to support
sensemaking, but a contraindicating concern will be flooding agents with a
``massive, complex, confusing, and potentially
deceptive\footnote{While deliberately adversarial network behavior
seems like less of a concern for disaster response agents than
warfighters, we conjecture that a similar ``fog of war'' may
lead to confusing or contradictory reports that share similarities
with intentionally deceptive behavior.} ocean of information.'' To
avoid ``swimming in sensors and drowning in data''
\cite{2010:magnuson}:
\begin{quote}
``Humans seek well-formed, reasonably-sized, essential information
  that is highly relevant to their cognitive needs, such as effective
  indications and warnings that pertain to their current situation and
  mission.'' \cite{2016:iobt}
\end{quote}
The most distinctive feature of the Internet of Battle Things, which
separates it from the everyday internet of things, is ``the
adversarial nature of the environment.'' To some extent this adversial
behavior is common also to disaster response. We previously cited a
real-world example of a critical communications station destroyed by
wildfire, perhaps not unlike an attack by enemy forces.

Lest we overstate the similarities between a battlefield environment
and civil disaster response, a distinctive feature of the latter is a
greater emphasis on the preservation of scarce network resources. More
so than a tactical military unit, a group of (say) volunteer
firefighters has to make do with off-the-shelf equipment rather than
purpose-built, best in class hardware like sophisticated satellite
links. Dedicated logistical support, and even things like allocated
radio frequencies, will likely be in shorter supply, while adverse
conditions like inclement weather will be almost guaranteed. Therefore
we expect a complex interaction between the high-level needs of
distributed applications and low-level concerns about network
resources. This is because only the applications have enough
information to determine which data is the most important and must be
shared with whom first, while only the network-level protocols have
enough information and control to make prudent use of scarce network
availability. In contravention to the common wisdom that applications
should be relatively blind to network considerations---or conversely
that network protocols ought to be transparent to applications---our
setting calls for mechanisms allowing the two layers to have some
influence over each other. This interaction between the network and
applications will be considered in more detail in Sections
\ref{sec:networking}, \ref{sec:continuous-consistency}, and
\ref{sec:data-fusion}.

To give an example, a central data fusion center may be used to detect
and alert responders to a fire that has accidentally moved beyond a
control line (known as \emph{slopover}), or it may warn firefighters
when they have strayed too far from an escape route or safety
zone. Such information would be of high importance, and it would be
worthwhile to expend network resources conveying this information to
the relevant parties. It might also be nice for firefighters to have
access to real-time information about the GPS location of every other
firefighter. However, if this strains the network, then perhaps the
exact location of teammates, but only the general location of other
crews, is called for. If the network is extremely constrained, perhaps
only information immediately relevant to preserving life should be
sent in order to ensure the network is able to deliver this
information quickly. The key point is that from where we stand, we
cannot give a blanket rule determining which information is important,
as this is partly a dynamic calculation influenced both by the
criticality of the information to the task at hand and how much unused
network capacity is available at that moment at that location.

\section{Introduction to Distributed Systems}
\label{sec:background}
In this section we distill some core aspects of distributed systems
theory, following manuscripts by Coulouris et al.
\cite{coulouris2005distributed} and Kshemkalyani and Singhal
\cite{kshemkalyani_singhal_2008}. As in Section
\ref{sec:disaster-response}, our motivating examples of distributed
systems consist of agents, nodes, and sensors in disaster response
environments where infrastructure is deployed on an ad-hoc basis or
carried by persons or vehicles in the field.

In general, a distributed system can be defined as a collection of
independent entities that cooperate to solve a problem that cannot be
individually solved \cite{kshemkalyani_singhal_2008}. Singhal and
Shivaratri \cite{10.5555/562065} offer the following definition:
\begin{quote}
  ``A collection of computers that do not share common memory or a common
  physical clock, that communicate by message passing over a communication
  network, and where each computer has its own memory and runs its own
  operating system.''
\end{quote}

The ``distributed'' aspect of the system means its components can only
communicate by passing messages over a network.\footnote{Components of
  a non-distributed system, e.g. processes on the same computer, can
  communicate by writing data to a shared memory location.} Our
foundational assumption is that messages sent over the network
experience a finite but unpredictable \emph{latency} or delay before
they are delivered. More precisely, a system is ``distributed'' when
network latencies are non-negligible compared to the timescale of
other events in the system. Per the locality principles discussed in
Section \ref{sec:disaster-response}, higher latencies are generally
expected as components of the system become more greatly separated
from each other.

This section will explain how the unpredictability of the network
essentially implies that the components of the system do not have a
shared notion of time, and ultimately no shared notion of global
state. We must either design mechanisms that attempt to recover such
features, or protocols that perform correctly without relying on
them. As any engineer would expect, the design space of distributed
systems and protocols considers many possible tradeoffs, which are
often between user-perceived notions of performance and consistency.

In our environment, we assume nodes have only intermittent access to
the network. Indeed, two or more nodes may need to coordinate without
end-to-end connectivity, i.e. without ever being on the same network
(or network partition) at the same time, necessitating the use of
delay-tolerant networking (DTN) concepts as discussed in Section
\ref{sec:networking}. Consequently, we consider latencies on an order
ranging from seconds when communicating with nearby agents, to
potentially hours or days for system-wide coordination.
Communications engineers and system designers at the hardware level
have only so much control over these latencies. Thus, we can only hope
to build systems that tolerate real conditions as optimally as
possible at the level of software.
% In a tactical environment, packet loss can be on the order of
% \%xx. \citationneeded

\subsection{Message Passing and Virtual Time}
\label{ssec:message-passing}
One can model a distributed system as a set
\(\mathcal{P} = \{P_i\}_{i\in I}\) of \emph{processes} which undergo
atomic (indivisible) state changes known as \emph{events}. We divide
events into three types: internal events, representing state changes
inside a single process, and send and receive events for for passing
messages between multiple processes. For now we treat processes and
the network as opaque blackboxes, and focus on the ramifications of
unpredictable communication delays for general coordination. The
latter condition is subsumed by saying we work in the
\emph{asychronous} network model.

Figure \ref{fig:message-latencies} shows several time diagrams for
messages $\{m_i\}_{i=1}^N$ sent between three processes $P_1$, $P_2$,
and $P_3$. The $x$-axis of the diagram depicts the flow of real
(physical) time from left to right. For each process a worldline is
shown depicting the events occurring in that process. Each message $m$
starts off as a send event $m^\textrm{send}$ indicating the moment the
message is sent over the network by its originating process; its
delivery generates a receive event $m^\textrm{recv}$. The (send,
receive) pairs are connected by an arrow and said to be corresponding
events. Note that the subscripts on the messages only serve to
disambiguate them to the reader, but they are not part of the message
and have no semantic importance.

\begin{figure}[p]
  \setlength\belowcaptionskip{5ex}
  \begin{subfigure}{1\textwidth}
    \centering
    \input{images/pgf/mpEx1.pgf}
    \caption{$P_1$ has a somewhat lower-latency connection to $P_2$ than to $P_3$}
    \label{fig:message-latencies-a}
  \end{subfigure}
  \begin{subfigure}{1\textwidth}
    \centering \input{images/pgf/mpEx2.pgf}
    \caption{$P_1$ has a much lower-latency connection to $P_2$ than to $P_3$}
    \label{fig:message-latencies-b}
  \end{subfigure}
  \caption{Message-passing time diagram examples}
  \label{fig:message-latencies}
\end{figure}

\afterpage{\clearpage}

The fact that arrows are diagonal, rather than vertical, represents
the varying delay with which messages are delivered. Because of this
delay, two messages $m$ and $m'$ may arrive in a different order than
they were sent in. In Figure \ref{fig:message-latencies-a}, $P_1$
sends messages $m_2$ and $m_4$ in that order, but $m_4$ arrives at its
recipient before $m_2$. Such a scenario might occur if $P_1$ and $P_3$
are far apart and have a high-latency communication link. In Figure
\ref{fig:message-latencies-b}, message $m_1$ is sent before any other
message, but it is also the last to be delivered. This diagram may
depict a scenario where the communication link between $P_1$ and $P_3$
has deteriorated, perhaps because they have grown farther apart or
because of environmental factors.

\subsubsection{Causality and  Sychrony}
A fundamental distributed systems challenge is to order events in a
way that individual components can calculate and agree upon. What
makes this task challenging is that the components do not share a
common time base. That is, participants in the system do not have
instantaneous access to a globally shared clock, nor are they equipped
with perfectly synchronized local clocks, so any two nodes do not
necessarily agree on physical time.

We briefly elaborate on the challenge of producing physically
sychronized clocks before discussing the consequences of this
difficulty. Physical clocks, especially consumer-grade ones, suffer
from \emph{drift}, which is to say they do not all run at the same
rate. Experienced system administrators will testify that clocks can
also be quite prone to misconfiguration, say if the date, time,
timezone, or daylight saving time policy (etc.) is set
incorrectly. Our devices may spend a long time sitting unpowered in
storage without maintaining an always-on clock. For these sorts of
reasons, we cannot assume that physical clocks in devices are very
reliable out-of-the-box.

Clock drift can be corrected for using, for instance, signals from GPS
satellites, but in disaster response environments GPS signals are not
always available. Protocols like the Network Time Protocol (NTP)
\citationneeded try to bring local clocks into synchronization with
respect to authoritative clocks, typically to within 100ms over even a
somewhat challenged internet connection, but this may not extend to
highly chaotic networking environments such as ours. For such reasons
we do not assume typical corrective measures will be able to reliably
synchronize the clocks in our system with great precision.

Fortunately, what seems most important about time for many purposes is
that \emph{the future cannot influence the past} \citationneeded, and
this is the sort of invariant we can enforce with appropriate
measures. The key relation between events we must respect is their
\emph{causal precedence}, also called Lamport's ``happens before''
relation \cite{1978:lamportclocks}. We begin with some definitions.

\begin{definition}
  For two events $e$ and $e'$ that both occur in process $P_i$, we
  write $e <_{P_i} e'$ if $e$ occurs before $e'$ in $P_i$'s
  worldline.
\end{definition}
The previous definition is ``local'' in that only relates events
within the same process. It is unambiguous because we assume events
occur in discrete, non-overlapping moments at each process. The next
definition describes an order among all (``global'') events.

\begin{definition}[Causal precedence]
  \label{def:causalprecedence}
  We define a binary relation $\to$ on the set of events as follows:
  \[e \to e' \iff
  \begin{cases}
    e <_{P_i} e' \textrm{ for some process $P_i$}
    \textbf{ or} \\
    e = m^\textrm{send} \textrm{ and } e' = m^\textrm{recv}
    \textbf{ or} \\
    \textrm{there is some } e'' \textrm{ such that } e \to e'' \textrm{ and } e'' \to e'
  \end{cases}
  \]
  If $e \to e'$, we say $e$ has \emph{causal precedence over} or
  \emph{happens before} $e'$.
\end{definition}% Should here be mentioned logical concurrency?

In words, there are three ways $e$ could have causal precedence over
$e'$.  First it may be that the two events occur on the same process
and $e$ literally happens before $e'$. It may also be that $e$ is a
message send event and $e'$ is the corresponding receive event at
another process. Finally, it could be that $e$ has precedence over
some intermediate event $e''$ with precedence over $e'$, i.e. we
consider the transitive closure of the previous two
conditions. Visually, $e \to e'$ holds whenever one can put a finger
on $e$ in the time diagram and trace a ``path of causality'' to $e'$
by moving along worldlines or following arrows.

Figure \ref{fig:causal-precedence} illustrates the causal precedence
relation corresponding to the time diagrams in Figure
\ref{fig:message-latencies}. For readability we just depict arrows
between (send, receive) pairs or adjacent events in each process,
hiding redundant transitive arrows. By comparison to Figure
\ref{fig:message-latencies}, we note that causal precedence only
describes a logical relation among events, but not their absolute time
or where they occurred.

Mathematically, causal precedence is an irreflexive partial
order. Events $e$ and $e'$ that satisfy neither $e \to e'$ nor
$e' \to e$ are said to be \emph{logically synchronous}, denoted
$e \sim e'$. The reader is warned that logical synchronicity is not a
transitive\footnote{Relations like $\sim$ that are reflexive and
  symmetric but not necessarily transitive are sometimes called
  \emph{compatibility relations}.} relation: it is possible to have
$e \sim e'$ and $e' \sim e''$ but not $e \sim e''$. For instance, in
Figure \ref{fig:message-co-a}, $m_2^{\textrm{recv}}$ is synchronous
with both $m_1^{\textrm{recv}}$ and $m_4^{\textrm{send}}$, but
$m_1^{\textrm{recv}} \to m_4^{\textrm{send}}$. In Figure
\ref{fig:message-co-b}, $m_1^\textrm{send}$ is logically synchronous
with every event except $m_1^\textrm{recv}$, but the other events are
totally ordered by causality.

Incidentally, ``causal precedence'' and ``happens before'' are
misnomers: $e \to e'$ does not mean caused $e'$ in any philosophical
sense, and conversely $e$ might occur before $e'$ in physical time
without having $e \to e'$. Intuitively, $e \to e'$ conveys merely the
possibility that information from $e$ influenced $e'$. The goal is
then to avoid systems that behave as if $e'$ takes effect before
$e$. This proscription ensures users never observe the system behaving
as if the future can influence the past.

\begin{figure}[p]
  \setlength\belowcaptionskip{5ex}
  \begin{subfigure}{1\textwidth}
    \centering
    \input{images/pgf/mpEx1CO.pgf}
    \caption{Causal precedence among the events in Figure \ref{fig:message-latencies-a}}
    \label{fig:message-co-a}
  \end{subfigure}
  \begin{subfigure}{1\textwidth}
    \centering
    \input{images/pgf/mpEx2CO.pgf}
    \caption{Causal precedence among the events in Figure \ref{fig:message-latencies-b}}
    \label{fig:message-co-b}
  \end{subfigure}
  \caption{Causal precedence relations for Figure \ref{fig:message-latencies} (transitive arrows not shown)}
  \label{fig:causal-precedence}
\end{figure}

\afterpage{\clearpage}

\subsubsection{Virtual Time}
\label{ssec:timestamps}
For all the preceding reasons, typical distributed applications
systematically track causality by employing a system of \emph{logical}
clocks. Such ``clocks'' measure the logical flow of time by storing
non-negative integers that are advanced according to certain
rules. Three major variants are common: scalar, vector, and matrix
clocks. These fall on a kind of spectrum, in that scalar clocks are
simple but provide the most coarse-grained information, while vector
and matrix clocks provide increasingly more information but impose
greater overheads.

For each event $e$, let $C(e)$ denote the timestamp attached to that
event. The fundamental property we want to satisfy is that if $e$
causally precedes $e'$, it should receive a lesser timestamp. This is
called the clock condition.
\begin{definition}
  A system of timestamps satisfies the \emph{clock consistency
  condition} if the following monotonicity property holds:
  \[ \textrm{For all events $e$ and $e'$, } e \to e' \implies C(e) < C(e') \label{eq:mp}\tag{CC} \]
\end{definition}
%In Example \ref{exmpl:ambulancedispatch}, we had two send events
%$m_1^\textrm{send} \to m_2^\textrm{send}$, so the clock consistency
%condition would require that
%\[C(m_1^\textrm{send}) < C(m_2^\textrm{send}).\]

Importantly, the clock consistency condition does \emph{not} imply
that we can decide if events are causally related by comparing
timestamps. Instead, their utility can be seen by replacing it
with the following logically equivalent condition.
\[ \textrm{For all events $e$ and $e'$, }C(e) \leq C(e') \implies e'
  \not\to e \label{eq:mp-conv} \] That is, we can be sure that some
ordering $e_1, e_2, e_3\ldots$ does \emph{not} violate the causal
precedence relation provided we ensure $C(e_{i}) \leq C(e_{i+1})$ for $i = 1, 2, 3\ldots$

For many applications, one does want to unambiguously determine
whether two events are causally related by comparing timestamps. That
is, one would like to have the following stronger condition.
\begin{definition}
  A system of timestamps satisfies the \emph{strong} clock consistency
  condition if the following property holds.
  \[ \textrm{For all events $e$ and $e'$, } e \to e' \iff C(e) < C(e') \label{eq:sc}\tag{SC} \]
  Note that $\iff$ is notation for ``if and only if,'' i.e. logical equivalence.
\end{definition}
\ref{eq:sc} strengthens \ref{eq:mp} by stipulating that we can always
compare two timestamps to infer whether the events are causally
related. A system of scalar clocks, defined below, can enforce the
weaker condition but not the strong one.

\subsubsection{Scalar clocks}
\begin{figure}
  \setlength\belowcaptionskip{5ex}

  \begin{subfigure}{1\textwidth}
    \centering \input{images/pgf/mpEx1Sc.pgf}
    \caption{Figure \ref{fig:message-latencies-a} redepicted with scalar clocks}
    \label{fig:message-latencies-scalar-a}
  \end{subfigure}

  \begin{subfigure}{1\textwidth}
    \centering \input{images/pgf/mpEx2Sc.pgf}
    \caption{Figure \ref{fig:message-latencies-b} redepicted with scalar clocks}
    \label{fig:message-latencies-scalar-b}
  \end{subfigure}

  \caption{Scalar clock examples}
  \label{fig:message-latencies-scalar}
\end{figure}

Scalar clocks, introduced by Lamport \cite{1978:lamportclocks},
require each process $P_i$ to maintain a single scalar value $C_i$, a
non-negative integer initialized at $0$. There are two update rules:
\begin{enumerate}
\item[\textbf{R1}] Before a message is sent, $C_i$ is updated according to the rule
  \[C_i := C_i + 1.\] This new value is the timestamp attached to the
  message send event. The value is sent (``piggybacked'') alongside
  the message as metadata.
\item[\textbf{R2}] When $P_i$ receives a message timestamped with value $C$, it
  updates $C_i$ according to the rule
  \[C_i := \max(C, C_i) + 1.\]
  The updated value is the timestamp associated with the receive
  event.
\end{enumerate}
Figure \ref{fig:message-latencies-scalar} depicts the same events in
Figure \ref{fig:message-latencies} alongside the scalar timestamp (in
parentheses) that would be assigned to each event. For send events,
the piggybacked timestamp is shown as a label attached to the arrow.

It is clear that scalar clocks satisfy the clock condition
\eqref{eq:mp}, which is observed by tracing the path of causality
between related events and seeing that the clock is incremented at
each step.
\begin{lemma}
  Scalar clocks satisfy $e \to e' \implies C(e) < C(e')$.
\end{lemma}

Scalar clocks do not satisfy the strong condition (\ref{eq:sc})
because knowing $e$ has a lesser timestamp than $e'$ leaves open the
question of whether $e$ causally preceded $e'$. For example, in Figure
\ref{fig:message-latencies-scalar-b}, $m_1^\textrm{send}$ has a
globally minimal timestamp value of $1$. However, we see clearly in
the Figure that it does not causally precede any event except
$m_1^\textrm{recv}$.

\subsubsection{Vector clocks}
\newcommand{\vt}{\textrm{vt}}

The strong clock condition \eqref{eq:sc} cannot hold either using
scalar clocks or even synchronized physical clocks. This is because
they assign timestamps whose values form a total order, meaning any
non-equal timestamps $C_1, C_2$ have to satisfy either $C_1 < C_2$ or
$C_2 < C_1$. However, logically synchronous events $e \sim e'$ do not
satisfy either $e \to e'$ or $e' \to e$. Thus, the strong consistency
requirement would require us to set $C(e) = C(e')$ for all logically
synchronous events. This is impossible, as we already saw that logical
concurrency is not transitive. For example, recall in Figure
\ref{fig:message-co-b} that $m_1^{\textrm{send}}$ is logically
synchronous with every event except $m_1^\textrm{recv}$, so they would
all have to receive the same timestamp as $m_1^{\textrm{send}}$, which
violates the fact that they are not logically synchronous with each
other.

\begin{figure}
  \setlength\belowcaptionskip{5ex}

  \begin{subfigure}{1\textwidth}
    \centering
    \input{images/pgf/mpEx1Vec.pgf}
    \caption{Figure \ref{fig:message-latencies-a} redepicted with vector clocks}
    \label{fig:message-latencies-vector-a}
  \end{subfigure}

  \vspace{4ex}

  \begin{subfigure}{1\textwidth}
    \centering \input{images/pgf/mpEx2Vec.pgf}
    \caption{Figure \ref{fig:message-latencies-b} redepicted with vector clocks}
    \label{fig:message-latencies-vector-b}
  \end{subfigure}

  \caption{Vector clock examples}
  \label{fig:message-latencies-vector}
\end{figure}
\afterpage{\clearpage}

The way out of this impasse is to assign timestamps from a partial
order, meaning two non-equal timestamps may not satisfy $C_1 < C_2$ or
$C_1 > C_2$. Vector clocks achieve this by storing one scalar value
for each process in the system. For a set of $N$ processes, $P_i$
maintains a vector $\vt_i[1 \ldots N]$ of non-negative integers, with
all values initialized to $\vt_i[x] = 0$. As with scalar clocks there
are two update rules:
\begin{enumerate}
\item[\textbf{R1}] Before a new message is sent, $\vt_i$ is updated according to the rule
  \[\vt_i[i] := \vt_i[i] + 1.\]
  The entire updated vector $\vt_i$ is piggybacked as part of the
  message's metadata so the receiver can use it.
\item[\textbf{R2}] When a message is received with a piggybacked timestamp $\vt$,
  $\vt_i$ is updated according to
  \[\vt_i[x] := \max(\vt[x], \vt_i[x]) \quad \textrm{for all $x = 1\ldots N$}.\]
  After this, $P_i$ advances its own local time according to the rule
  \[ \vt_i[i] := \vt_i[i] + 1.\]
  This new vector is the timestamp attached to the receive event.
\end{enumerate}

These rules are perhaps more intuitively explained by demonstration.
Figure \ref{fig:message-latencies-vector} redepicts the diagrams in
Figure \ref{fig:message-latencies} showing the vector timestamp that
would be assigned to each event. The $i^\textrm{th}$ component of
$P_i$'s vector clock, or $\vt_i[i]$, is called the \emph{local time}
for $P_i$. For all other $j \neq i$, the $j^\textrm{th}$ component
$\vt_i[j]$ of $P_i$'s clock represents $P_i$'s \emph{estimate} of
$P_j$'s local time. This estimate is always a lower bound, since
$P_j$'s local time may have advanced in the meantime, but we never
advance our estimate ahead of $P_j$'s actual local time. By examing
timestamps sent to it by other processes, $P_i$ can learn that $P_j$'s
local clock has advanced, even without directly communicating with
$P_j$.

% A slight difference between
%the local time and a scalar clock is that after receiving a message,
%$P_i$'s local time is not necessarily greater than the sender's local
%time that was piggybacked with the message. (What matters is that
%$P_i$'s overall vector time is greater.)

Vector timestamps are compared component-wise. This is of course a
partial order, as one vector may be greater than another in some
components and less in others.
\begin{definition}[Vector comparison]
  Let $v, w$ be two vector clocks. We define the following relations:
  \begin{align*}
             v = w &\iff \forall i, v[i] = w[i] \\
  v \preccurlyeq w &\iff \forall i, v[i] \leq w[i] \\
         v \prec w &\iff v \preccurlyeq w \textrm{ and } \exists i, v[i] < w[i] \\
            v || w &\iff \textrm{ neither } v \preccurlyeq w \textrm{ nor } w \preccurlyeq v
  \end{align*}
  That is, $v \prec w$ if all of $w$'s components are at least as
  great as $v$'s, and at least one of its components is strictly
  greater. When two non-equal vector timestamps are compared, and
  neither is greater than the other, we write $v || w$ and say the
  events are \emph{logically concurrent}.
\end{definition}

The reason to consider vector clocks over scalar clocks is precisely
that they satisfy \ref{eq:sc}.
\begin{lemma}
  Vector clocks satisfy the strong clock consistency condition. That
  is, where $C(e)$ is the vector timestamp of an event, then
  \[ e \to e' \iff C(e) \prec C(e'). \]
  From this it follows that for non-equal events $e$ and $e'$ we have
  \[ e \sim e' \iff C(e) || C(e'). \]
\end{lemma}

For reasons of space we omit a proof of the preceding lemma.  The
direction $e \to e' \implies C(e) \prec C(e')$ is
straightforward. More tedious is the right-to-left direction
$C(e) \prec C(e') \implies e \to e'$, but the key idea is that if
$C(e) \prec C(e')$, then it is possible to follow a backwards chain of
send and receive events connecting $e$ to $e'$, hence $e \to e'$.

% \begin{proof}
%   The direction $e \to e' \implies C(e) \prec C(e')$ is
%   straightforward. Slightly more tedious is the right-to-left
%   direction $C(e) \prec C(e') \implies e \to e'$.

%   Assume two events satisfy $C(e) \prec C(e')$. If the events occur on
%   the same process $P_i$, then $C(e')$ must have a strictly greater
%   $i^\textrm{th}$ component, i.e. $C(e)[i] \prec C(e')[i]$, in which
%   case $e <_{P_i} e'$ and hence $e \to e'$.

%   The last case to consider is that $C(e) \prec C(e')$ with $e$ on
%   process $P_i$ and $e'$ on process $P_j$ with $j \neq i$. If
%   $e \to e'$ then we are done, so suppose $e \not \to e'$.
%   Without loss of generality, let $e'$ be the first event (in physical
%   time) satisfying both
%   \[ C(e) \prec C(e') \textrm{ and } e \not \to e'.\] We will derive a
%   contradiction. If $e$ is a message receive event, let $x$ be the
%   corresponding send event. $x$ cannot satisfy


%   . Let $e_p$ be the event immediately
%   preceding $e'$ on process $j$. By assumption,
%   $C(e) \not \prec C(e_p)$, as otherwise our we contradict our
%   assumption that $e_p$ is the first event satisfying both
%   $C(e) \prec C(e_p)$ and $e \not \to e_p$. However, $C(e_p)$ and
%   $C(e')$ can only differ in their $j^\textrm{th}$ component, as

%   .  If $e'$ is a message \emph{send} event, then


%   We can prove this
%   direction by its contrapositive:
%   \[ e \not\to e' \implies C(e) \not\prec C(e'). \]

%   Assume $e \not\to e'$ where event $e'$ occurs on process $P_i$.  If
%   $e'$ also occurs on process $i$, then because events on $P_i$ are
%   totally ordered with respect to $P_i$'s local time, it must be that
%   $e' <_{P_i} e$, i.e. $e'$ happened first. In this case,
%   $C(e')[i] < C(e)[i]$, hence $C(e) \not \prec C(e')$.

%   Suppose $e'$ occurs on $P_j$ where $j \neq i$. Without loss of
%   generality, consider the \emph{first} event $e'$ (in physical time)
%   with the property $e \not\to e'$. If $e'$ is a message send event,

%   . So let $e'$ occur on some other process.
%   \textbf{TODO}
% \end{proof}

\subsubsection{Matrix clocks}
If a vector clock stores an estimate of every other processes' local
time, a matrix clock stores an estimate of every other processes'
vector clock. We give the details below. Figure
\ref{fig:message-latencies-matrix} shows our running examples with
matrix clocks.

\begin{enumerate}
\item[\textbf{R1}] Before a new message is sent, $\vt_i[i]$ is updated according to the rule
  \[\vt_i[i,i] := \vt_i[i,i] + 1.\]
  The entire updated matrix $\vt_i$ is piggybacked with the message.
\item[\textbf{R2}] When a message is received with a piggybacked timestamp $\vt$,
  $\vt_j$ is updated according to
  \[\vt_j[x] := \max(\vt_j[x], \vt[x]) \quad \textrm{for all $x = 1\ldots N$}.\]
  After this, $P_i$ advances its own local time according to the rule
  \[ \vt_i[i] := \vt_i[i] + 1.\]
  This new vector is the timestamp attached to the receive event.
\end{enumerate}

\begin{figure}[p]
  \begin{subfigure}{1\textwidth}
    \centering
    \input{images/pgf/mpEx1Mat.pgf}
    \caption{Figure \ref{fig:message-latencies-a} redepicted with matrix clocks}
    \label{fig:message-latencies-matrix-a}
  \end{subfigure}
  \begin{subfigure}{1\textwidth}
    \input{images/pgf/mpEx2Mat.pgf}
    \label{fig:message-latencies-matrix-b}
  \end{subfigure}
  \caption{Figure \ref{fig:message-latencies-b} redepicted with matrix clocks}
  \label{fig:message-latencies-matrix}
\end{figure}

\afterpage{\clearpage}

but we shall see a clear example in Section
\ref{sec:continuous-consistency} of why tracking this extra
information can prove useful.

Matrix clocks subsume vector clocks, so they can be used whenever the
strong consistency condition is needed. The justification for matrix
clocks is somewhat subtle.

In Section \ref{sec:background}, we explained that distributed systems
fundamentally involve epistemic logic, i.e. reasoning about not just
what is true but what is known. Other nodes store
knowledge. Frequently the nodes want to collaborate to ensure every
other process is made aware. Since a vector clock represents the state
of knowledge, by keeping a lower bound estimate of all other vector
clocks, we have a lower bound estimate of the knowledge that is
already known to others. In Section
\ref{sec:continuous-consistency} we will see an example where this
sort of lower bound estimate of other process's clocks is important
for distributed database replication.

\subsection{Message Ordering and Group Communication}

In this section we summarize some different assumptions we could
impose about how messages are ordered. Then we discuss allowing
messages to be sent to multiple receipients at once, such as in a
group chat application.

\subsubsection{Message Ordering}
We have seen that the network latencies tend to cause messages to be
delayed and arrive in a different order than they were sent in.

\paragraph{FIFO ordering}
A very modest requirement is to impose the \emph{first-in, first-out}
(FIFO) condition, which stipulates that on any logical communication
link between two nodes in the system, messages arrive in the order
they were sent.

\begin{definition}[FIFO condition]
  A FIFO execution is one that satisfies the following condition. Let
  $P_i$ and $P_j$ be any two processes and $m_1$ and $m_2$ be two
  messages sent from $P_i$ to $P_j$. Then
  $m_1^{\textrm{send}} m_2^{\textrm{send}} \implies
  m_1^{\textrm{recv}} m_2^{\textrm{recv}}$.
\end{definition}

The FIFO assumption is less strict than it may look. Crucially, the
requirement only applies to messages that share a common sender and a
common destination. No guarantee is made for any other messages. FIFO
order is faily easy to implement and often built into the transport
layer of a network protocol stack. For instance, the internet protocol
(IPv4) by itself does not provide FIFO semantics, but the TCP protocol
does. Enforcing FIFO on a logical communication link between $P$ and
$P'$ can be as simple as requiring both sides to mark their messages
with consecutive increasing numbers (e.g. $1,2,3\ldots$): If message
$1$ arrives and then message $3$ arrives, then the middleware can
infer that message $2$ is lagging behind, so it can deliver $1$ to the
higher-level application but will not deliver $3$ until $2$ arrives.

\paragraph{Causal ordering}
Because FIFO does not apply to messages sent between three or more
nodes, distributed applications can experience anomolies in message
order. One possibility is for messages to violate the \emph{causally
  ordered delivery} assumption. ``Causality'' in this context refers
to the potential for information from one event to influence
another. A key problem for distributed systems is that network
latencies mean events with a causal relation may appear to happen in
an illogical order to users, so tracking causality a key issue for
system designers. We start by defining an order between events on the
same process.

Now message $m_1$ is timestamped and sent by $P_1$, while $m_2$ is
timestamped and sent by $P_2$. A hallmark of distributed systems is we
cannot assume processes have instantaneous access to a common physical
clock, which means the inequality cannot be guaranteed unless $P_1$
and $P_2$ happen to have two different clocks that are
synchronized. Whether we can assume clocks are synchronized depends on
the exact environment and the level of synchronicity required, but
generally sychronization is either unavailable or requires additional
mechanisms to prevent clocks from falling out of sync. For some
coarse-grained purposes, possibly including the database replication
framework in Section \ref{sec:continuous-consistency}, it may be
enough for devices to maintain clocks that are only somewhat precisely
synchronized. For other purposes, such as enforcing a total order on a
sequence of broadcasts among large distributed groups, sufficiently
precise synchronization of clocks untenable.


\begin{definition}(Causally ordered delivery)
  \label{def:causalorder}
  A network satisfies the causally ordered (CO) delivery model if
  messages are received in an order consistent with the causality
  between send events. That is, if the following property holds:
  \begin{quote}
  For all messages $m$ and $n$ with the same destination, if
  $m^\textrm{send} \to n^\textrm{send}$, then $m^\textrm{recv} \to
  n^\textrm{recv}$.
  \end{quote}
  In mathematical terms, for each destination $P_\mathrm{dest}$, when
  we consider the set of all messages sent from anywhere to
  $P_\mathrm{dest}$, the function mapping send events to corresponding
  receive events must be monotonic with respect to causal
  precedence.
\end{definition}

\subsubsection{Group Communication}
The importance of message ordering is particularly acute

\paragraph{Causal order broadcast}
For motivational purposes we consider a simple example. We slightly
generalize the notion of a message and allow messages with multiple
receipients (which could be realized by sending independent messages
treated as one unit for present purposes). Our example is a group
messaging application used by three firefighters, shown in Figure
\ref{fig:message-latencies-c}, though in reality the processes may be
machines sending messages at orders of magnitude faster than a human
conversation. To post a message, a process sends it to the other two
members of the group. For reasons explained above, the two recipients
generally do not receive the message at the same time.

\begin{figure}
  \centering \input{images/pgf/mpEx3.pgf}
  \caption{Broadcasting example satisfying FIFO but not CO}
  \label{fig:message-latencies-c}
\end{figure}

\begin{example}
  \label{exmpl:ambulancedispatch}
  In Figure \ref{fig:message-latencies-c}, we imagine the following
  conversation has taken place:
  \begin{itemize}
\item [$P_1$]: ``I need an ambulance at location A.''
\item [$P_2$]: ``Understood, the last ambulance has been dispatched.''
\end{itemize}
However, $P_3$ witnesses $P_2$'s answer before seeing $P_1$'s
question. This results in $P_3$ hearing a rather different conversation snippet:
\begin{itemize}
\item [$P_2$]: ``Understood, the last ambulance has been dispatched.''
\item [$P_1$]: ``I need an ambulance at location A.''
\end{itemize}
\end{example}
From $P_3$'s perspective, it seems that $P_1$ is requesting resources
that are not available. The apparent conflict can lead to requests
going unanswered, or possibly handled twice, leading to a chaotic
situation and misallocation of resources. If these broadcasts
represented machine-to-machine traffic between instances of a
distributed application, it is likely the application would misbehave
if not explicitly designed for this situation.

FIFO is vaccuously satisfied in Figure \ref{fig:message-latencies-c}
because all four arrows have a distinct (sender, receiver)
pair. However, the diagram violates Definition \ref{def:causalorder}
because the send event of $P_1$'s question has causal precedence over
$P_2$'s answer, while in $P_3$'s worldline, the receive event of the
answer has causal precedence over the receipt of question.

\begin{figure}
  \begin{subfigure}{1\textwidth}
    \centering \input{images/pgf/mpEx3Sc.pgf}
    \caption{Scalar clocks for \ref{fig:message-latencies-c}}
    \label{fig:message-latencies-scalar-c}
  \end{subfigure}

  \begin{subfigure}{1\textwidth}
    \centering \input{images/pgf/mpEx3Vec.pgf}
    \label{fig:message-latencies-vector-c}
  \end{subfigure}

  \begin{subfigure}{1\textwidth}
    \input{images/pgf/mpEx3Mat.pgf}
    \label{fig:message-latencies-matrix-c}
  \end{subfigure}

  \caption{Broadcasts with logical clocks}
  \label{fig:broadcast-latencies}
\end{figure}

Returning to the previous example, Figure \ref{fig:message-latencies-c} and
\ref{fig:message-latencies-scalar-c}, because $m_1^\textrm{send}$
causally precedes $m_2^\textrm{send}$, we find
\[C(m_1^\textrm{send}) = 1 < 5 = C(m_2^\textrm{send}).\] Using this
fact $P_3$ can learn (by examining piggybacked timestamps) that
$P_2$'s response could not causally precede $P_1$'s question,
partially resolving the ambiguity posed earlier.

However, the scalar clock regimen does not given $P_3$ enough
information to conclusively affirm that the question causally precedes
the answer: the opposite is ruled out, but it is possible the events
are causally independent.

\paragraph{Total order broadcast}
%Scalar clocks achieve the clock consistency condition without
%synchronized physical clocks. They are often used when an application
%needs to establish a shared global ordering among all events in the
%system, such as in state machine replication (SMR) protocols. Because
%timestamps are not globally unique, they do not carry quite enough
%information for recipients to compute a shared total order by
%themselves. A simple way to obtain a total order is to use an
%arbitrary tie-breaking mechanism for deciding how to order two
%(necessarily causally unrelated) events that happen to share a common
%scalar clock timestamp, typically by comparing the (necessarily
%distinct) identifiers of the processes where they took place. Indeed,
%this application was the motivating example when scalar clocks were
%introduced by Lamport.

%It is certainly possible to implement broadcast-ordering protocols
%that present the abstraction of multiple parties who receive group
%messages in the same order at all locations, but it should be
%unsurprising that such protocols require mechanisms for tracking the
%causal relation between events.


\newpage
\subsection{Shared Memory}
Framing a distributed application directly in terms of message passing
can be challenging. The low-level details required to implement the
correct message passing behavior distract from the high-level
application design, making it desirable to separate these concerns. In
this section we summarize a more abstract paradigm called the
\emph{distributed shared memory} (DSM) framework. In this setting,
programmers think in terms of reading and writing values to memory
locations rather than transmitting messages over the network. The key
feature is that all instances of the application act as if they are
reading and writing from the same pool of memory locations, when they
are actually running on distributed computers that do not share
memory.

To implement the DSM model, a ``middleware'' memory management layer
is employed to handle read and write requests issued by the
application. Under the hood, the middleware coordinates with other
nodes by message passing in a way that facilitates the abstraction of
a common virtual memory space. The programmer is not directly
concerned with how this is accomplished. They are, however, required
to understand what guarantees are and are not provided by the memory
management layer, in other words what kinds of \emph{semantics} the
virtual memory implements. Systems generally differ in this respect
because they make different tradeoffs between safety guarantees and
performance.

\begin{figure}
    \centering
    \input{images/pgf/smEx1NoEdges.pgf}
    \caption{Time diagram for memory operations}
    \label{fig:smEx1}
\end{figure}

\subsubsection{Shared memory time diagrams}
\newcommand{\Op}{\mathrm{Op}}

Figure \ref{fig:smEx1} depicts a time diagram for the shared memory
abstraction, not unlike the diagrams in Figure
\ref{fig:message-latencies} for message passing. Two kinds of
operations are shown:
\begin{description}
\item[Reads] A \emph{read} request $R(x)$ indicates reading the value
  in memory at location $x$, which returns some value $v$. When it is
  important to indicate the return value, we notate read requests as
  $R(x, v)$.
\item[Writes] A \emph{write} operation $W(x, v)$ indicates writing
  value $v$ to memory location $x$. Other common notations include
  $x := v$ or $x \leftarrow v$.
\end{description}
We write $\Op$ to indicate an arbitrary (read or write)
operation.

Each operation consists of a horizontal span along the worldline of a
process representing the duration of the operation. Each operation
begins at a time $\Op.s$ ($s$ for ``start'') called its
\emph{invocation event}, beginning when the programmer invokes the
memory manager. The operation ends at some time $\Op.t$ ($t$ for
``terminate'') called its \emph{response event}, when a response is
given to the caller (either an acknowledgement for a write operation,
or a value for a read operation). The entire collection of invocation
and response events, as depicted in a time diagram, is called a
\emph{history}. If $H$ is a history, we write $H|_P$ for the set of
operations that happen on process $P$, called the \emph{local history}
of $P$.

Transparently and in the background, the memory management layer
coordinates with other processes over the network. While executing a
read request $R(x)$ it may, for example, send messages to another
server to lookup the current value of memory location $x$. Such
behind-the-scenes coordination is hidden from the programmer and not
reflected in the time diagrams.

\subsubsection{Semantics and consistency}

For a sequential application, i.e. one running on a single computer,
it is clear how read and write requests should be interpreted. A read
request $R(x)$ should return the most recent value $v$ that was
written to $x$ by a write $W(x, v)$ (or resort to some default
behavior if no such write exists, but we will not consider such
examples). This interpretation is unambiguous because we assume that
operations running on a single process do not overlap in time, so it
always makes sense to ask which of two events happened before the
other.
\begin{figure}
  \input{images/pgf/smEx0.pgf}
  \caption{Memory operations on a single process}
  \label{fig:smEx0}
\end{figure}

\begin{example}
  \label{exmpl:updatesoneprocess}
  Consider Figure \ref{fig:smEx0}, depicting just a single process. Since there is
  no ambiguity in the order of events, it is clear that this process
  should execute in the following order (read requests have been underlined with their results shown):
  \[ W(x, 0) \to W(y, 5) \to \underline{R(x, 0)} \to W(x, 3) \to \underline{R(y, 5)} \to \underline{R(x, 3)}. \]
\end{example}

In the distributed context, the metaphorical wrench in the works is
that operations by different processes can execute concurrently, so
there is no obvious total order that can be used to compare
events. Without such a comparison, the notion of ``most recent''
operation is ambiguous, which makes it difficult to say exactly how
the memory requests should even be interpreted.

\begin{figure}
  \input{images/pgf/smEx2.pgf}
  \caption{Concurrent updates by two processes}
  \label{fig:smEx2}
\end{figure}

\begin{example}
  \label{exmpl:concurrentupdates}
  Consider Figure \ref{fig:smEx2}. Two writes are depicted that overlap
  in physical time, making it unclear whether $W(x,3)$ or $W(x,5)$
  should be considered as happening first.
\end{example}

In example \ref{exmpl:concurrentupdates}, it seems likely that the
programmer should expect each $R(x)$ operation to return either 3 or
5, but definitely not, say, 37. After some thought, especially about
how to implement the memory manager, a few other questions seem less
obvious.
\begin{itemize}
\item Must the read requests on $P_1$ and $P_2$ return the same value?
\item May the second read at $P_2$ return a different value
  than the preceding one?
\item Is there a world in which an $R(x)$ could return 4?
\end{itemize}

It is possible to consider different ways of answering these
questions. The role of a \emph{memory model} is to specify exactly
which possibilities are allowed and which are prohibited, which comes
down to constraining which kinds of values can be returned by read
operations under different conditions. An application designed
assuming one memory model may misbehave if executed in an environment
that actually implements a different one. On the other hand, memory
models that provide greater guarantees to the programmer also impose
greater constraints on the memory manager and, ultimately, the
network. Thus, the selection of a memory model requires balancing the
needs and expectations of the application against the practical
constraints of the environment.

\subsubsection{Concurrency and External order}
\label{sssec:externalorder}
As before, it is tempting to imagine sidestepping these issues by
assigning a physical timestamp to each operation as to establish a
total global order among events. However, we have seen that physical
clocks are usually not so precisely sychronized as to allow comparing
physical timestamps from different nodes.

One fundamental relation in the DSM setting is \emph{external
order}. Intuitively, it is the partial order that orders
non-overlapping events by their physical times, but does not assign an
ordering to events whose executions overlap in physical time.

\begin{definition}[External order]
  Let $H$ be a history. An operation $\Op^1$ \emph{externally
    precedes} operation $\Op^2$ if $\Op^1.t < \Op^2.s$. (Here, we are
  comparing events in terms of real, physical time.) That is, one
  operation externally precedes another if its response occurs before
  the invocation of the other. This induces an irreflexive partial
  order on $H$ called external order.
\end{definition}

Because each process handles operations one-at-a-time, a local history
$H|_P$ is totally ordered by external order. However, operations by
different process may overlap.

\begin{definition}[Physical concurrency]
  Consider two operations $\Op^1$ and $\Op^2$. If neither externally
  precedes the other, in another words if there is some moment in time
  during which both operations are executing, the operations are said
  to be \emph{physically concurrent}, written $\Op^1 || \Op^2$.
\end{definition}

Physical concurrency is a reflexive and symmetric relation, but
typically not a transitive one. Such structures are generally known as
compatibility relations, the intuition being that if \(A\) and \(B\) are
both ``compatible'' with \(C\), it need not be the case that \(A\) and
\(B\) are compatible with each other. The reader should convince
themselves that if $\Op^1 || \Op^2$ and $\Op^2 || \Op^3$, it need not
be the case that $\Op^1 || \Op^3$.

For performance reasons, memory models commonly allow distributed
operations to have semantics that do not strictly conform to external
order. For instance, below we show how sequential consistency
(Definition \ref{def:sequentiallyconsistent}), one of two ``strong''
models, allows for this possibility. Yet even this model imposes too
many constraints to be workable for networking environments as
unpredictable as ours.

\begin{figure}[h]
    \centering
    \input{images/pgf/smEx1DAG.pgf}
    \caption{External order relation among operations in Figure \ref{fig:smEx1}}
    \label{fig:smEx1DAG}
  \end{figure}

\begin{example}
  Figure \ref{fig:smEx1DAG} shows a time diagram involving three
  processes depicts the external order relation among the operations
  shown in Figure \ref{fig:smEx1} in the form of a directed acyclic
  graph (DAG).
\end{example}

\begin{definition}[Sequential history]
  Let $H$ be a history of invocation/response events. A
  \emph{sequential history} of $H$ is any choice of total order among
  the events in $H$---that is, some rearrangement of the operations as
  to ensure none of them overlap in time.
\end{definition}

The first two memory models in the next section require, among other
things, that any history $H$ must be consistent with some sequential
rearrangement of $H$.

\subsection{Memory Consistency Models}

\subsubsection{Linearizability}
\label{sssec:linearizability}

\emph{Linearizability}, the strongest consistency model, can be
concisely defined as providing the appearance that ``each operation
applied by concurrent processes takes effect instantaneously at some
point between its invocation and response.''
\cite{10.1145/78969.78972} The same condition is known variously as
atomic consistency, strict consistency, and sometimes external
consistency. In the context of database transactions (which come with
other database-specific guarantees, particularly serializability), the
analogous condition is called strict serializability.

A linearizable history is defined by three features.
\begin{definition}[Linearizable history]
  \label{def:linearizable}
  A \emph{linearizable history} is one satisfying the following three rules.
\begin{enumerate}
  \tightlist
\item[R1] All processes act like they all see some common sequential
  history of operations.
\item[R2] Responses are semantically correct, meaning a read request
  \(R(x, a)\) returns the value of the most recent write request
  \(W(x, a)\) to \(x\).
\item[R3] The sequential history is consistent with external
  order. That is, if the response of $\Op$ precedes the invocation of
  $\Op'$, then $\Op$ precedes $\Op'$ in the sequential history.
\end{enumerate}
\end{definition}

We return to Example \ref{exmpl:concurrentupdates}. Rule 1 of
Definition \ref{def:linearizable} requires $P_1$ and $P_2$ to act like
they both witness the same total order of operations, which in
particular assigns a total order on the three write operations
$W(x,4), W(x,3),$ and $W(x,5)$. By Rule 3 this order must agree with
external order, meaning $W(x,4)$ comes first, followed by $W(x,3)$ and
$W(x,5)$ in either order (due to their physical concurrency), followed
by the three read operations. By Rule 2, the three read operations
must return the value of $x$ assigned by whichever of $W(x,3)$ or
$W(x,5)$ happens last. Altogether, linearizability leaves only two
possibilities: all three read operations return $3$, or all three
return $5$. These are known as the possible \emph{linearizations} of
the history shown in Figure \ref{fig:smEx1}.

A more visually intuitive way of approaching linearizability is by
defining it in terms of \emph{linearization points.}

\begin{definition}
  A \emph{linearization point} $t \in \mathbb{R} \in [\Op.s, \Op.t]$
  for an operation $\Op$ is a time between the event's invocation and
  response, at which time the operation appears to take effect in
  whole and instantaneously.
\end{definition}

\begin{example}
  \label{exmpl:linearizations}
  The two possible linearizations of Figure \ref{fig:smEx1} are shown
  in Figure \ref{fig:smEx3}. Possible choices for linearization points
  are shown in yellow. (Note that only the relative order of
  linearization points is important.)
\end{example}

Example \ref{exmpl:linearizations} demonstrates that linearizability
leaves some room for non-determinism in the execution of distributed
applications. In the example, we see that the reads in Figure
\ref{fig:smEx1} may return 3 or 5, though they must all return the
same value.
\begin{figure}[p]
  \setlength\belowcaptionskip{5ex}

  \begin{subfigure}{1\textwidth}
    \centering
    \input{images/pgf/smEx3L1.pgf}
    \caption{One of two possible linearizations}
    \label{fig:smEx1L1}
  \end{subfigure}

  \begin{subfigure}{1\textwidth}
    \input{images/pgf/smEx3L2.pgf}
    \caption{The other possible linearization}
    \label{fig:smEx3L2}
  \end{subfigure}

  \caption{The two possible linearizations of Figure \ref{fig:smEx2}. Possible choices of linearization points are shown in yellow.}
  \label{fig:smEx3}
\end{figure}
\afterpage{\clearpage}

Linearizability is a condition on individual executions of a
distributed application. When only linearizable executions are
permitted by the memory manager, the entire system is called
linearizable.

\begin{definition}[Linearizable system]
  A memory management layer is linearizable if all possible histories
  of the system are linearizable.
\end{definition}

\subsubsection{The CAP Theorem}
Real-world systems often fall short of behaving as a single perfectly
coherent system. The root of this phenomenon is a deep and
well-understood tradeoff between system coherence and performance.
Enforcing consistency comes at the cost of additional communications,
and communications impose overheads, often unpredictable ones.

\begin{definition}[Network partition]
  A \emph{network partition} is a span of time where some nodes are
  unable to communicate with another set of nodes on the network.
\end{definition}

Fox and Brewer \cite{1999foxbrewer} are crediting with observing a
particular tension between the three competing goals of consistency,
availability, and partition-tolerance. This tradeoff was precisely
stated and proved in 2002 by Gilbert and Lynch
\cite{2002gilbertlynchCAP}.  The theorem is often somewhat
misunderstood, as we discuss, so it is worth clarifying the terms
used.

\begin{description}
\item[Consistency] Gilbert and Lynch define consistency as linearizability.
\item[Availability] A CAP-available system is one that responds to
  every client request in a finite time.
\item[Partition tolerance] A partition-tolerant system continues to
  function, in the face of arbitrary partitions in the network. (It is
  possible that a partition never recovers, say if a critical
  communications cable is permanently severed.)
\end{description}

Some reflection shows that the full set of requirements is
unattainable---a partition tolerant system simply cannot enforce both
consistency and availability. We give only the informal sketch here,
leaving the interested reader to consult the more formal analysis by
Gilbert and Lynch. The key technical assumption is that a processes'
behavior can only be influenced by the messages it actually
receives---it cannot be affected by messages that are sent to it but
never delivered.

\begin{theorem}[The CAP Theorem]
  \label{thm:cap}
  In the presense of indefinite network partitions, a distributed system
  cannot guarantee both linearizability and eventual-availability.
\end{theorem}
\begin{proof}
  Consider the concurrent writes by separate processes in Figure
  \ref{fig:smEx2}.  Figure \ref{fig:smEx3} showed that both possible
  linearizations of this history require all reads to return the same
  value, either $3$ or $5$. Now suppose the network is unavailable, so
  the processes cannot communicate, in particular to share write
  updates.

  Now there are only two possibilities. First, the processes could
  proceed despite the lack of communication. In this case, because
  processes do not otherwise affect each other, $P_1$ could not see
  the $W(x,5)$ operation and its read must return the value
  $3$. Likewise, $P_2$ does not see $W(x,3)$ and its reads return
  $5$. This situation violates linearizability, hence the 'C' of CAP.

  Alternatively, the processes might detect that the network is
  unavailable and refuse to respond to read requests. However, this
  indefinitely suspects progress until the network recovers, which
  violates the 'A' of CAP.
\end{proof}

To provide both 'C' and 'A', the last remaining option is to assume
that the network never suffers from partitions, in other words to
sacrifice the 'P' of CAP. Clearly this is unacceptable in the systems
we are considering. Consequently, our systems cannot ensure atomic
consistency and availability.

\begin{comment}
A partition-tolerant CAP-available system cannot indefinitely suspend
handling a request to wait for network activity like receiving a
message. In the event of a partition that never recovers, this would
mean the process could wait indefinitely for the partition to heal,
violating availability.

On the other hand, a CAP-consistent system is not allowed to return
anything but the most up-to-date value in response to client
requests. Keep in mind that any (other) process may be the originating
replica for an update.
\end{comment}

\subsubsection{Sequential consistency}
\label{sequential-consistency}

Enforcing atomic consistency means that an access \(E\) at process
\(P_i\) cannot return to the client until every other process has been
informed about \(E\). For many applications this is an unacceptably
high penalty. A weaker model that is still strong enough for most
purposes is \emph{sequential} consistency. This is an appropriate
model if a form of strong consistency is required, but the system is
agnostic about the precise physical time at which events start and
finish, provided they occur in a globally agreed upon order.

A sequentially consistent system ensures that any execution is
equivalent to some global serial execution, even if this is serial order
is not the one suggested by the real-time ordering of events. When
real-time constraints are not important, this provides essentially the
same benefits as linearizability. For example, it allows programmers to
reason about concurrent executions of programs because the result is
always guaranteed to represent some possible interleaving of
instructions, never allowing instructions from one program to execute
out of order.

\begin{definition}[Legal sequential history]
Let $H$ be a history of invocation/response events occurring on a set
of processes $\{P_i\}_{i = 1 \ldots N}$. A \emph{sequential history}
of $H$ is any choice of total order among the events in $H$---that is,
a rearrangement of the operations as to ensure none of them overlap in
time. If each $P_i$ issues
$r_i$-many requests for some non-negative integer $r_i$, observe there
are a total of
\[
\frac{\left(\sum_{i = 1}^N r_i\right)!}{\prod_{i = 1}^N r_i!}
\]
possible sequential histories.
\end{definition}

\begin{definition}
  \label{def:sequentiallyconsistent}
  A \emph{sequentially consistent} execution is
  characterized by three features:
  \begin{itemize}
  \item All processes act like they agree on a single, global total order
    defined across all accesses.
  \item This sequential order is consistent with the program order of each process.
  \item Responses are semantically correct, meaning reads return the most recent writes (as determined by the global order)
  \end{itemize}
\end{definition}

\begin{figure}[p]
  \setlength\belowcaptionskip{5ex}

  \begin{subfigure}{1\textwidth}
    \centering
    \input{images/pgf/smEx4S1.pgf}
    \caption{An example where the processes read different values}
    \label{fig:smEx1L1}
  \end{subfigure}

  \begin{subfigure}{1\textwidth}
    \input{images/pgf/smEx4S2.pgf}
    \caption{An example where the first write operation affects a read}
    \label{fig:smEx4S2}
  \end{subfigure}

  \caption{Sequentially consistent, nonlinearizable executions of Figure \ref{fig:smEx2}}
  \label{fig:smEx3}
\end{figure}

\begin{figure}[p]
  \setlength\belowcaptionskip{5ex}

  \begin{subfigure}{1\textwidth}
    \centering
    \input{images/pgf/smEx4S3.pgf}
    \caption{}
    \label{fig:smEx1L1}
  \end{subfigure}

  \begin{subfigure}{1\textwidth}
    \input{images/pgf/smEx4S4.pgf}
    \caption{}
    \label{fig:smEx4S2}
  \end{subfigure}

  \caption{Two non-sequentially-consistent executions of Figure \ref{fig:smEx2}}
  \label{fig:smEx3}
\end{figure}

\afterpage{\clearpage}

Processes in a sequentially consistent system are required to agree on
a total order of events, presenting the illusion of a shared database
from an application programmer's point of view. However, this order
need not be given by external order. Instead, the only requirement is
that sequential history must agree with process order, i.e.~the events
from each process must occur in the same order as in they do in the
process.  This is nearly the definition of linearizability, except
that external order has been replaced with merely program order. We
immediately get the following lemma.

\begin{lemma}
  \label{lem:linearsequential}
  A linearizable execution is sequentially consistent.
\end{lemma}
\begin{proof}
  This follows because process order is a subset of external order.
\end{proof}

Visually, sequential consistency allows reordering an execution by
sliding events along each process' time axis like beads along a
string.  Two events from the same process cannot pass over each other
(as this would precisely be a violation of program order), but events
on different processes may be commuted past each other, contravening
external order. This sliding allow noe to construct a fairly arbitrary
interleaving of events, a totally ordered execution with no events
overlapping. From this perspective, while linearizability requires the
existence of a linearization, sequential consistency requires the
existence of an interleaving.

It may seem strange to consider executions such as the one shown in
REF in which operations appear to take effect at different times for
different processes, or at times that do not agree with external
order. The reader should remember that in the background, these
processes would be engaged in message-passing over the network and are
therefore subject to all the complexities previous discussed in
Section \ref{sec:message-passing}, including delayed and out-of-order
messages. Looser requirements by the memory model impose fewer
constraints on the message passing layer requiring less coordination,
and allowing for greater performance.

The converse of Lemma \ref{lem:linearsequential} does not hold. For
example, Figures \ref{fig:smEx4S1} and \ref{fig:smEx4S2} are
nonlinearizable executions of Figure \ref{fig:smEx2} that are
nonetheless sequentially consistency.

\subsubsection{The CAP Theorem for SC}

Recall Figure \ref{fig:smEx1L1}. This example demonstrated that
sequential consistency allows the history in Figure \ref{fig:smEx2} to
return non-equal values for the two read operations. That is, if $P_1$
reads $3$, and both reads on $P_2$ return $5$, the execution is
sequentially consistent. This raises the possibility that sequential
consistency is not subject to the limits of the CAP
theorem. Unfortunately the hope is fleeting: like linearizability,
sequential consistency is CAP-unavailabile.

\begin{lemma}[CAP for sequential consistency]
  \label{thm:cap-sequential}
  An eventually-available system cannot provide sequential consistency in the presense of network partitions.
\end{lemma}
\begin{proof}
  The proof is an adaptation of Theorem \ref{thm:cap}. Suppose $P_1$ and
  $P_2$ form of CAP-available distributed system and consider the
  following execution: $P_1$ reads $x$, then assigns $y$ the value
  $1$. $P_2$ reads $y$, then assigns $x$ the value $1$. (Note that this
  is the sequence of requests shown in Figure \ref{fig:nonsequential1},
  but we make no assumptions about the values returned by the read
  requests). By availability, we know the requests will be handled (with
  responses sent back to clients) after a finite amount of time. Now
  suppose $P_1$ and $P_2$ are separated by a partition so they cannot
  read each other's writes during this process. For contradiction,
  suppose the execution is equivalent to a sequential order.

  If $W(y,1)$ precedes $R(y)$ in the sequential order, then $R(y)$ would
  be constrained to return to $1$. But $P_2$ cannot pass information to
  $P_1$, so this is ruled out. To avoid this situation, suppose the
  sequential order places $R(y)$ before $W(y,1)$, in which case $R(y)$
  could correctly return the initial value of $0$. However, by
  transitivity the $R(x)$ event would occur after $W(x,1)$ event, so it
  would have to return $1$. But there is no way to pass this information
  from $P_1$ to $P_2$. Thus, any attempt to consistently order the
  requests would require commuting $W(y,1)$ with $R(x)$ or $W(x,1)$ with
  $R(y)$, which would violate program order.
\end{proof}

\subsubsection{Causal consistency}
\label{causal-consistency}

\emph{Causal} consistency\citationneeded is a weaker memory model than
sequential consistency. Whereas sequential consistency requires the
system as a whole to behave as if all write operations take place in
some total order (as long as this is consistent with program order),
causal consistency allows different processes to see write operations
take effect in different orders. Instead, only write operations that
are \emph{causally related} need to take effect in a common order
at all processes.

We have not defined what it means for memory operations to be causally
related. The notion is very similar to causal precedence in the
context of message passing (Definition \ref{def:causalprecedence}),
with a small wrinkle: in a message passing context, each receive event
is uniquely paired with a send event. However, because multiple
operations can write the same value into the same location, there is
generally not a unique way to associate each read operation with the
write operation that ``caused'' it to read a particular value.

\begin{definition}[Writes-into order]
  Consider a set of memory operations. A ``writes into''
  $\rightsquigarrow$ order is any binary relation that satisfies the
  following conditions:
  \begin{itemize}
  \item If $o \rightsquigarrow o'$, then $o = W(x, v)$ and $o' = R(x,v)$ for memory location $x$ and value $v$.
  \item For each $o'$, there is exactly one $o$ such that $o \rightsquigarrow o'$
  \end{itemize}
\end{definition}

This definition is a slight simplication of that found
in\citationneeded: To avoid talking having to talk about default
values, we only consider cases here where all read operations are
preceded by some write operation. Intuitively, the idea is that a
writes-into order pairs reads with the write operations whose values
they are ``reading from.'' If all values written into each location
are unique, the writes-into order is unambiguous, as an operation that
reads $v$ from location $x$ could only be paired with the operation
that writes $v$ into $x$.

\begin{definition}[Causal order on memory operations]
  \label{def:memorycausalprecedence}
  For some choice of a writes-into order, we define a binary relation
  $\to$ on the set of memory operations as follows:
  \[o \to o' \iff
  \begin{cases}
    o <_{P_i} o' \textrm{for some process $P_i$}
    \textbf{ or} \\
    o \rightsquigarrow o'
    \textbf{ or} \\
    \textrm{there is some } o'' \textrm{ such that } o \to o'' \textrm{ and } o'' \to o'
  \end{cases}
  \]
  If $o \to o'$, we say $o$ causally precedes $o'$.
\end{definition}

We can now define causally consistent executions of memory operations.

\begin{definition}[Causal consistency]
  \label{def:causalconsistency}
  A history is causally consistent if each process (acts like it) sees
  write requests take place in some order that is consistent with
  causal order for some choice of writes-into order.
\end{definition}

\begin{figure}[p]
  \begin{subfigure}{1\textwidth}
    \centering
    \input{images/pgf/smEx4S3.pgf}
    \caption{}
    \label{fig:smEx1L1}
  \end{subfigure}

  \begin{subfigure}{1\textwidth}
    \input{images/pgf/smEx4S4.pgf}
    \caption{}
    \label{fig:smEx5}
  \end{subfigure}

  \caption{PLACEHOLDER: Causally consistent and inconsistent executions}
  \label{fig:smEx3}
\end{figure}

\afterpage{\clearpage}

\subsubsection{CAP-availability of Causal Consistency}
\label{the-cap-theorem}

\begin{lemma}[Causal consistency is CAP-available]
  \label{thm:cap-causal}
  Causal consistency can be enforced during network partitions. That
  is, causal consistency is not subject to the CAP thereom.
\end{lemma}
\begin{proof}
  Consider processes that execute read and write operations purely
  locally. That is, they never send messages to other processes and
  they only read to their own writes. In this situation, the causal
  relation generated by Definition \ref{def:memorycausalprecedence} is
  essentially unique: operations at $P_i$ are causally preceded by the
  previous operations at $P_i$, and no others. In the absence of a
  non-trivial causal order, causal consistency imposes no constraints
  and the execution is vacuously consistent.
\end{proof}

Unfortunately, the proof of the prior lemma has more to do the
weakness of the promises made causal consistency. The example allows
different processes to deviate arbitrary far from consistency, in the
sense that no processes need to agree on any of the updates applied to
the database. The effect is that causal consistency is too weak to
apply any kind of bound on divergence, which suggests it is not strong
enough for the kinds of safetly-related applications we have in mind.

\subsection{Consequences of CAP}
\label{interpretation-of-the-cap-theorem}
While the CAP theorem is conceptually simple, its interpretation is
subtle and has been the subject of much discussion since it was
introduced \cite{2012CAP12Years}. It is sometimes assumed that the CAP
theorem claims that a distributed system can only offer two of the
properties C, A, and P.  In fact, the theorem constrains, but does not
prohibit the existence of, applications that apply some relaxed amount
of all three features. The CAP theorem only rules out their
combination when all three are interpreted in a highly idealized
sense.

In practice, applications can tolerate much weaker levels of
consistency than linearizability. Furthermore, network partitions are
usually not as dramatic as an indefinite communications blackout. Real
conditions in our context are likely to be chaotic, featuring many
smaller disruptions and delays and sometimes larger
ones. Communications between different clients may be affected
differently, with nearby agents generally likely to have better
communication channels between them than agents that are far
apart. Finally, CAP-availability is a suprisingly weak condition.
Generally one cares about the actual time it takes to handle user
requests, but the CAP theorem exposes difficulties just ensuring the
system handles requests at all. Altogether, the extremes of C, A, and
P in the CAP theorem are not the appropriate conditions to apply to
many, perhaps most, real-world applications.

\subsection{Summary}
\label{sec:background-summary}

We have seen that a distributed system is built from geographically
distant components that must communicate over a network. Sending
messages over the network causes messages to suffer unpredictable
delays. Particularly in the context of broadcasts sent to multiple
members of a group at once, this varying delay often causes messages
to arrive in different orders to different members of the group, which
can lead to chaos if a message-ordering discipline is not imposed.

As part of taming this chaos, it is critical for distributed systems
to track the causal precedence relation between events. Physical
clocks cannot usually be relied upon to maintain adequate
synchronization for this purpose. Instead, logical clocks may be
employed. The different paradigms---scalar, vector, and matrix
clocks---vary in how much information they track and how much overhead
they impose on the system with respect to bandwidth and storage
space. The latter two require nodes to know about every other member
of the group, as is typical for many mechanisms in distributed
systems. If groups can change dynamically, as in our scenarios, then a
group membership protocol is also needed.

Programmers may find it easier to frame distributed applications in
terms of reading and writing from a shared pool of virtual memory
instead of sending messages over a network. However, the fact that
many nodes can access this virtual memory at the same time leads to
non-determinism in how the memory accesses are ordered, which makes it
non-trivial to decide what it means for the system to be
consistent. The two strongest notions of consistency---linearizability
and sequential consistency---essentially provide the illusion of a
single source of truth, but the CAP theorem makes it virtually
impossible to realize these consistency models in the kinds of chaotic
networks we are considering. The causal consistency model ensures a
minimum of coherence and has the advantage that it is not subject to
the limitations of the CAP theorem. However, it makes no guarantees
about how far apart replicas of the same data may diverge, which makes
this model too weak for the kinds of safety-related applications we
have in mind.

\section{Resilient Network Architectures}
\label{sec:networking}



A priori, a network may not deliver a message at all. Alternatively it
might deliver the message multiple times, for example if a device is
unaware that a message has already been delivered. In either case, the
network itself does not alert the sender or receiver to the fact---the
responsibility for detecting such conditions belongs to the sorts of
protocols considered here and in Section \ref{sec:networking}.

If we were to inspect the network blackbox, we would expect to find
so-called ``transport'' protocols like TCP\citationneeded being used
to provide the abstraction of reliable delivery over an otherwise
unreliable network. For example, they might add metadata to messages
that allow the receiver to arrange them in their intended order,
discard duplicates, and verify their integrity. Transport protocols
might also handle retranmission when messages become corrupted in
transit. We consider these kinds of low-level details in Section
\ref{sec:networking}; for now, one could say we are working at a level
of abstraction above the transport layer. Note that transport
reliability mechanisms contribute to the latency in passing messages,
so they cannot solve the problems under consideration in this section.


\subsection{Ad-hoc networking}
\label{ad-hoc-networking}

\subsubsection{Physical communications}
\label{physical-communications}

The details of the physical communication between processes is outside
the scope of this memo. We make just a few high-level observations about
the possibilities, as the details of the network layer are likely to
have an impact on distributed applications, such as the shared memory
abstraction we discuss below and in Section
\ref{sec:continuous-consistency}. For such applications, it may be
important to optimize for the sorts of usage patterns encountered in
real scenarios, which are affected by (among other things) the low-level
details of the network.

The \emph{celluar} model (Figure \ref{fig:centralized}) assumes nodes
are within range of a powerful, centralized transmission station that
performs routing functions. Message passing takes place by transmitting
to the base station (labeled \(R\)), which routes the message to its
destination. Such a model could be supported by the ad-hoc deployment of
portable cellphone towers transported into the field, for instance.

The \emph{ad-hoc} model (Figure \ref{fig:decentralized}) assumes nodes
communicate by passing messages directly to each other. This requires
nodes to maintain information about things like routing and the
approximate location of other nodes in the system, increasing complexity
and introducing a possible source of inconsistency. However, it may be
more workable given (i) the geographic mobility of agents in our
scenarios (ii) difficult-to-access locations that prohibit setting up
communication towers (iii) the inherent need for system flexibility
during disaster scenarios.

\begin{figure}[h]
  \centering
  \begin{subfigure}[b]{0.48\textwidth}
    \centering
    \includegraphics[width=\textwidth]{images/Centralized.png}
    \caption{Cellular network topology}
    \label{fig:centralized}
  \end{subfigure}
  \hfill
  \begin{subfigure}[b]{0.48\textwidth}
    \centering
    \includegraphics[width=\textwidth]{images/Decentralized.png}
    \caption{Ad-hoc network topology}
    \label{fig:decentralized}
  \end{subfigure}
  \caption{Network topology models for geodistributed agents. Edges represent communication links (bidirectional for simplicity).}
  \label{fig:nettopology}
\end{figure}

One can also imagine hybrid models, such as an ad-hoc arrangement of
localized cells. In general, one expects more centralized topologies to
be simpler for application developers to reason about, but to require
more physical infrastructure and support. On the other hand, the ad-hoc
model is more fault resistant, but more complicated to implement and
potentially offering fewer assurancess about performance. In either
case, higher-level applications such as shared memory abstractions
should be tuned for the networking environment. It would be even better
if this tuning can take place dynamically, with applications
reconfiguring manually or automatically to the particulars of the
operating environment. This requires examining the relationship between
the application and networking layers, rather than treating them as
separate blackboxes.

\hypertarget{delay-tolerant-networking}{%
  \subsection{Delay-tolerant networking}\label{delay-tolerant-networking}}

\hypertarget{ad-hoc-dtns}{%
  \subsection{Ad-hoc DTNs}\label{ad-hoc-dtns}}

An interesting possibility is for the \emph{network} to automatically
configure itself to the quality-of-service needs of the application. For
example, a client that receives a lot of requests may be marked as a
preferred client and given higher-priority access to the network. If UAV
vehicles can be used to route messages by acting as mobile transmission
base stations, one can imagine selecting a flight pattern based on
networking needs. For example, if the communication between two
firefighting teams is obstructed by a geographical feature, a UAV could
be dispatched to provide overhead communication support. Such an
arrangement could greatly blur the line between the networking and
application layers.

\hypertarget{software-defined-networking}{%
  \subsection{Software-defined
    networking}\label{software-defined-networking}}

\hypertarget{verification-of-networking-protocols}{%
  \subsection{Verification of networking
    protocols}\label{verification-of-networking-protocols}}

\newpage

\hypertarget{continuous-consistency}{%
  \section{Continuous Consistency}\label{continuous-consistency}}

\label{sec:continuous-consistency}


%We shall consider the DSM abstraction and some of the different
%consistency requirements we can enforce in this setting.

%A fundamental distributed application is the \emph{shared distributed
%memory} abstraction. We shall assume that all processes maintain a
%local replica of a globally shared data object, as replication
%increases system fault tolerance. For simplicitly, we shall discuss
%the data store as a simple key-value store, but it could be something
%else like a database, filesystem, persistent object, etc.



Strong consistency is a discrete proposition: an application provides
strong consistency or it does not. For many real-world applications, it
evidently makes sense to work with data that is consistent up to some
\(\epsilon \in \mathbb{R}^{\geq 0}\). Thus, we shift from thinking about
consistency as an all-or-nothing condition, towards consistency as a
bound on inconsistency.

The definition of \(\epsilon\) evidently requires a more or less
application-specific notion of divergence between replicas of a shared
data object. Take, say, an application for disseminating the most
up-to-date visualization of the location of a fire front. It may be
acceptable if this information appears 5 minutes out of date to a
client, but unacceptable if it is 30 minutes out of date. That is, we
could measure consistency with respect to \emph{time}. One should expect
the exact tolerance for \(\epsilon\) will be depend very much on the
client, among other things. For example, firefighters who are very close
to a fire have a lower tolerance for stale information than a central
client keeping only a birds-eye view of several fire fronts
simultaneously.

Now suppose many disaster-response agencies coordinate with to update
and propagate information about the availability of resources. A client
may want to lookup the number of vehicles of a certain type that are
available to be dispatched within a certain geographic range. We may
stipulate that the value read by a client should always be \(4\) of the
actual number, i.e.~we could measure inconsistency with respect to some
numerical value.

In the last example, the reader may wonder we should tolerate a client
to read a value that is incorrect by 4, when clearly it is better to be
incorrect by 0. Intuitively, the practical benefit of tolerating weaker
values is to tolerate a greater level of imperfection in network
communications. For example, suppose Alice and Bob are individually
authorized to dispatch vehicles from a shared pool. In the event that
they cannot share a message.

Or, would could ask that the the value is a conservative estimate,
possibly lower but not higher than the actual amount. In these examples,
we measure inconsistency in terms of a numerical value.

As a third example,

By varying \(\epsilon\), one can imagine consistency as a continuous
spectrum. In light of the CAP theorem, we should likewise expect that
applications with weaker consistency requirements (high \(\epsilon\))
should provide higher availability, all other things being equal.

Yu and Vahdat explored the CAP tradeoff from this perspective in a
series of papers \cite{2000tact,2000tactalgorithms,10.5555/1251229.1251250,DBLP:conf/icdcs/YuV01,2002tact}
propose a theory of \emph{conits}, a logical unit of data subject to
their three metrics for measuring consistency. By controlling the
threshold of acceptable inconsistency of each conit as a continuous
quantity, applications can exercise precise control the tradeoff between
consistency and performance, trading one for the other in a gradual
fashion.

They built a prototype toolkit called TACT, which allows applications to
specify precisely their desired levels of consistency for each conit. An
interesting aspect of this work is that consistency can be tuned
\emph{dynamically}. This is desirable because one does not know a priori
how much consistency or availability is acceptable.

The biggest question one must answer is the competing goals of
generality and practicality. Generality means providing a general notion
of measuring \(\epsilon\), while practicality means enforcing
consistency in a way that can exploit weakened consistency requirements
to offer better overall performance.

\begin{itemize}
\item
  The tradeoff of CAP is a continuous spectrum between linearizability
  and high-availability. More importantly, it can be tuned in real time.
\item
  TACT captures neither CAP-consistency (i.e.~neither atomic nor
  sequential consistency) nor CAP-availability (read and write requests
  may be delayed indefinitely if the system is unable to enforce
  consistency requirements because of network issues).
\end{itemize}

\hypertarget{causal-consistency-1}{%
  \subsection{Causal consistency}\label{causal-consistency-1}}

Causal consistency is that each clients is consistent with a total order
that contains the happened-before relation. It does not put a bound on
divergence between replicas. Violations of causal consistency can
present clients with deeply counterintuitive behavior.

\begin{itemize}
  \tightlist
\item
  In a group messaing application, Alice posts a message and Bob
  replies. On Charlie's device, Bob's reply appears before Alice's
  original message.
\item
  Alice sees a deposit for \$100 made to her bank account and, because
  of this, decides to withdraw \$50. When she refreshes the page, the
  deposit is gone and her account is overdrawn by \(50\). A little while
  later, she refreshes the page and the deposit reappears, but a penalty
  has been assessed for overdrawing her account.
\end{itemize}

In these scenarios, one agent takes an action \emph{in response to} an
event, but other processes observe these causally-related events taking
place in the opposite order. In the first example, Charlie is able to
observe a response to a message he does not see, which does not make
sense to him. In the second example, Alice's observation at one instance
causes her to take an action, but at a later point the cause for her
actions appears to have occurred after her response to it. Both of these
scenarios already violate atomic and sequential consistency because
those models enforce a system-wide total order of events. Happily, they
are also ruled out by causally consistent systems. The advantage of the
causal consistency model is that it rules out this behavior without
sacrificing system availability, as shown below.

Causal consistency enforces a global total order on events that are
\emph{causally related}. Here, causal relationships are estimated very
conservatively: two events are potentially causally if there is some way
that the outcome of one could have influenced another.

\begin{figure}
  \center
  \includegraphics[scale=0.4]{images/causal1.png}
  \caption{A causally consistent, non-sequentially-consistent execution}
\end{figure}

\begin{lemma}
  Sequential consistency implies causal consistency.
\end{lemma}
\begin{proof}
  This is immediate from the definitions. Sequential consistency
  requires all processes to observe the same total order of events,
  where this total order must respect program order. Causal consistency
  only requires processes to agree on events that are potentially
  causally related. Program order is a subset of causal order, so any
  sequential executions also respects causal order.
\end{proof}

However, causal consistency is not nearly as strong as sequential
consistency, as processes do not need to agree on the order of events
with no causal relation between them. This weakness is evident in the
fact that the CAP theorem does not rule out highly available systems
that maintain causal consistency even during network partitions.

\begin{lemma}
  A causally consistent system need not be unavailabile during partitions.
\end{lemma}
\begin{proof}

  Suppose $P_1$ and $P_2$ maintain replicas of a key-value store, as
  before, and suppose they are separated by a partition. The strategy is
  simple: each process immediately handles read requests by reading from
  its local replica, and handles write requests by applying the update
  to its local replica. It is easy to see this leads to causally
  consistent histories. Intuitively, the fact that no information flows
  between the processes also means the events of each process are not
  related by causality, so causality is not violated.  \end{proof}

Note that in this scenario, a client's requests are always routed to the
same processor. If a client's requests can be routed to any node, causal
consistency cannot be maintained without losing availability. One
sometimes says that causal consistency is ``sticky available'' because
clients must stick to the same processor during partitions.

The fact that causal consistency can be maintained during partitions
suggests it is too weak. Indeed, there are no guarantees about the
difference in values for \(x\) and \(y\) across the two replicas.

\hypertarget{tact-system-model}{%
  \subsection{TACT system model}\label{tact-system-model}}

As in Section \ref{sec:background}, we assume a distributed set of
processes collaborate to maintain local replicas of a shared data object
such as a database. Processes accept read and write requests from
clients to update items, and they communicate with each other to ensure
to ensure that all replicas remain consistent.

However, access to the data store is mediated by a middleware library,
which sits between the local copy of the replica and the client. At a
high level, TACT will allow an operation to take place if it does not
violate user-specific consistency bounds. If allowing an operation to
proceed would violate consistency constraints, the operation blocks
until TACT synchronizes with one or more other remote replicas. The
operation remains blocked until TACT ensures that executing it would not
violate consistency requirements.

\[\textrm{Consistency} = \langle \textrm{Numerical error, \textrm{Order error}, \textrm{Staleness}} \rangle.\]

Processes forward accesses to TACT, which handles commiting them to the
store. TACT may not immediately process the request---instead it may
need to coordinate with other processes to enforce consistency. When
write requests are processed (i.e.~when a response is sent to the
originating client), they are only commited in a \emph{tenative} state.
Tentative writes eventually become fully committed at some point in the
future, but when they are commited, they may be reordered. After
fullying committing, writes are in a total order known to all processes.

\begin{figure}[h]
  \center
  \includegraphics[scale=0.4]{images/TACT Logs.png}
  \caption{Snapshot of two local replicas using TACT}
  \label{fig:tact_logs}
\end{figure}

A write access \(W\) can separately quantify its \emph{numerical weight}
and \emph{order weight} on conit \(F\). Application programmers have
multiple forms of control:

Consistency is enforced by the application by setting bounds on the
consistency of read accesses. The TACT framework then enforces these
consistency levels.

\hypertarget{measuring-consistency-on-conits}{%
  \subsection{Measuring consistency on
    conits}\label{measuring-consistency-on-conits}}

\hypertarget{numerical-consistency}{%
  \paragraph{Numerical consistency}\label{numerical-consistency}}

\hypertarget{order-consistency}{%
  \paragraph{Order consistency}\label{order-consistency}}

When the number of tentative (uncommitted) writes is high, TACT executes
a write commitment algorithm. This is a \emph{pull-based} approach which
pulls information from other processes in order to advance \(P_i\)'s
vector clock, raising the watermark and hence allowing \(P_i\) to commit
some of its writes.

\hypertarget{real-time-consistency}{%
  \paragraph{Real time consistency}\label{real-time-consistency}}

\hypertarget{enforcing-inconsistency-bounds}{%
  \subsection{Enforcing inconsistency
    bounds}\label{enforcing-inconsistency-bounds}}

\hypertarget{numerical-consistency-1}{%
  \paragraph{Numerical consistency}\label{numerical-consistency-1}}

We describe split-weight AE. Yu and Vahdat also describe two other
schemes for bounding numerical error. One, compound AE, bounds absolute
error trading space for communication overhead. In their simulations,
they found minimal benefits to this tradeoff in general. It is possible
that for specific applications the savings are worth it. They also
consider a scheme, Relative NE, which bounds the relative error.

\hypertarget{order-consistency-1}{%
  \paragraph{Order consistency}\label{order-consistency-1}}

\hypertarget{real-time-consistency-1}{%
  \paragraph{Real time consistency}\label{real-time-consistency-1}}

\hypertarget{future-work}{%
  \subsection{Future work}\label{future-work}}

\hypertarget{data-fusion}{%
  \section{Data Fusion}\label{data-fusion}}

\cite{1999:lucien-datafusion}

\label{sec:data-fusion}

Strong consistency models provide the abstraction of an idealized global
truth. In the case of conits, the numerical, commit-order, and real-time
errors are measured with respect to an idealized global state of the
database. This state may not exist on any one replica, but it is the
state each replica would converge to if it were to see all remaining
unseen updates.

We consider distributed applications that receive data from many
different sources, such as from a sensor network (broadly defined). It
will often be the case that some sources of data should be expected to
agree with each other, but they may not. A typical scenario, we want to
integrate these data into a larger model of some kind. Essentially take
a poll, and attempt to synthesize a global picture that agrees as much
as possible with the data reported from the sensor network.

Here, we need a consistency model to measure how successful our attempts
are to synthesize a global image. And to tell us how much our sensors
agree. Ideally, we could use this system to diagnose disagreements
between sensors, identifying sensors that appear to be malfunctioning,
or to detect abberations that necessitate a response.

\hypertarget{fusion-centers}{%
  \subsection{Fusion centers}\label{fusion-centers}}

To be written.

\hypertarget{sheaf-theory}{%
  \subsection{Sheaf theory}\label{sheaf-theory}}

\hypertarget{introduction-to-presheaves}{%
  \subsubsection{Introduction to
    presheaves}\label{introduction-to-presheaves}}

\begin{definition}
  A \emph{partially order-indexed family of sets} is a family of sets indexed by a partially-ordered set,
  such that orders between the indices correspond to functions between the sets.
\end{definition}

We can also set \((P, \leq)\) \emph{acts on} the set
\(\{S_i\}_{i \in I}\).

\begin{definition}
  A \emph{semiautomaton} is a monoid paired with a set.
\end{definition}

This is also called a \emph{monoid action} on the set.

\begin{definition}
  A copresheaf is a *category acting on a family of sets*.
\end{definition}

\begin{definition}
  A presheaf is a *category acting covariantly on a family of sets*.
\end{definition}

\hypertarget{introduction-to-sheaves}{%
  \subsubsection{Introduction to sheaves}\label{introduction-to-sheaves}}

To be written.

\hypertarget{the-consistency-radius}{%
  \subsubsection{The consistency radius}\label{the-consistency-radius}}

To be written.

\hypertarget{conclusion}{%
  \section{Conclusion}\label{conclusion}}

\label{sec:conclusion}

To be written.

\section*{Bibliography}\label{bibliography}
\addcontentsline{toc}{section}{Bibliography}

\bibliographystyle{abbrv}
\bibliography{bibliography}
\end{document}

%%% Local Variables:
%%% mode: LaTeX
%%% TeX-master: t
%%% End:


\begin{figure}[p]
  \begin{subfigure}[a]{1\textwidth} \center
    \includegraphics[scale=0.4]{images/linear1.png} \caption{A
      linearizable execution. Any choice of linearization works here.}
    \label{fig:linear_example11} \end{subfigure}
  \begin{subfigure}[b]{1\textwidth} \center
    \includegraphics[scale=0.4]{images/nonlinear0.png} \caption{A
      non-linearizable execution. The request to read $y$ returns a
      stale value. } \label{fig:linear_example12} \end{subfigure}
  \caption{A linearizable and non-linearizable execution.}
  \label{fig:linear_example1} \end{figure}

\begin{figure}[p]
  \begin{subfigure}[a]{1\textwidth}
    \center
    \includegraphics[scale=0.4]{images/linearTemplate.png}
    \caption{An execution with read responses left unspecified.}
    \label{fig:nonlinear}
  \end{subfigure}
  \begin{subfigure}[b]{1\textwidth}
    \center
    \includegraphics[scale=0.4]{images/linear3.png}
    \caption{A linearizable execution for which both reads return $1$.}
  \end{subfigure}
  \begin{subfigure}[c]{1\textwidth}
    \center
    \includegraphics[scale=0.4]{images/linear2.png}
    \caption{A linearizable execution for which both reads return $2$.}
  \end{subfigure}
  \caption{Two linearizable executions of the same underlying events that return different responses. Possible linearization points are shown in red.}
  \label{fig:linearization}
\end{figure}

\begin{figure}[p]
  \begin{subfigure}[a]{1\textwidth}
    \center
    \includegraphics[scale=0.4]{images/nonlinear1.png}
    \caption{A nonlinearizable execution with the read access returning disagreeing values. We will see later (Figure \ref{fig:sequential}) that this execution is still sequentially consistent. }
    \label{fig:nonlinear1}
  \end{subfigure}
  \begin{subfigure}[b]{1\textwidth}
    \center
    \includegraphics[scale=0.4]{images/nonlinear2.png}
    \caption{Another nonlinearizable execution with read access values swapped. This execution is not sequentially consistent.}
    \label{fig:nonlinear2}
  \end{subfigure}
  \caption{Two non-linearizable executions of the same events shown in Figure \ref{fig:linearization}.}
  \label{fig:nonlinearizable}
\end{figure}

\begin{figure}
  \begin{subfigure}[a]{1\textwidth}
    \center
    \includegraphics[scale=0.4]{images/sequential1.png}
    \caption{A non-linearizable, sequentially consistent execution.}
    \label{fig:sequential1}
  \end{subfigure}
  \begin{subfigure}[b]{1\textwidth}
    \center
    \includegraphics[scale=0.4]{images/sequential2.png}
    \caption{An equivalent interleaving of \ref{fig:sequential1}.}
    \label{fig:interleaving1}
  \end{subfigure}
  \caption{A sequentially consistent execution and a possible interleaving.}
  \label{fig:sequential}
\end{figure}

\begin{figure}
  \begin{subfigure}[a]{1\textwidth}
    \center
    \includegraphics[scale=0.4]{images/nonsequential1.png}
    \caption{A non-sequentially consistent execution.}
    \label{fig:nonsequential1}
  \end{subfigure}
  \begin{subfigure}[b]{1\textwidth}
    \center
    \includegraphics[scale=0.4]{images/nonsequential_x.png}
    \caption{The sequentially consistent history of $x$.}
    \label{fig:sequentialx}
  \end{subfigure}
  \begin{subfigure}[b]{1\textwidth}
    \center
    \includegraphics[scale=0.4]{images/nonsequential_y.png}
    \caption{The sequentially consistent history of $y$.}
    \label{fig:sequentialy}
  \end{subfigure}
  \caption{A non-sequentially consistent execution with sequentially-consistent executions at each variable.}
  \label{fig:nonsequential}
\end{figure}

%%% Local Variables:
%%% mode: LaTeX
%%% TeX-master: t
%%% End:
