% !TeX document-id = {beb7ced9-b3cd-42b2-b16a-3ed3c633a1d9}
\documentclass[]             % options: RDPonly, coveronly, nocover
{NASA}                       %   plus standard article class options
%\DeclareRobustCommand{\mmodels}{\mathrel{|}\joinrel\Relbar}

\usepackage[utf8]{inputenc}
\usepackage{csquotes}
\usepackage{setspace}
\usepackage{hyperref}
\usepackage{amsmath, amssymb, amscd, amsthm, amsfonts}
\usepackage{mathtools}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{amsthm}
\usepackage[english]{babel}
\usepackage{stmaryrd}
\usepackage{proof}
\usepackage{tikz-cd}
\tikzcdset{scale cd/.style={every label/.append style={scale=#1},
    cells={nodes={scale=#1}}}}
% Added for subfigures
\usepackage{caption}
\usepackage{subcaption}
\usepackage{afterpage}
\usepackage{comment}

\newtheorem{theorem}{Theorem}[section]
\newtheorem{corollary}{Corollary}[theorem]
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{example}{Example}
\theoremstyle{definition}
\newtheorem{definition}{Definition}[section]

\newcommand{\B}{\mathbf{B}}
\newcommand{\w}{\mathbf{w}}
\newcommand{\citationneeded}{\footnote{\textbf{CITATION NEEDED}}}

% Globally redefine pgfpicture to use \Large fonts
\let\origpgfpicture=\pgfpicture
\def\pgfpicture{\origpgfpicture\LARGE}

% Try loading this package to prevent so much hyphenation
% as recommended by https://stackoverflow.com/questions/1609837/latex-breaking-up-too-many-words
\usepackage{microtype}

% Try setting matrix columns closer
\setlength\arraycolsep{2pt}

\title{A Survey of Distributed Systems Challenges for Wildland
  Firefighting and Disaster Response}

\author{Lawrence Dunn and Alwyn E. Goodloe}

\AuthorAffiliation{Lawrence Dunn \\ Department of Computer and Information
  Science \\ University of Pennsylvania \\ Philadelphia, PA \\ Alwyn Goodloe\\                                          % for cover page
  NASA Langley Research Center, Hampton, Virginia
}
\NasaCenter{Langley Research Center\\Hampton, Virginia 23681-2199}
\Type{TM}                    % TM, TP, CR, CP, SP, TT
\SubjectCategory{64}         % two digit number
\LNumber{XXXXX}              % Langley L-number
\Number{XXXXXX}              % Report number
\Month{12}                   % two digit number
\Year{2022}                  % four digit number
\SubjectTerms{Distributed Systems, Formal Methods, Logic, }     % 4-5 comma separated words
\Pages{46}                   % all the pages from the front to back covers
\DatesCovered{}              % 10/2000--9/2002
\ContractNumber{}            % NAS1-12345
\GrantNumber{}               % NAG1-1234
\ProgramElementNumber{}
\ProjectNumber{}             % NCC1-123
\TaskNumber{}                % Task 123
\WorkUnitNumber{}            % 123-45-67-89
\SupplementaryNotes{}
\Acknowledgment{The work was conducted during a summer internship at the NASA Langley Research Center in the Safety-Critical Avionics Systems Branch focusing on distributed computing  issues arising in the Safety Demonstrator challenge in the NASA Aeronautics System Wide Safety (SWS) program.}

%Added for Pandoc
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}


\abstract{The System Wide Safety (SWS) program has been investigating
  how crewed and uncrewed aircraft can safely operate in shared
  airspace. Enforcing safety requirements for distributed agents
  requires coordination by passing messages over a communication
  network. Unfortunately, the operational environment will not admit
  reliable high-bandwidth communication between all agents,
  introducing theoretical and practical obstructions to global
  consistency that make it more difficult to maintain safety-related
  invariants. Taking disaster response scenarios, particularly
  wildfire suppression, as a motivating use case, this self-contained
  memo discusses some of the distributed systems challenges involved
  in system-wide safety through a pragmatic lens. We survey topics
  ranging from consistency models and network architectures to data
  replication and data fusion, in each case focusing on the practical
  relevance of topics in the literature to the sorts of scenarios and
  challenges we expect from our use case.  }

\begin{document}
\newpage
\setcounter{tocdepth}{2}
\tableofcontents
\newpage

\section{Introduction}\label{introduction}

Civil aviation has traditionally focused primarily on the efficient
and safe transportation of people and goods via the airspace. Despite
the inherent risks, the application of sound engineering practices and
conservative operating procedures has made flying the safest mode of
transport today. Now the desire not to compromise this safety makes it
difficult to integrate unmanned vehicles into the airspace, accomodate
emerging applications, and keep pace with unprecedented recent growth
in commercial aviation. To that end, the System Wide Safety (SWS)
project of the NASA Aeronautics' Airspace Operations and Safety
Program (AOSP) has been investigating technologies and methods by
which crewed and uncrewed aircraft may safely operate in shared
airspace.

This memo surveys topics in computing that are relevant to maintaining
system-wide safety across large, physically distributed data and
communication systems. It aims to be self-contained and accessible to
a technical audience without a deep background in distributed
systems. Our primary motivating use cases have been taken from civil
emergency response scenarios, especially wildfire suppression and
hurricane relief, primarily for three reasons. First, improved
technology for wildfire suppression, especially related to
communications and data sharing, is frequently cited as a national
priority \cite{pcast2023}.  Second, the rules for operating in the US
national airspace are typically relaxed during natural disasters and
relief efforts, so this is a suitable setting for testing new
technologies. Finally, this setting is an excellent microcosm for the
sorts of general challenges faced by other, non-emergency
applications.

If there is a central theme uniting the sections of this manuscript,
it is \emph{continuity} in the sense considered by
topology.\footnote{For a typical introductory textbook see
\cite{mendelson2012introduction}.} The systems we consider will be
subject to harsh operating conditions that limit how well they can
perform---for example, wireless communication is typically less
reliable during inclement weather. To build a system that is
predictable (clearly a prerequisite for safety), one must ensure the
system is flexible enough to perform reasonably well under a wide
variety of adverse conditions. In other words, the behavior of a safe
system should in some sense be a \emph{continuous} function of its
inputs and environment. This sort of robust design is particularly
challenging because distributed systems designers are forced to make
delicate tradeoffs between competing objectives, most famously between
performance and consistency, the topic of Section
\ref{sec:background}.

\subsection{Summaries of the sections}\label{summaries-of-the-sections}

Sections \ref{sec:disaster-response}--\ref{sec:desiderata} contain
background material on disaster response, distributed systems, and the
specifics of our use case. The critical takeaway of these sections is
that system-wide safety is, at least in part, a computer science
problem, indeed a software problem, and not ``just'' a matter of
engineering better hardware. Sections
\ref{sec:networking}--\ref{sec:data-fusion} survey particular topics
from the distributed systems literature, proceeding from lower-level
considerations to higher-level ones; these sections may be read
independently of each other. Below we summarize each section.

Section \ref{sec:disaster-response} starts with a pragmatic summary of
disaster response and some of the relevant computing challenges in
that setting. We aim to justify and explain the role of distributed
systems theory in system-wide safety by citing real examples
encountered in disaster response scenarios.

Section \ref{sec:background} is an introduction to distributed
systems, culminating in a illustrative result: the ``CAP'' theorem(s)
for the atomic and sequential consistency models (Theorems
\ref{thm:cap} and \ref{thm:cap-sequential}, respectively). CAP is
considered a ``negative'' result, meaning it proves something cannot
be done. The CAP theorem proves that strong consistency for a
distributed system makes systemwide network performance an upper bound
on the availability of a system to do useful work for clients, which
for our purposes is an unacceptable restriction. The practical
significance of CAP is that in emergency response environments, agents
will always act with incomplete information about the global system, a
key motivation for Section \ref{sec:continuous-consistency}.

Section \ref{sec:desiderata} refines our assumptions and identifies
desirable properties of systems for our use case. We use these points to
frame the discussion of systems and protocols in subsequent sections.

Section \ref{sec:networking} examines networking considerations. Our
vision of future emergency communication networks integrates concepts
from delay/disruption-tolerant networks (DTN) and mobile ad-hoc
networks (MANET) to provide digital communications that are robust to
a turbulent operational environment. We also examine the state of
software-defined networking (SDN). SDN is an emerging field that puts
networking protocols on the same footing as ordinary computer
programs. In theory, this should furnish computer networking with all
the benefits of modern software engineering, such as reprogrammable
hardware, rapid iteration, version control, and especially formal
verification.

Section \ref{sec:continuous-consistency} describes a hypothetical
application that might be used in a disruption-heavy network: a data
replication service built on Yu and Vahdat's theory of ``conits"
(short for ``consistency unit'') \cite{2002tact}. This framework
realizes a \emph{continuous} consistency model in the sense that, as
typically configured, it provides neither strong consistency nor
guaranteed high-availability, but rather a quantifiable and
controllable tradeoff between the two. The key idea is that many
applications can tolerate inconsistency among replicas of a data item
if an upper bound on the divergence between replicas is enforced. A
conit-based database replication framework would allow system
designers to define units of replicated data whose consistency is of
interest, enforce policies bounding inconsistency between replicas of
these items, and even dynamically tune these policies on the fly. We
believe that only a conit-based replication infrastructure can provide
the strict guarantees required for safety-related systems while also
tolerating the adverse environments and real-world limitations of the
systems we have in mind.

Section \ref{sec:data-fusion} concerns data fusion. Now and in the
future, agents in disaster scenarios will make decisions informed by
many different kinds of information. Efficient integration,
processing, filtering, and dissemination of this information will be
necessary to avoid ``swimming in sensors and drowning in data''
\cite{2010:magnuson}.  This task is especially challenging because
agents will often work with incomplete or out of date information, and
different sources of the same data may be contradictory, e.g. first
responders may receive contradictory reports about whether a structure
is occupied. One promising trend in this space, which we briefly
introduce in this section, is the development of sheaf theory as a
natural mathematical model for data fusion
\cite{2017robinsonCanonical}. Sheaf theory provides a rigorous
framework for discussing how heterogeneous sources of noisy data can
be integrated into a coherent picture, and can formally measure how
well this task has been achieved.

We conclude in Section \ref{sec:conclusion} by recapping some of the
main themes in this document and highlighting areas where design
decisions at various levels must be made to build a system that is
tuned to the exact conditions we can expect from real-world
scenarios. Such decisions might be informed by a combination of
simulation and experimentation in the field.

\section{Coordination Challenges in Disaster Response}
\label{sec:disaster-response}

This section explains aspects of disaster response (particularly
firefighting) scenarios that motivate the remainder of this
document. We describe how real-world environments give rise to
foundational challenges that must be addressed through the application
of distributed computing principles. Even deploying the best
communications equipment cannot realistically avoid the fundamental
computer scientific problems raised when many distributed agents try
to coordinate their actions over a widespread area.

The operational environment of wildfire suppression, hurricane relief,
and other disaster settings is generally characterized by systemic
communications challenges. It is not surprising that a 2023 report on
wildland firefighting modernization by the President’s Council of
Advisors on Science and Technology (PCAST) cites ``the vulnerabilities
and shortfalls in wildland firefighter communications, connectivity,
and technology interoperability'' in their first of five
recommendations \cite{pcast2023}. These shortfalls can be partly
attributed to factors that are simply inherent to disaster response:
remote locations, difficult terrain, damaged infrastructure, harsh
weather, and limited battery power, to name a few.

Agents in the field generally experience high packet loss (when
considering digital communication), garbled transmissions, and
unpredictable latencies when passing information over the
communication network(s). A conservative view suggests expecting the
worst performance at particularly inopportune times, simply because
the conditions that prompt urgent communication can be expected to
correlate with those that make communication difficult. One
possibility is that the disaster itself damages the communications
infrastructure.  Another example, which is true tautologically, is
that a communications network is the most congested, and therefore the
least available, precisely when everyone needs to use it. Both of
these phenomena were famously exhibited in the immediate aftermath of
the September $11^\textrm{th}$ terrorist attacks when sudden user
demand and severed trunk cables brought New York public cell phone
networks virtually to a halt \cite{2011:Reardon}. Other networks along
the East Coast, as well as dedicated networks for first responders,
experienced similar effects.

From a systems perspective, an unreliable network presents a challenge
for coordinating distributed agents. A root cause of this problem is
that coordinated action requires some notion of consistency,
i.e.~agreement, among data shared between agents. We shall make this
somewhat vague notion more precise in Section \ref{sec:background},
but a simple example is that it is very important for everyone to
agree which firetrucks have been dispatched to which scenes, which
tasks should be prioritized, or which radios have been reserved from
the {\mbox{NIICD}} radio cache \cite{radiocache} and for whom. The
exact meaning of ``consistency'' varies between applications, but a
general observation is that stronger standards for consistency are
more difficult to maintain than relaxed ones because they require
exchanging more information with other agents quickly, putting a
heavier burden on the network. When the network is slow, system
components that need to communicate may have to wait for network to
deliver their messages, diminishing the efficacy and coherence of the
system.

\subsection{Communication and Safety}
\label{communication-and-safety}

Many complications in the field are exacerbated by a poor
communications environment. In this setting, agents must choose
between long delays in sending and receiving information or acting
with only limited knowledge. Typically they will experience some
amount of both. Both options present safety challenges because
operational safety, by nature, requires agents to gather information
about their environment and react to it quickly and systematically. In
other words, both inaction and uninformed action are problems. This
turns out to be deeply related to a computer science phenomenon
generally known as the safety/liveness tradeoff.

As a running example, we consider the use of firefighting airtankers,
the largest of which are the Very Large Airtankers (VLATs), defined as
those carrying more than 8,000 gallons of water or fire retardant
\cite{2019:airtankerops}. The largest VLATs can carry more than 20,000
gallons, weighing about 170,000 pounds. This weight is typically
dropped from a mere 250 feet above the tree canopy
\cite{2019:airtankerops}, though the complexity of the maneuver means
errant drops are sometimes performed even lower than this, easily
crushing a ground vehicle \cite{2019:stickney}. A 2018 accident led to
the death of one firefighter and the injury of three others when an
87-foot Douglas Fir tree was knocked down by an unexpectedly forceful
drop from a Boeing 747-400 Supertanker \cite{2018:calfire}.

\begin{figure}[h]
  \label{fig:airtanker}
  \centering
  \includegraphics[scale=0.4]{images/dc10.jpg}
  \caption{A DC-10 airtanker, rated for 9,400 gallons, drops retardant above Greer, Arizona. \textbf{CITATION NEEDED}}
\end{figure}

Suppose that in the future, firefighters are equipped with GPS sensors
and digital transmitters---this could be an application running on
their personal cell phones, or better yet something running on
dedicated and more reliable equipment. A seemingly reasonable policy
would be to prohibit VLATs from performing a drop if its computers do
not have up-to-date information about the location of firefighters on
the ground. The problem is that obtaining this information may be
difficult or impossible: perhaps heavy smoke, a damaged radio tower,
or a tall ridge prevents communications between the air and ground. In
these scenarios, rigid enforcement of the safety policy would prevent
airtankers from operating.

This scenario exemplifies a classic tradeoff between opposing goals:
system \emph{safety} and system \emph{availability} (or
\emph{liveness}), elaborated on in Section \ref{sec:background}. In
the distributed computing context, safety properties guarantee that a
system will not perform an action that violates a constraint. In this
example, a reasonable safety property could look like the following:
\begin{quote}
  \interlinepenalty=10000 % Punish pagebreaks inside this quote!
  $\textbf{P}_\textrm{safe}$: All ground agents are known to be at
  least 100 feet outside the drop zone, and this information is
  current to within 30 seconds, or airtankers will not perform a drop.
\end{quote}
By contrast, liveness properties stipulate that the system
will certainly perform requested actions, typically within some time
bound. In our scenario, an expected liveness property might be the
following:
\begin{quote}
  $\textbf{P}_\textrm{live}$: Airtankers will perform a drop within 20
  minutes\footnote{In an interview with PBS, the Chief of Flight
  Operations for Cal Fire cited 20 \mbox{minutes} as the response time
  for aerial firefighting units within designated responsibility areas
  \cite{2021:aerialfirefighting}.} of receiving a request.
\end{quote}

Safety and liveness are frequently dual mandates: safety, in the sense
used here, requires a system \textbf{never} to perform certain
actions, while liveness requires a system to \textbf{always} perform
certain actions. The tension between these ideals means the two often
cannot be guaranteed simultaneously. Such is the case in our example:
if firefighters are unable to broadcast their locations to the pilot,
then the pilot's actions are impeded to maintain
\(\textbf{P}_\textrm{safe}\) at the cost of
\(\textbf{P}_\textrm{live}\), allowing the fire to spread in the
meantime.\footnote{A slight linguistic idiosyncrasy exhibited here is
that liveness properties---not just ``safety'' properties---can also
be relevant to human safety. Thus, the narrow technical meaning of
safety properties for distributed systems does not capture the whole
meaning of System Wide Safety.}

Besides the safety/liveness tradeoff, the previous example exhibits
two other aspects of reasoning about distributed systems. We pause to
draw attention to them.

\paragraph{Epistemology}
Observe that the issue in the VLAT example does not simply disappear
if no ground personnel are actually within 100 feet of a drop
zone. That is, it is not simply a matter of whether a danger is
factually present. To guarantee \(\textbf{P}_\textrm{safe}\), an
airtanker's actions must be restricted when its computers do not
\emph{know} whether an action would violate
\(\textbf{P}_\textrm{safe}\)---knowledge of the fact, and not merely
the fact of it, is the crucial part. In philosophical terms, the logic
of distributed agents is inherently an \emph{epistemic} one, meaning
it must take into account not just what is true but what is known. The
need to share knowledge is what drives communication and puts a burden
on the network.

\paragraph{Discontinuity}
The properties $\mathbf{P}_\textrm{safe}$ and
$\mathbf{P}_\textrm{live}$ are inflexible, all-or-nothing
propositions. The complexity of the operational environment demands
considering more flexible kinds of properties. Suppose that agents are
known to be $500$ feet outside the drop zone, the extra margin meaning
they are well away from any danger, but the information is only
current to within 35 seconds. Clearly this is good enough information
to authorize a drop, but strictly speaking the 5-second difference is
a violation of $\mathbf{P}_\textrm{safe}$. When safety properties are
this rigid, the system's behavior becomes overly sensitive to the
particulars of the environment and therefore difficult to predict,
which is precisely the kind of \emph{discontinuity} that we aim to
prevent. Our goal is to build reliable systems that perform well in a
wide range of circumstances.



%To make the example more extreme, $\mathbf{P}_\textrm{safe}$ is
%violated even if the information is current to within 31 seconds, so
%a mere extra second of delay makes all the difference between whether
%the VLAT can operate, even though the firefighters in question are
%clearly far outside the drop zone.



%as it makes the system's behavior difficult to predict, being so
%sensitive to the particulars of the environment. Preventing this kind
%of discontinuity can be difficult because we still want to provide
%rigorous guarantees about the system's behavior, i.e. we cannot forgo
%enforcing properties like $\mathbf{P}_\textrm{safe}$ altogether.

%In reality, a human controller would likely decide that
%the computer \emph{does} have enough information, and decide to
%override the policy. However, it is difficult to make assurances about
%a system if external actors are frequently in the habit of overruling
%its decisions.

\subsection{Communication Patterns in the Field}
\label{communication-in-practice}

We now consider some of the communication patterns that occur in
wildland firefighting. The layman reader may be surprised to learn
that the state of the art is somewhat primitive, which is partly
attributable to the fact that very little permanent infrastructure
exists in this setting. This fact is also what makes wildfires an
interesting and generalizable example for other kinds of civil
disaster environments.

One trend we will draw attention to is a kind of ``geospatial locality
of reference'' principle that system designers should take into
account. By this, we mean the happy coincidence of two observations
which, if not exactly guaranteed rules, are at least approximately
true in many circumstances. The first observation is simple:
\begin{quote}
  $\textbf{O}_1$: Agents with the most urgent need to
  coordinate their actions will tend to be located closer to each
  other and require the same kinds of information.
\end{quote}
The second observation is as follows:
\begin{quote}
  $\textbf{O}_2$: Agents that are located closer together
  will tend to have more reliable communications between them than
  agents that are far apart. Conversely, information that must travel
  a long distance tends to be delayed or degrade in quality.
\end{quote}

We will refer to the concomitance of these two facts as simply the
``locality'' principle. The reason the locality principle is crucial
is that, as we see in Section \ref{sec:background}, there are major
theoretical and practical limits to how well \emph{all} agents in the
system can share \emph{all} information with each other, i.e. how well
a system can achieve global consistency. As luck would have it, in
many cases this will not be required: it will be often be enough for
\emph{some} agents to share \emph{some} information with each other, a
fact that raises opportunities to optimize scare network resources. Of
course, this does raise the question of how to decide which
information must be shared with whom, and how to use this knowledge to
best exploit the communication network. We will revisit this question,
without the pretense of answering it conclusively, throughout Sections
\ref{sec:networking}, \ref{sec:continuous-consistency} and
\ref{sec:data-fusion}. For now, we resume our examination of what
communication patterns look like today.

%Generally speaking, we expect that agents with a higher need to
%coordinate their actions will tend to be located closer to each
%other, which in turn correlates with an ability to communicate
%quickly and reliably. This kind of principle motivates the sort of
%decentralized, ad-hoc networking protocols considered in Section
%\ref{sec:networking}. It can also affect the design of higher-level
%applications like the one in Section
%\ref{sec:continuous-consistency}.

\paragraph{Communication on the ground}

In the field, communication between firefighters and other agents is
often facilitated by handheld (analog) land-mobile radios, which are
inherently limited in their battery life, bandwidth, effective range,
and ability to work around environmental factors like foliage and
smoke.

As an alternative to using a radio, it is common for wildland
firefighters in the field simply to shout commands and notifications
to nearby personnel. This is a clear manifestation of the locality
principle: a substantial amount of communication occurs directly
between nearby firefighters working on the same or closely related
tasks, and in some cases they are so nearby they can communicate
without any network infrastructure at all. In a future environment
where agents might be equipped with body-worn sensors and heads-up
displays (HUDs) \citationneeded, this sort of local communication
might be facilitated by simple low-power technologies such as
Bluetooth \citationneeded, without the need for more sophisticated
(and heavy) equipment.

Communication over a long distance requires infrastructural support,
such as the use of cell towers and repeater stations. Typically,
disaster response environments have scarce permanent infrastructure:
in a wildland fire setting, perhaps a few repeaters mounted to a
nearby watch tower. Ad-hoc infrastructure, such as a cell on wheels
(COW) or cell on light truck (COLT), can sometimes be deployed on an
as-needed basis if the location allows for it. An extremely common
issue is making sure that all equipment is properly configured, for
instance that all radios are listening on the correct frequencies,
particularly when different agencies and groups need to
interoperate. The difficulty of interoperability was another issue
exhibited during the September $11^\textrm{th}$ attacks, which was a
major impetus for the creation of the nationwide public safety
broadband network (NPSBN) FirstNet \citationneeded. However, we can
only imagine that interoperability between different agencies and
vendors will remain a challenge in the future.

\begin{figure}[t]
  \centering
  \includegraphics[scale=0.085]{images/ironside.jpg}
  \caption{The Ironside Mountain lookout and radio repeater station, destroyed in the 2021
    Monument fire, shown with protective foil on August
    $10^\textrm{th}$, 2015 during the 2015 River Complex fire. This
    particular fire burned 77,077 acres over 77 days. \textbf{CITATION NEEDED}}
  \label{fig:ironside}
\end{figure}

Use of centralized infrastructure comes with the potential for
widespread failure when the infrastructure breaks down. For example,
in California, the Ironside Mountain lookout/repeater station, seen in
Figure \ref{fig:ironside}, was destroyed during the 2021 Monument
Fire, which burned approximately 223,124 acres over 88 days
\cite{2021:monumentfire}. The Ironside Mountain station had strategic
importance, being located on a tall ridge. According to a video blog
from a volunteer firefighter involved in the incident\citationneeded,
its loss prevented communication between operators on different sides
of the ridge, in networking parlance creating a \emph{partition} that
lasted until crews could ascend the ridge to deploy a temporary
station:
\begin{quote}
  ``When {[}the Ironside Mountain lookout station{]} burned down the radio
  repeater went with it. And so communications were lost across the
  fire\ldots{} one side of the fire couldn't talk to the other side\ldots.
  So it was kind of a critical job to get that road cleared so that the
  radio crews could go back up there and set up a temporary radio tower.''
\end{quote}
A scenario where communication between two groups is completely
severed is exactly the sort of thing considered by the CAP theorem in
Section \ref{sec:background}.

\paragraph{Vehicles on the ground}
Large numbers of ground vehicles are involved in wildfire
suppression. A large wildfire response can involve up to 100
firetrucks distributed over a large geographical area. Bulldozers and
similar vehicles are also commonly used to control the landscape and
perimeter of the fire. An advantage of vehicles is that they can carry
heavier, which is to say better, communications equipment than a
human. For instance, a vehicle could be equipped with a satellite link
as well as a local wireless area network (WLAN) base station, serving
as a bridge between agents in the field and central coordinators
(e.g. incident commanders).

\paragraph{Communication in the air}

Wildland firefighting increasingly involves the use of helicopters and
fixed wing aircraft. Civil aviation has traditionally employed simpler
communication patterns than this use case demands. For instance,
aircraft equipped with Automatic Dependent Surveillance-Broadcast
(ADS-B) monitor their location using GPS and periodically broadcast
this information to air traffic controllers and nearby aircraft. This
sort of scheme has worked well in traditional applications, where
pilots typically only monitor the general locations of a few nearby
aircraft. The locality principle is exhibited here, too: aircraft have
the highest need to coordinate when they are physically close and
therefore in range of each other's ADS-B broadcasts.

In our setting, a large number or aircraft, easily a half dozen or
more, may need to operate in a small area, near complex terrain,
during adverse conditions, often at low altitude. In other words, the
demands are many and the margins for error are small. This sort of use
case calls for more sophisticated coordination schemes between
airborne and ground-based elements than solutions like ADS-B provide
by themselves.

As aircraft generally have better line-of-site to ground crews than
ground crews have to each other, firefighters sometimes relay messages
to air-based units over the radio, which in turn is relayed back down
to other ground units. The locality principle comes into play here
too, but this time in the reverse direction: this relay scheme allows
knowledge to travel farther but requires more effort, and the extended
reach comes at the cost of introducing delays and possible degradation
of message quality, as in the classic game of telephone. Hence, this
sort of message passing should be reserved for more critical
information.

In some cases, planes from the Civil Air
Patrol\citationneeded\footnote{A civil auxiliary of the U.S. Air
Force. https://www.gocivilairpatrol.com/} have been equipped with
radio repeaters and dispatched to wildfires to provide service to
ground-based units. In the future, this sort of service could be
provided autonomously by base stations mounted to unmanned aerial
vehicles (UAVs), which might perform additional functions such as
tracking the fire perimeter. More generally, future systems should
transparently facilitate exchanging information between agents in a
decentralized fashion that is robust to the failure of any one
component. In Section \ref{sec:networking} we imagine a resilient
ad-hoc digital network built from handheld and ground- and
air-vehicle-mounted devices, permanent base stations, portable
temporary infrastructure, and so on.
%---a high-tech modernization of the sort of informal relay schemes
%operating today over traditional radio channels.

\subsection{Towards the Future}\label{towards-the-future}

So far we have said a lot about the state of disaster response
today. A distributed system for this sort of challenge should be
designed for the kinds of environments and conditions expected in the
near- to medium-term future, so we briefly turn our attention to some
of our expectations for this topic.

Perhaps the most prominent expectation for future disaster response
events is a heavy reliance on \emph{data}. Besides improvements to
communications that facilitate information sharing, we expect advances
in machine intelligence to greatly influence how this data is
handled. Agents in disaster response environments will be both
producers and consumers of data, and this data will need to processed
by humans and machines in ways that agents can readily make sense of
to support their decision-making. Our background research indicated
many different kinds of data that could be valuable for
responders. Just some of the kinds of information and communication we
expect include the following:
\begin{itemize}
  \tightlist
\item
  Free-form communication, especially recorded voice messages
  broadcast to many agents at once, which may need to be processed by
  machines to extract the most pertinent information into a more
  actionable format
\item
  The exact or estimated location of victims, firefighters, vehicles,
  hazards, etc.
\item
  Medical information gathered from victims, perhaps stored in and
  collected from digital triage tags\citationneeded
\item
  Data about current and predicted fire or weather patterns
\item
  Topographic information about the terrain, highlighting for instance
  the location of rivers and roads that could form a fire control line
\item
  Planned escape routes, rendezvous points, safety zones, and landing
  zones
\item
  Availability and dispatching of assets, e.g.~ambulances, airtankers,
  or crews on standby\footnote{NOTE Need to mention Monares et al 2011}
\end{itemize}
In a perfect environment, such information would be shared with all
necessary agents in whole and instantly. In reality, agents will be
presented with information that is sometimes incomplete, out of date,
or contradictory---all problems that are further exacerbated by an
unreliable network. A superficially contradictory concern is that the
information presented will be \emph{overcomplete}, filled with petty
details that distract agents from their important tasks.

%Of course, there is no actual contradiction between incomplete
%information and information that is unneeded and useless at best.

In some ways, future systems for disaster response will bear
resemblence to future systems for warfighting, such as the conceptual
\emph{Internet of Battle Things} (IoBT) \citationneeded. Chiefly,
agents ``under extreme cognitive and physical stress'' will be subject
to a highly dynamic and dangerous environment. Various kinds of
technology will assist humans by providing data to support
sensemaking, but a contraindicating concern will be flooding agents with a
``massive, complex, confusing, and potentially
deceptive\footnote{While deliberately adversarial network behavior
seems like less of a concern for disaster response agents than
warfighters, we conjecture that a similar ``fog of war'' may
lead to confusing or contradictory reports that share similarities
with intentionally deceptive behavior.} ocean of information.'' To
avoid ``swimming in sensors and drowning in data''
\cite{2010:magnuson}:
\begin{quote}
``Humans seek well-formed, reasonably-sized, essential information
  that is highly relevant to their cognitive needs, such as effective
  indications and warnings that pertain to their current situation and
  mission.'' \citationneeded
\end{quote}
The most distinctive feature of the Internet of Battle Things, which
separates it from the everyday internet of things, is ``the
adversarial nature of the environment.'' To some extent this adversial
behavior is common also to disaster response. We previously cited a
real-world example of a critical communications station destroyed by
wildfire, perhaps not unlike an attack by enemy forces.

Lest we overstate the similarities between a battlefield environment
and civil disaster response, we expect that a distinctive feature of
the latter is a greater emphasis on the preservation of scarce network
resources. More so than a tactical military unit, a group of (say)
volunteer firefighters has to make do with off-the-shelf equipment
rather than purpose-built, best in class hardware like sophisticated
satellite links. Dedicated logistical support, and even things like
allocated radio frequencies, will likely be in shorter supply, while
adverse conditions like inclement weather will be almost
guaranteed. Therefore we expect a complex interaction between the
high-level needs of distributed applications and low-level concerns
about network resources. This is because only the applications have
enough information to determine which data is the most important and
must be shared with whom first, while only the network-level protocols
have enough information and control to make prudent use of scarce
network availability. In contravention to the common wisdom that
applications should be relatively blind to network considerations---or
conversely that network protocols ought to be transparent to
applications---our setting calls for mechanisms allowing the two
layers to have some influence over each other. This interaction
between the network and applications will be considered in more detail
in Sections \ref{sec:networking}, \ref{sec:continuous-consistency},
and \ref{sec:data-fusion}.

To give an example, a central data fusion center may be used to detect
and alert responders to a fire that has accidentally moved beyond a
control line (known as \emph{slopover}), or it may warn firefighters
when they have strayed too far from an escape route or safety
zone. Such information would be of high importance, and it would be
worthwhile to expend network resources conveying this information to
the relevant parties. It might also be nice for firefighters to have
access to real-time information about the GPS location of every other
firefighter. However, if this strains the network, then perhaps the
exact location of teammates, but only the general location of other
crews, is called for. If the network is extremely constrained, perhaps
only information immediately relevant to preserving life should be
sent in order to ensure the network is able to deliver this
information quickly. The key point is that from where we stand, we
cannot give a blanket rule determining which information is important,
as this is partly a dynamic calculation influenced both by the
criticality of the information to the task at hand and how much unused
network capacity is available at that moment at that location.

%For developers, it is clear that this requires some coordination
%between the application and the networking layers, as the latter must
%be given enough information to make the most prudent use of limited
%resources.

\section{Introduction to Distributed Systems}
\label{sec:background}
In this section we review two core aspects of distributed systems
theory, message passing and distributed shared memory, and address how
these fit into the picture painted in Section
\ref{sec:disaster-response}. The primary reference we are following is
the manuscript by Kshemkalyani and Singhal
\cite{kshemkalyani_singhal_2008}, particularly chapters 1, 3, 6, and
12. The main theme of the section is that distributed applications are
sensitive to tradeoffs, typically between performance and a
user-perceived notion of consistency.

In general, a distributed system is a collection of independent
entities that cooperate to solve a problem that cannot be individually
solved \cite{kshemkalyani_singhal_2008}. Our motivating examples are
systems that facilitate cooperation between firefighters and other
disaster response agents.  The system components are typically
computers, smartphones, sensors, or other kinds of communication
equipment. These may be carried by individuals, mounted to ground or
air-based vehicles, deployed as temporary or permanent infrastructure,
or attached to autonomous devices. The kinds of tasks considered
include navigating safely in close proximity, delivering resources to
remote locations, suppressing fires, and collecting, processing, and
disseminating data about the environment.

We consider two paradigms for framing distributed applications. In the
message passing framework, the fundamental activity is sending and
receiving information over the network. Alternatively, the fundamental
activity of the shared memory framework is collaboratively updating
data items shared by multiple agents. In either case, a chief concern
is to shield users from the complexity of the underlying system and
``{[}appear{]} to the users of the system as a single coherent
computer'' \cite{TanenbaumSteen07}, when it is really a distributed
collection of loosely coupled devices. The main obstacle to this goal
is that the network connecting the devices causes messages to be
delayed unpredictably. When communication is delayed, information
becomes out of date, and separated nodes will have different and
possibly conflicting information. This situation tends to force a
choice: delay operations and wait for messages to be passed, or
proceed at the risk of diverging from total global consistency.

\subsection{Message Passing}
\label{ssec:message-passing}

Singhal and Shivaratri \cite{10.5555/562065} offer the following
definition of a distributed computing system:

\begin{quote}
  ``A collection of computers that do not share common memory or a common
  physical clock, that communicate by message passing over a communication
  network, and where each computer has its own memory and runs its own
  operating system.''
\end{quote}
The most notable thing which makes a system ``distributed'' is that
the nodes communicate by passing messages. Communication is
facilitated by some form of network, a restriction implied by absence
of a common memory. (Processes with common memory generally share
information by writing it to a common memory location.) Section
\ref{sec:disaster-response} explained why messages sent over the
network experience a perceptible delay or \emph{latency}, especially
in the context of disaster response where latencies may be on the
order of minutes or even hours. First, passing messages requires the
sender and receiver both to be within range of suitable access points
to the network, which in these environments is likely true only
intermittently. Then, messages must be sent over the air or through
cables and processed, carried, and forwarded by any number of
intermediate networking devices, and they must be retransmitted in
case of unrecoverable errors during transit. In a tactical
environment, packet loss can be on the order of \%xx. \citationneeded

The limitations of the network should be taken as environmental
factors over which system designers can exercise only limited
control. Consequently, network latencies must be tolerated to within
the range of conditions experienced in the field. This raises
fundamental questions of computer science: How, and how well, can
agents coordinate their actions when communication between them is
subject to unpredictable delays?

\subsubsection{Assumptions about the Network}
To focus on high-level questions about consistency and coordination,
we introduce a few simplifying assumptions. We treat the communication
network as a blackbox that sends messages from a single destination to
a single receiver, subject to an unpredictable delay.  We assume that
if the same sender dispatches more than one message to a single
destination, they arrive at their destination in the order they were
sent. Below, this is called the FIFO (first-in, first-out) network
model. Section \ref{sec:networking} will consider the low-level
details of how the network facilitates this kind of message passing
model, but for now we take the fact for granted.

A major limitation of the FIFO model is that it does not constrain the
order of delivery of messages from different senders or with different
destinations. This means whenever group communication is considered
between three or more agents, messages typically arrive in different
orders at different destinations, at least at the lowest level of the
network protocol stack used by each device. To facilitate a coherent
abstraction for users and applications, higher-level protocols must be
employed that account for out-of-order delivery and offer a more
predictable interface to the network, such as by enforcing a common
message order across an entire group. We shall just scratch the
surface of this topic by discussing a basic mechanism common to many
such protocols: using a system of ``logical'' clocks to track the
causal order of events throughout the system.

\subsubsection{FIFO and Causal Ordering}

We model a system as a set \(\mathcal{P} = \{P_i\}_{i\in I}\) of
\emph{processes} executing on independent, geographically dispersed
computing devices, which we consolidate under the generic term
\emph{nodes}. It is important to note, in general, that different
nodes can have different characteristics, such as capabilities and
intended uses. In other words, the system is heterogeneous. For now we
take a high-level view that treats nodes simply as blackboxes.


Figure \ref{fig:message-latencies} shows several time diagrams for
messages $\{m_i\}_{i=1}^N$ sent between three processes $P_1$, $P_2$,
and $P_3$. The $x$-axis of the diagram depicts the flow of physical
time from left to right. Each process consists of a worldline
depicting the \emph{events} occurring in that process. Each message
$m$ starts off as a message \emph{send event} $m^\textrm{send}$
indicating the moment the message is sent over the network by its
originating process; its delivery generates a receive event
$m^\textrm{recv}$. The (send, receive) pairs are connected by an arrow
and said to be corresponding events. Note that the subscripts on the
messages only serve to disambiguate them to the reader. They are not
part of the message and have no semantic importance.

\begin{figure}[p]
  \setlength\belowcaptionskip{5ex}

  \begin{subfigure}{1\textwidth}
    \centering
    \input{images/pgf/mpEx1.pgf}
    \caption{Depiction of latencies in message passing}
    \label{fig:message-latencies-a}
  \end{subfigure}

  \begin{subfigure}{1\textwidth}
    \centering \input{images/pgf/mpEx2.pgf}
    \caption{$P_1$ has a low-latency connection to $P_2$ and high-latency connection to $P_3$}
    \label{fig:message-latencies-b}
  \end{subfigure}


  \begin{subfigure}{1\textwidth}
    \centering \input{images/pgf/mpEx3.pgf}
    \caption{Broadcasting example satisfying FIFO but not CO}
    \label{fig:message-latencies-c}
  \end{subfigure}

  \caption{Message-passing time diagram examples}
  \label{fig:message-latencies}
\end{figure}

\afterpage{\clearpage}

The fact that arrows are diagonal, rather than vertical, represents
the varying delay with which messages are delivered. Because of this
delay, two messages $m$ and $m'$ may arrive in a different order than
they were sent in. In Figure \ref{fig:message-latencies-a}, $P_1$
sends messages $m_2$ and $m_4$ respectively to $P_3$ and $P_2$, in
that order, but the message to $P_2$ arrives first. This may depict a
scenario where $P_1$ and $P_3$ are far apart and have a higher-latency
connection to each other. \ref{fig:message-latencies-b} depicts an
extreme example where message $m_1$ is sent before all others but is
last to be delivered. This diagram may depict a scenario where $P_2$
has an even better connection $P_1$, while $P_3$'s connection has
gotten worse, perhaps because the nodes are moving around the
environment.

We assume just one exception to the fact that messages can arrive out
of order. As highlighted earlier, at this level of abstraction it is
reasonable to consider the FIFO (first-in, first-out) model, which
means messages from the same sender to the same recipient arrive in
the order they were sent in. FIFO ordering is one of the basic
services that might be provided by the network transport protocol (see
Section \ref{sec:networking}). However, this guarantee only applies to
the relative order of two pairs of messages with the same source and
destination; it does not constrain messages with different
destinations or senders. The reader may wish to verify that all
figures in this section satisfy FIFO: a violation of it would
correspond to intersecting lines with the same (sender, receiver)
pair.

Because FIFO does not apply to messages sent between three or more
nodes, distributed applications can experience anomolies in message
order. A distinct possibility is for messages to violate the
\emph{causal ordering} (CO) assumption. ``Causality'' in this context
refers to the potential for information from one event to influence
another. A key problem for distributed systems is that network
latencies mean events with a causal relation may appear to happen in
an illogical order, which makes tracking causality a key issue. We
start by defining an order between events on the same process.

\begin{definition}
  For two events $e$ and $e'$ that both occur in process $P_i$, we
  write $e <_{P_i} e'$ if $e$ occurs before $e'$ in $P_i$'s
  worldline.
\end{definition}

The previous definition is unambiguous because we assume events occur
at discrete, non-overlapping moments at each process. Less clear is
how to order events that occur on different processes. The most
fundamental order considered is the \emph{causal precedence} relation,
also called Lamport's ``happens before'' relation \citationneeded.

\begin{definition}[Causal precedence]
  \label{def:causalprecedence}
  We define a binary relation $\to$ on the set of events as follows:
  \[e \to e' \iff
  \begin{cases}
    e <_{P_i} e' \textrm{ for some process $P_i$}
    \textbf{ or} \\
    e = m^\textrm{send} \textrm{ and } e' = m^\textrm{recv}
    \textbf{ or} \\
    \textrm{there is some } e'' \textrm{ such that } e \to e'' \textrm{ and } e'' \to e'
  \end{cases}
  \]
  If $e \to e'$, we say $e$ has \emph{causal precedence over} or
  \emph{happens before} $e'$.
\end{definition}% Should here be mentioned logical concurrency?

In words, there are three ways $e$ could have causal precedence over
$e'$.  First it may be that the two events occur on the same process
and $e$ literally happens before $e'$. It may also be that $e$ is a
message send event and $e'$ is the corresponding receive event at
another process. Finally, it could be that $e$ has precedence over
some intermediate event $e''$ that has precedence over $e'$, i.e. we
consider the transitive closure of the previous two
conditions. Informally, $e \to e'$ holds whenever one can put a finger
on an event $e$ in the time diagram and trace a path to $e'$ by moving
left-to-right along the worldlines or following the arrows connecting
corresponding message event pairs.

The names ``causal precedence'' and ``happens before'' can be misnomers: $e
\to e'$ can hold without it being the case that $e$ really caused $e'$
to happen, nor is it that case that if $e$ literally occurs before
$e'$ that $e \to e'$ holds. Intuitively, $e \to e'$ conveys that
information from $e$ potentially affected or could have influenced
event $e'$. A general desideratum is to avoid a scenario where $e \to
e'$ holds in reality but, from a process's or user's perspective, it
seems that $e'$ actually occured before $e$. We define this formally
before considering an example.

\begin{definition}(Causally ordered delivery)
  \label{def:causalorder}
  A network satisfies the causally ordered (CO) delivery model if
  messages are received in an order consistent with the causality
  between send events. That is, the following property holds:
  \begin{quote}
  For all messages $m$ and $n$ with the same destination, if
  $m^\textrm{send} \to n^\textrm{send}$, then $m^\textrm{recv} \to
  n^\textrm{recv}$.
  \end{quote}
  In mathematical terms, for each destination $P_\mathrm{dest}$, when
  we consider the set of all messages sent from anywhere to
  $P_\mathrm{dest}$, the function mapping send events to corresponding
  receive events must be monotonic with respect to causal
  precedence.
\end{definition}

For motivational purposes we consider a simple example. We slightly
generalize the notion of a message and allow messages with multiple
receipients (which could be realized by sending independent messages
treated as one unit for present purposes). Our example is a group
messaging application used by three firefighters, shown in Figure
\ref{fig:message-latencies-c}, though in reality the processes may be
machines sending messages at orders of magnitude faster than a human
conversation. To post a message, a process sends it to the other two
members of the group. For reasons explained above, the two recipients
generally do not receive the message at the same time.

\begin{example}
  \label{exmpl:ambulancedispatch}
  In Figure \ref{fig:message-latencies-c}, we imagine the following
  conversation has taken place:
\begin{itemize}
\item [$P_1$]: ``I need an ambulance at location A.''
\item [$P_2$]: ``Understood, the last ambulance has been dispatched.''
\end{itemize}
However, $P_3$ witnesses $P_2$'s answer before seeing $P_1$'s
question. This results in $P_3$ hearing a rather different conversation snippet:
\begin{itemize}
\item [$P_2$]: ``Understood, the last ambulance has been dispatched.''
\item [$P_1$]: ``I need an ambulance at location A.''
\end{itemize}
\end{example}
From $P_3$'s perspective, it seems that $P_1$ is requesting resources
that are not available. The apparent conflict can lead to requests
going unanswered, or possibly handled twice, leading to a chaotic
situation and misallocation of resources. If these broadcasts
represented machine-to-machine traffic between instances of a
distributed application, it is likely the application would misbehave
if not explicitly designed for this situation.

FIFO is vaccuously satisfied in Figure \ref{fig:message-latencies-c}
because all four arrows have a distinct (sender, receiver)
pair. However, the diagram violates Definition \ref{def:causalorder}
because the send event of $P_1$'s question has causal precedence over
$P_2$'s answer, while in $P_3$'s worldline, the receive event of the
answer has causal precedence over the receipt of question. It is
possible to implement broadcast-ordering protocols that present the
abstraction of multiple parties who receive group messages in the same
order at all locations. Unsurprisingly, such protocols often require
mechanisms for tracking the causal relation between events.

\subsection{Timestamps and Synchronicity}
It may appear that we can avoid the ambiguous situation described
above by attaching to each event a \emph{timestamp} tracking when the
event occurred. Let us see why a na\"ive application of this idea
fails. For each event $e$, let $C(e)$ denote the timestamp attached to
that event. The fundamental property we want to satisfy is the
following one:
\begin{definition}
  A system of timestamps satisfies the \emph{clock consistency
  condition} if the following monotonicity property holds:
  \[ \textrm{for all events $e$ and $e'$, } e \to e' \implies C(e) < C(e') \label{eq:mp}\tag{CC} \]
\end{definition}
In words, \ref{eq:mp} requires that if $e$ causally precedes $e'$,
then $e$ should have a lesser timestamp. In Example
\ref{exmpl:ambulancedispatch}, we had two send events
$m_1^\textrm{send} \to m_2^\textrm{send}$, so the clock consistency
condition would require that
\[C(m_1^\textrm{send}) < C(m_2^\textrm{send}).\]

Now message $m_1$ is timestamped and sent by $P_1$, while $m_2$ is
timestamped and sent by $P_2$. A hallmark of distributed systems is we
cannot assume processes have instantaneous access to a common physical
clock, which means the inequality cannot be guaranteed unless $P_1$
and $P_2$ happen to have two different clocks that are
synchronized. Whether we can assume clocks are synchronized depends on
the exact environment and the level of synchronicity required, but
generally sychronization is either unavailable or requires additional
mechanisms to prevent clocks from falling out of sync.

Physical clocks, especially inexpensive ones in consumer-grade
devices, suffer from \emph{drift}, which is to say they do not all run
at the same rate. They are also notoriously prone to misconfiguration,
say if a device administrator sets the date, time, timezone, or
daylight saving time policy (etc.) incorrectly. The devices we are
considering may also spend a long time unpowered in storage, during
which time they may not maintain an always-on clock.  For all these
reasons, ordinary device clocks by themselves are not very
reliable. They can be made more reliable using protocols like the
venerable Network Time Protocol (NTP) \citationneeded to bring them
into synchronization with respect to authoritative atomic clocks. Over
a standard internet connection, the NTP works well enough to bring a
device clock into synchronization with atomic clocks within 100ms,
typically better. However, though NTP is robust to the network
environment, this may not extend into the sorts of highly intermittent
networks considered here and in Section \ref{sec:networking}. We will
consider further issues of synchronization in that section.

%The NTP assumes devices already have a somewhat reliable network connection
%(by means that cannot themselves rely on synchronized clocks, of
%course, which may rule out settings that rely on time-sensitive access
%control mechanisms or public key infrastructure).

%In general, we cannot assume access to physical clocks so precisely
%synchronized as to fundamentally solve the problems discussed in this
%section.

For some coarse-grained purposes, possibly including the database
replication framework in Section \ref{sec:continuous-consistency}, it
may be enough for devices to maintain clocks that are only somewhat
precisely synchronized. For other purposes, such as enforcing a total
order on a sequence of broadcasts among large distributed groups,
sufficiently precise synchronization of clocks is an unrealistic
assumption.

It also happens that merely timestamping messages, even with perfectly
sychronized clocks, is not enough to answer every question we could
ask about causal precedence. For example, it is especially desirable
if we can assume the \emph{strong} clock consistency condition:
\begin{definition}
  A system of timestamps satisfies the strong clock consistency
  condition if the following property holds:
  \[ \textrm{for all events $e$ and $e'$, } e \to e' \iff C(e) < C(e') \label{eq:sc}\tag{SC} \]
\end{definition}
\ref{eq:sc} strengthens \ref{eq:mp} by stipulating that we can always
compare two timestamps to infer whether the events are causally
related. This condition cannot be achieved using simple timestamps
even from synchronized clocks. This is because physical timestamps
always assign a relative order between any two messages. However, we
have seen that some events are physically ordered but not causally
related, and simply comparing physical timestamps does not reveal the
lack of causal relation. An example of this situation is found in
Figure \ref{fig:message-latencies-b}, where $m_1^\textrm{send}$
physically precedes all other events but does not causally precede any
event except $m_1^\textrm{recv}$.

\subsection{Logical Clocks}
Distributed applications can systematically track causality by
employing a system of \emph{logical clocks}, which usually measure the
logical flow of time by storing non-negative integers that are
advanced according to certain rules. Three major variants are common:
scalar, vector, and matrix clocks. These fall on a kind of spectrum,
in that scalar clocks are simple but provide the most coarse-grained
information, while vector and matrix clocks provide increasingly more
information but impose greater overheads.

%quadratic in the number of processes but provide extremely precise
%information about causality, with vector clocks in the middle.

\subsubsection{Scalar clocks}
Scalar clocks, introduced by Lamport\citationneeded, require each
process $P_i$ to maintain a single scalar value $C_i$, a non-negative
integer initialized at $0$. There are two update rules:
\begin{enumerate}
\item Before a message is sent, $C_i$ is updated according to the rule
  \[C_i := C_i + 1.\]
  This new value is the timestamp attached to the message send
  event. The value is sent (``piggybacked'') as part of the message
  metadata for use by the receiver.
\item When $P_i$ receives a message timestamped with value $C$, the
  process updates $C_i$ according to the rule
  \[C_i := \max(C, C_i) + 1.\]
  The updated value is the timestamp associated with the receive
  event.
\end{enumerate}

\begin{figure}[p]
  \setlength\belowcaptionskip{5ex}

  \begin{subfigure}{1\textwidth}
    \centering \input{images/pgf/mpEx1Sc.pgf}
    \caption{Scalar clocks for \ref{fig:message-latencies-a}}
    \label{fig:message-latencies-scalar-a}
  \end{subfigure}

  \begin{subfigure}{1\textwidth}
    \centering \input{images/pgf/mpEx2Sc.pgf}
    \caption{Scalar clocks for \ref{fig:message-latencies-b}}
    \label{fig:message-latencies-scalar-b}
  \end{subfigure}


  \begin{subfigure}{1\textwidth}
    \centering \input{images/pgf/mpEx3Sc.pgf}
    \caption{Scalar clocks for \ref{fig:message-latencies-c}}
    \label{fig:message-latencies-scalar-c}
  \end{subfigure}
  \caption{Scalar clock examples}
  \label{fig:message-latencies-scalar}
\end{figure}

\afterpage{\clearpage}

Figure \ref{fig:message-latencies-scalar} depicts the same events in
Figure \ref{fig:message-latencies} alongside the scalar timestamp that
would be assigned to each event. It is not hard to prove, and the
reader should verify, that scalar clocks satisfy the clock consistency
condition (\ref{eq:mp}). Intuitively, this is because a node ensures
its clock is incremented before every event and always greater than
the timestamp piggybacked on top of any message it receives.

\begin{lemma}
  A system of scalar clocks satisfies $e \to e' \implies C(e) < C(e')$.
\end{lemma}

However, scalar clocks do not satisfy the strong condition
(\ref{eq:sc}). That is, even if the timestamp of one event $e'$ is
greater than that of $e$, the possiblity is left open that $e$ does
not have causal precedence over $e'$. For example, examining Figures
\ref{fig:message-latencies-b} and
\ref{fig:message-latencies-scalar-b}, $C(m_1^\textrm{send})$ has a
globally minimal timestamp value of $1$, which is less than the
timestamp of every other event except
$m_2^\textrm{send}$. Nonetheless, the sending of $m_1$ is causally
unrelated to every event but its own delivery.

Returning to the previous example, Figure \ref{fig:message-latencies-c} and
\ref{fig:message-latencies-scalar-c}, because $m_1^\textrm{send}$
causally precedes $m_2^\textrm{send}$, we find
\[C(m_1^\textrm{send}) = 1 < 5 = C(m_2^\textrm{send}).\] Using this
fact $P_3$ can learn (by examining piggybacked timestamps) that
$P_2$'s response could not causally precede $P_1$'s question,
partially resolving the ambiguity posed earlier. However, the scalar
clock regimen does not given $P_3$ enough information to conclusively
affirm that the question causally precedes the answer: the opposite is
ruled out, but it is possible the events are causally independent.

Scalar clocks achieve the clock consistency condition without
synchronized physical clocks. They are often used when an application
needs to establish a shared global ordering among all events in the
system, such as in state machine replication (SMR) protocols. Because
timestamps are not globally unique, they do not carry quite enough
information for recipients to compute a shared total order by
themselves. A simple way to obtain a total order is to use an
arbitrary tie-breaking mechanism for deciding how to order two
(necessarily causally unrelated) events that happen to share a common
scalar clock timestamp, typically by comparing the (necessarily
distinct) identifiers of the processes where they took place. Indeed,
this application was the motivating example when scalar clocks were
introduced by Lamport.

However, like physical timestamps from synchronized clocks, scalar
clocks are insufficient for applications that need to perform detailed
causality tracking that relies on the stronger clock consistency
condition.

\subsubsection{Vector clocks}
\newcommand{\vt}{\textrm{vt}}

Vector clocks require each process to store one scalar value for each
process in the system. That is, for a set of $N$ processes, $P_i$
maintains a vector $\vt_i[1 \ldots N]$ of non-negative integers, with
all values initialized to $\vt_i[x] = 0$. As with scalar clocks there
are two update rules:
\begin{enumerate}
\item Before a new message is sent, $\vt_i$ is updated according to the rule
  \[\vt_i[i] := \vt_i[i] + 1.\]
  The entire updated vector $\vt_i$ is piggybacked as part of the
  message's metadata so the receiver can use it.
\item When a message is received with a piggybacked timestamp $\vt$,
  $\vt_i$ is updated according to
  \[\vt_i[x] := \max(\vt[x], \vt_i[x]) \quad \textrm{for all $x = 1\ldots N$}.\]
  After this, $P_i$ advances its own local time according to the rule
  \[ \vt_i[i] := \vt_i[i] + 1.\]
  This new vector is the timestamp attached to the receive event.
\end{enumerate}

According to these rules, the $i^\textrm{th}$ component $\vt_i[i]$ of
$P_i$'s vector clock acts somewhat like a scalar clock: we call it the
\emph{local time} for $P_i$. A slight difference between the local
time and a scalar clock is that after receiving a message, $P_i$'s
local time is not necessarily greater than the sender's local time
that was piggybacked with the message. (What matters is that $P_i$'s
overall vector time is greater.) For all other $j \neq i$, the
$j^\textrm{th}$ component $\vt_i[j]$ of $P_i$'s clock represents
$P_i$'s \emph{estimate} of $P_j$'s local time. This estimate is always
a lower bound, since $P_j$'s actual local time may have advanced in
the meantime.

Figure \ref{fig:message-latencies-vector} redepicts the diagrams in
Figure \ref{fig:message-latencies} showing the vector timestamp that
would be assigned to each event.

\begin{figure}[p]
  \setlength\belowcaptionskip{5ex}

  \begin{subfigure}{1\textwidth}
    \centering
    \input{images/pgf/mpEx1Vec.pgf}
    \label{fig:message-latencies-vector-a}
  \end{subfigure}

  \vspace{4ex}

  \begin{subfigure}{1\textwidth}
    \centering \input{images/pgf/mpEx2Vec.pgf}
    \label{fig:message-latencies-vector-b}
  \end{subfigure}

  \begin{subfigure}{1\textwidth}
    \centering \input{images/pgf/mpEx3Vec.pgf}
    \label{fig:message-latencies-vector-c}
  \end{subfigure}

  \caption{Vector clock examples}
  \label{fig:message-latencies-vector}
\end{figure}

\afterpage{\clearpage}

Vector clocks are compared component-wise. Crucially, this only gives a partial
order among timestamps, as one vector may be greater than another in
some components and less in others.
\begin{definition}[Vector comparison]
  Let $v, w$ be two vector clocks. We define the following relations:
  \begin{align*}
             v = w &\iff \forall i, v[i] = w[i] \\
  v \preccurlyeq w &\iff \forall i, v[i] \leq w[i] \\
         v \prec w &\iff v \preccurlyeq w \textrm{ and } \exists i, v[i] < w[i] \\
            v || w &\iff \textrm{ neither } v \preccurlyeq w \textrm{ nor } w \preccurlyeq v
  \end{align*}
  That is, $v \prec w$ if all of $w$'s components are at least as
  great as $v$'s, and at least one of its components is strictly
  greater. When two non-equal vector timestamps are compared, and
  neither is greater than the other, we write $v || w$ and say the
  events are \emph{logically concurrent}.
\end{definition}

The reason to consider vector clocks over scalar clocks is precisely
that they satisfy \ref{eq:sc}, so they can be used to unambiguously
decide whether two timestamped events had causal influence over the
other.
\begin{lemma}
  Vector clocks satisfy the strong clock consistency condition. That
  is, where $C(e)$ is the vector timestamp of an event, then
  \[ e \to e' \iff C(e) \prec C(e'). \]
\end{lemma}

For reasons of space we omit a proof of the precending lemma.  The
direction $e \to e' \implies C(e) \prec C(e')$ is
straightforward. More tedious is the right-to-left direction
$C(e) \prec C(e') \implies e \to e'$, but the key idea is that if
$C(e) \prec C(e')$, then it is possible to follow a backwards chain of
send and receive events connecting $e$ to $e'$, hence $e \to e'$.

% \begin{proof}
%   The direction $e \to e' \implies C(e) \prec C(e')$ is
%   straightforward. Slightly more tedious is the right-to-left
%   direction $C(e) \prec C(e') \implies e \to e'$.

%   Assume two events satisfy $C(e) \prec C(e')$. If the events occur on
%   the same process $P_i$, then $C(e')$ must have a strictly greater
%   $i^\textrm{th}$ component, i.e. $C(e)[i] \prec C(e')[i]$, in which
%   case $e <_{P_i} e'$ and hence $e \to e'$.

%   The last case to consider is that $C(e) \prec C(e')$ with $e$ on
%   process $P_i$ and $e'$ on process $P_j$ with $j \neq i$. If
%   $e \to e'$ then we are done, so suppose $e \not \to e'$.
%   Without loss of generality, let $e'$ be the first event (in physical
%   time) satisfying both
%   \[ C(e) \prec C(e') \textrm{ and } e \not \to e'.\] We will derive a
%   contradiction. If $e$ is a message receive event, let $x$ be the
%   corresponding send event. $x$ cannot satisfy


%   . Let $e_p$ be the event immediately
%   preceding $e'$ on process $j$. By assumption,
%   $C(e) \not \prec C(e_p)$, as otherwise our we contradict our
%   assumption that $e_p$ is the first event satisfying both
%   $C(e) \prec C(e_p)$ and $e \not \to e_p$. However, $C(e_p)$ and
%   $C(e')$ can only differ in their $j^\textrm{th}$ component, as

%   .  If $e'$ is a message \emph{send} event, then


%   We can prove this
%   direction by its contrapositive:
%   \[ e \not\to e' \implies C(e) \not\prec C(e'). \]

%   Assume $e \not\to e'$ where event $e'$ occurs on process $P_i$.  If
%   $e'$ also occurs on process $i$, then because events on $P_i$ are
%   totally ordered with respect to $P_i$'s local time, it must be that
%   $e' <_{P_i} e$, i.e. $e'$ happened first. In this case,
%   $C(e')[i] < C(e)[i]$, hence $C(e) \not \prec C(e')$.

%   Suppose $e'$ occurs on $P_j$ where $j \neq i$. Without loss of
%   generality, consider the \emph{first} event $e'$ (in physical time)
%   with the property $e \not\to e'$. If $e'$ is a message send event,

%   . So let $e'$ occur on some other process.
%   \textbf{TODO}
% \end{proof}

\subsubsection{Matrix clocks}

Matrix clocks are to vector clocks what vector clocks are to scalar
clocks. That is, a matrix clock is a vector clock combined with a
lower bound estimate of every other process's vector clock. The
justification for matrix clocks is somewhat subtle, but we shall see a
clear example in Section \ref{sec:continuous-consistency} of why
tracking this extra information can prove useful.

Matrix clocks subsume vector clocks, so they can be used whenever the
strong consistency condition is needed. The reader may then be left
wondering what extra utility is provided. It is worth highlighting the
fact that matrix clocks are used to keep track of (a lower bound
estimate of) what other processes know about the global state of the
system. In Section \ref{sec:background}, we explained that distributed
systems fundamentally involve epistemic logic, i.e. reasoning about
not just what is true but what is known. If logical clocks are roughly
thought of as a sense of ``time,'' the matrix clocks are useful when
processes need to know what time other processes think it is. More
concretely, when (CONDITION), the process knows (STATE). In Section
\ref{sec:continuous-consistency} we will see an example where this
sort of lower bound estimate of other process's clocks is important
for distributed database replication.


\begin{figure}[p]
  \setlength\belowcaptionskip{5ex}
  \renewcommand*{\arraystretch}{0.9}

  \begin{subfigure}{1\textwidth}
    \centering
    \input{images/pgf/mpEx1Mat.pgf}
    \label{fig:message-latencies-matrix-a}
  \end{subfigure}

  \vspace{4ex}

  \begin{subfigure}{1\textwidth}
    \input{images/pgf/mpEx2Mat.pgf}
    \label{fig:message-latencies-matrix-b}
  \end{subfigure}

  \begin{subfigure}{1\textwidth}
    \input{images/pgf/mpEx3Mat.pgf}
    \label{fig:message-latencies-matrix-c}
  \end{subfigure}

  \caption{Matrix clock examples}
  \label{fig:message-latencies-matrix}
\end{figure}

\afterpage{\clearpage}



\newpage
\subsection{Shared Memory}
Programming distributed applications in terms of sending and receiving
messages over the network is a complex task. As we have seen, it
raises issues of synchronization and consistency that must be dealt
with in order to provide understandable system behavior. In this
section we summarize a simpler but in some sense equivalent paradigm
called the \emph{distributed shared memory} (DSM) abstraction.

In the DSM model, programmers think in terms of reading and writing
values to memory locations (one might also say \emph{variables})
rather than directly transmitting messages over the network. The key
feature is that all instances of the application act as if they are
reading and writing from the same memory locations, when in reality
they are running on distributed computers that do not share
memory. Transparently and behind the scenes, a ``middleware'' memory
management protocol layer translates the read/write requests into
send/receive requests that facilitate the abstraction of a common
virtual memory space. The programmer does not have to know how this is
achieved exactly, but it is critical for them to know what guarantees
the memory management layer provides. As is typically the case,
systems can differ in this respect because they make different
tradeoffs between safety guarantees and performance.

\begin{figure}
    \centering
    \input{images/pgf/smEx1NoEdges.pgf}
    \caption{Time diagram for memory operations}
    \label{fig:smEx1}
\end{figure}

\subsubsection{Shared memory time diagrams}

Figure \ref{fig:smEx1} depicts a time diagram for the shared memory
abstraction, not unlike the diagrams in Figure
\ref{fig:message-latencies} for message passing. Two kinds of
operations are shown:
\begin{description}
\item[Reads] A \emph{read} request $R(x)$ indicates reading the value
  in memory at location $x$, which returns some value $v$. When it is
  important to indicate the return value, we notate read requests as
  $R(x, v)$.
\item[Writes] A \emph{write} operation $W(x, v)$ indicates writing
  value $v$ to memory location $x$. In pseudocode the notation might
  be $x := v$ or $x \leftarrow v$.
\end{description}

Each request consists of a horizontal span along the worldline of a
process. The span represents the duration when the request is
executing, beginning when the request is submitted by the application
and ending when the operation has finished. From the programmer's
perspective, if the read/write primitives are blocking operations, the
execution is when the memory management layer is running and the main
application is blocked.

Transparently and in the background, the memory management layer
coordinates with other processes over the network. While executing a
read request $R(x)$ it may, for example, send messages to another
server to lookup the current value of memory location $x$. Such
behind-the-scenes coordination is hidden from the programmer and not
reflected in the time diagrams.

\subsubsection{Semantics and consistency}

For a non-distributed application, i.e. one running on a single
computer, it is clear how read and write requests should be
interpreted. Mainly, a read request $R(x)$ should return the most
recent value $v$ that was written to $x$ by a write $W(x, v)$, or
resort to some default behavior if no such write exists. This
interpretation is unambiguous because we assume that operations
running on a single process do not overlap in time, so it always makes
sense to ask which of two events happened before the other.
\begin{figure}
  \input{images/pgf/smEx0.pgf}
  \caption{Memory operations on a single process}
  \label{fig:smEx0}
\end{figure}

\begin{example}
  \label{exmpl:updatesoneprocess}
  Consider Figure \ref{fig:smEx0}, depicting just a single process. Since there is
  no ambiguity in the order of events, it is clear that this process
  should execute in the following order (read requests have been underlined with their results shown):
  \[ W(x, 0) \to W(y, 5) \to \underline{R(x, 0)} \to W(x, 3) \to \underline{R(y, 5)} \to \underline{R(x, 3)}. \]
\end{example}

In the distributed context, the metaphorical wrench in the works is
that operations by different processes can and frequently will
overlap, so by default there is no total order that can be used to
compare events. Without such a comparison, the notion of ``most
recent'' operation is ambiguous, which makes it difficult to say
exactly how the memory requests should even be interpreted.

\begin{figure}
  \input{images/pgf/smEx2.pgf}
  \caption{Concurrent updates by two processes}
  \label{fig:smEx2}
\end{figure}

\begin{example}
  \label{exmpl:concurrentupdates}
  Consider Figure \ref{fig:smEx2}. Two writes are depicted that overlap
  in physical time, making it unclear whether $W(x,3)$ or $W(x,5)$
  should be considered as happening first. Thus, it is unclear whether
  the read request at process 1 will return 3 or 5.
\end{example}

In example \ref{exmpl:concurrentupdates}, it seems intuitively clear
that the programmer should expect each read request to return either 3
or 5, and not, say, 37. What is less clear is whether the first
request requests on each process, which run concurrently, must be
constrained to return the same value. Now consider the second read
request $R(x)$ running on process $2$. Can the programmer be confident
that this request will return the same value as the one immediately
preceding it?

It is possible to consider different ways of answering these
questions. The role of a \emph{memory model} is to constrain exactly
which possibilities are allowed and which are prohibited, namely which
values can be read under different conditions. A single program can
behave differently depending on what memory model is offered by the
shared memory layer.

\begin{comment}
A \emph{memory consistency model} formally constrains the allowable
system responses during executions. ``Strong'' models are generally
understood as ones providing the illusion that all clients are
accessing a single global database. As we will see, this still leaves
room for different possible behaviors (i.e.~allows non-determinism in
the execution of a distributed application), but the allowable
behavior is tightly constrained.  Weaker models tolerate deviation
from this standard. For a given application, the most appropriate
model depends on the semantics expected by the application and its
clients, which must be weighed against other requirements. Strongly
consistent systems are easier to understand and reason about and so,
all other things being equal, one wants to have as much consistency as
possible. However, we shall see that strong notions of consistency are
brittle in the sense that they generally cannot be achieved for the
kinds of systems we consider in this document.
\end{comment}

\subsubsection{Concurrency and External order}
\label{sssec:externalorder}

As before, it is tempting to imagine assigning a physical timestamp to
reads and writes to establish an order among events, resolving the
ambiguities above. However, we have seen that physical clocks are
usually not so precisely sychronized as to allow meaningfully
comparing timestamps from different nodes, so this is not a total
solution.

One fundamental relation in the DSM setting is \emph{external
order}. Intuitively, it is the partial order that orders
non-overlapping events by their physical times, but does not assign an
ordering to events whose executions overlap in physical time.

\begin{comment}
\begin{definition}
  Recall that an irreflexive partial order is a binary relation \(<\)
  such that \(A \not < A\), \(A < B \implies B \not < A\), and \(A <
  B, B < C \implies A < C\), where $x \not < y$ means it is not the
  case that $x < y$.
\end{definition}
\end{comment}

\begin{definition}
  Let $E$ be an execution. Request $E1$ \emph{externally precedes}
  request $E2$ if $E1.t < E2.s$. That is, if the first request
  terminates before the second request is accepted. This induces an
  irreflexive partial order called \emph{external order}.
\end{definition}

We assume processes handle operations one-at-a-time, so the events
handled at any one process are totally ordered by external order---one
event cannot start before another has finished. However, events by
different process may overlap.

For performance reasons, memory models commonly allow distributed
operations to have semantics that do not strictly conform to external
order. For instance, below we show how sequential consistency
(Definition \ref{def:sequentiallyconsistent}), one of two ``strong''
models, allows for this possibility. Yet even this model imposes too
many constraints to be workable for networking environments as
unpredictable as ours.

\begin{figure}[h]
    \centering
    \input{images/pgf/smEx1DAG.pgf}
    \caption{External order relation among operations in Figure \ref{fig:smEx1}}
    \label{fig:smEx1DAG}
  \end{figure}

\begin{example}
  Figure \ref{fig:smEx1DAG} shows a time diagram involving three
  processes depicts the external order relation among the operations
  shown in Figure \ref{fig:smEx1} in the form of a directed acyclic
  graph (DAG).
\end{example}


If \(E1\) and \(E2\) are events at different processes that are not be
comparable by external order, i.e.~neither \(E1.t < E2.s\) nor
\(E2.t < E1.s\), we say they are \emph{physically concurrent}.

\begin{definition}
  If two events overlap in physical time (equivalently, if they are
  not comparable by external order), we call the events
  \emph{physically concurrent} and write $E1 || E2$.
\end{definition}

Physical concurrency is a reflexive and symmetric---but usually not
transitive--- binary relation. Such structures are generally known as
\emph{compatibility relations}, the idea being that if \(A\) and
\(B\) are both compatible with \(C\), it need not be the case that
\(A\) and \(B\) are compatible with each other. The reader should come
up with an example of two operations that are physically concurrent
with a third operation but not with each other.

\subsection{Memory Consistency Models}

Since the goal of the DSM abstraction is to present the illusion of a
reading and writing from a common memory, the observed result should
be equivalent to some interleaving of the requests into a linear
order. If each processes $P_i$ of $N$ total processes issues $r_i$
requests for some non-negative integer $r_i$, observe there are a
total of
\[
\frac{\left(\sum_{i = 1}^N r_i\right)!}{\prod_{i = 1}^N r_i!}
\]
such interleavings. The question we must consider is which of these
are allowed.

\subsubsection{Linearizability}
\label{sssec:linearizability}

\emph{Linearizability}, the strongest consistency model, can be
concisely defined as providing the appearance that ``each operation
applied by concurrent processes takes effect instantaneously at some
point between its invocation and response.'' \citationneeded The same
condition is known variously as atomic consistency, strict
consistency, and sometimes external consistency. In the context of
database transactions (which come with other database-specific
guarantees like isolation), the analogous condition is called strict
serializability.

A linearizable execution is defined by three features.

\begin{definition}[Linearizable execution]
  A \emph{linearizable execution} is one satisfying the following three conditions.
\begin{enumerate}
  \tightlist
\item
  All processes act like they agree on a single, global total order
  defined across all accesses.
\item
  This sequential order is consistent with the actual external order.
\item
  Responses are semantically correct, meaning a read request \(R(x, a)\)
  returns the value of the most recent write request \(W(x, a)\) to
  \(x\).
\end{enumerate}
\end{definition}

An intuitive way of approaching linearizability is by defining it
in terms of \emph{linearization points.}

\begin{definition}
  A \emph{linearization point} $t \in \mathbb{R} \in [E.s, E.t]$ for an
  event $E$ is a time between the event's start and termination. An
  execution is \emph{linearizable} if and only if there is a choice of
  linearization point for each access, which induces a total order called a \emph{linearization},
  such that $E$ is equivalent to
  the serial execution of events when totally ordered by their
  linearization points.
\end{definition}

\begin{example}
  \label{exmpl:linearizations}
  Two different linearizations of Figure \ref{fig:smEx1} are shown in
  Figure \ref{fig:smEx3}. Possible choices for linearization points
  are shown in yellow. No other linearizations are possible.
\end{example}

Example \ref{exmpl:linearizations} demonstrates that linearizability
leaves some room for non-determinism in the execution of distributed
applications. In the example, we see that the read requests in Figure
\ref{fig:smEx1} may return \emph{either} 3 or 5. However,
linearizability constrains all three reads to return the same value,
depending on which of $W(x,3)$ or $W(x,5)$ happens to take effect
first.

\begin{figure}[p]
  \setlength\belowcaptionskip{5ex}

  \begin{subfigure}{1\textwidth}
    \centering
    \input{images/pgf/smEx3L1.pgf}
    \caption{One of two possible linearizations}
    \label{fig:smEx1L1}
  \end{subfigure}

  \begin{subfigure}{1\textwidth}
    \input{images/pgf/smEx3L2.pgf}
    \caption{The other possible linearization}
    \label{fig:smEx3L2}
  \end{subfigure}

  \caption{The two possible linearizations of Figure \ref{fig:smEx2}. Possible choices of linearization points are shown in yellow.}
  \label{fig:smEx3}
\end{figure}
\afterpage{\clearpage}

\begin{comment}
Figure \ref{fig:linear_example11} shows a prototypical example of a
linearizable execution. We assume that all memory locations are
initialized to \(0\) at the system start time. Intuitively, it should
appear to an external observer that each access instantaneously took
effect at some point between its start and end time. Hence, the request
to read the value of \(y\) returns \(1\), because at some point between
\(W(y,1).s\) and \(W(y,1).t\) that change took effect. If client on
\(P_1\) read a stale value, we would say the execution is not
linearizable. Figure \ref{fig:linear_example12} shows an
non-linearizable execution that returns stale data instead of reflecting
the write access to \(y\) on \(P2\).
\end{comment}

Linearizability is a condition on individual executions of a
distributed application. When only linearizable executions are
permitted by the memory manager, the entire system is called
linearizable.

\begin{definition}[Linearizable system]
  A shared-memory application is \emph{linearizable} if all possible
  executions of that system are linearizable executions.
\end{definition}

\subsubsection{Sequential consistency}
\label{sequential-consistency}

Enforcing atomic consistency means that an access \(E\) at process
\(P_i\) cannot return to the client until every other process has been
informed about \(E\). For many applications this is an unacceptably
high penalty. A weaker model that is still strong enough for most
purposes is \emph{sequential} consistency. This is an appropriate
model if a form of strong consistency is required, but the system is
agnostic about the precise physical time at which events start and
finish, provided they occur in a globally agreed upon order.

A sequentially consistent system ensures that any execution is
equivalent to some global serial execution, even if this is serial order
is not the one suggested by the real-time ordering of events. When
real-time constraints are not important, this provides essentially the
same benefits as linearizability. For example, it allows programmers to
reason about concurrent executions of programs because the result is
always guaranteed to represent some possible interleaving of
instructions, never allowing instructions from one program to execute
out of order.

\begin{definition}
  \label{def:sequentiallyconsistent}
  A \emph{sequentially consistent} execution is
  characterized by three features:
  \begin{itemize}
  \item All processes act like they agree on a single, global total order
    defined across all accesses.
  \item This sequential order is consistent with the program order of each process.
  \item Responses are semantically correct, meaning reads return the most recent writes (as determined by the global order)
  \end{itemize}
\end{definition}

\begin{figure}[p]
  \setlength\belowcaptionskip{5ex}

  \begin{subfigure}{1\textwidth}
    \centering
    \input{images/pgf/smEx4S1.pgf}
    \caption{An example where the processes read different values}
    \label{fig:smEx1L1}
  \end{subfigure}

  \begin{subfigure}{1\textwidth}
    \input{images/pgf/smEx4S2.pgf}
    \caption{An example where the first write operation affects a read}
    \label{fig:smEx4S2}
  \end{subfigure}

  \caption{Sequentially consistent, nonlinearizable executions of Figure \ref{fig:smEx2}}
  \label{fig:smEx3}
\end{figure}

\begin{figure}[p]
  \setlength\belowcaptionskip{5ex}

  \begin{subfigure}{1\textwidth}
    \centering
    \input{images/pgf/smEx4S3.pgf}
    \caption{}
    \label{fig:smEx1L1}
  \end{subfigure}

  \begin{subfigure}{1\textwidth}
    \input{images/pgf/smEx4S4.pgf}
    \caption{}
    \label{fig:smEx4S2}
  \end{subfigure}

  \caption{Two non-sequentially-consistent executions of Figure \ref{fig:smEx2}}
  \label{fig:smEx3}
\end{figure}

\afterpage{\clearpage}

Processes in a sequentially consistent system are required to agree on
a total order of events, presenting the illusion of a shared database
from an application programmer's point of view. However, this order
need not be given by external order. Instead, the only requirement is
that sequential history must agree with process order, i.e.~the events
from each process must occur in the same order as in they do in the
process.  This is nearly the definition of linearizability, except
that external order has been replaced with merely program order. We
immediately get the following lemma.

\begin{lemma}
  \label{lem:linearsequential}
  A linearizable execution is sequentially consistent.
\end{lemma}
\begin{proof}
  This follows because process order is a subset of external order.
\end{proof}

Visually, sequential consistency allows reordering an execution by
sliding events along each process' time axis like beads along a
string.  Two events from the same process cannot pass over each other
(as this would precisely be a violation of program order), but events
on different processes may be commuted past each other, contravening
external order. This sliding allow noe to construct a fairly arbitrary
interleaving of events, a totally ordered execution with no events
overlapping. From this perspective, while linearizability requires the
existence of a linearization, sequential consistency requires the
existence of an interleaving.

It may seem strange to consider executions such as the one shown in
REF in which operations appear to take effect at different times for
different processes, or at times that do not agree with external
order. The reader should remember that in the background, these
processes would be engaged in message-passing over the network and are
therefore subject to all the complexities previous discussed in
Section \ref{sec:message-passing}, including delayed and out-of-order
messages. Looser requirements by the memory model impose fewer
constraints on the message passing layer requiring less coordination,
and allowing for greater performance.

The converse of Lemma \ref{lem:linearsequential} does not hold. For
example, Figures \ref{fig:smEx4S1} and \ref{fig:smEx4S2} are
nonlinearizable executions of Figure \ref{fig:smEx2} that are
nonetheless sequentially consistency.

\subsubsection{Causal consistency}
\label{causal-consistency}

\emph{Causal} consistency\citationneeded is a weaker memory model than
sequential consistency. Whereas sequential consistency requires the
system as a whole to behave as if all write operations take place in
some total order (as long as this is consistent with program order),
causal consistency allows different processes to see write operations
take effect in different orders. Instead, only write operations that
are \emph{causally related} need to take effect in a common order
at all processes.

We have not defined what it means for memory operations to be causally
related. The notion is very similar to causal precedence in the
context of message passing (Definition \ref{def:causalprecedence}),
with a small wrinkle: in a message passing context, each receive event
is uniquely paired with a send event. However, because multiple
operations can write the same value into the same location, there is
generally not a unique way to associate each read operation with the
write operation that ``caused'' it to read a particular value.

\begin{definition}[Writes-into order]
  Consider a set of memory operations. A ``writes into''
  $\rightsquigarrow$ order is any binary relation that satisfies the
  following conditions:
  \begin{itemize}
  \item If $o \rightsquigarrow o'$, then $o = W(x, v)$ and $o' = R(x,v)$ for memory location $x$ and value $v$.
  \item For each $o'$, there is exactly one $o$ such that $o \rightsquigarrow o'$
\end{definition}

This definition is a slight simplication of that found
in\citationneeded: To avoid talking having to talk about default
values, we only consider cases here where all read operations are
preceded by some write operation. Intuitively, the idea is that a
writes-into order pairs reads with the write operations whose values
they are ``reading from.'' If all values written into each location
are unique, the writes-into order is unambiguous, as an operation that
reads $v$ from location $x$ could only be paired with the operation
that writes $v$ into $x$.

\begin{definition}[Causal order on memory operations]
  \label{def:memorycausalprecedence}
  For some choice of a writes-into order, we define a binary relation
  $\to$ on the set of memory operations as follows:
  \[o \to o' \iff
  \begin{cases}
    o <_{P_i} o' \textrm{for some process $P_i$}
    \textbf{ or} \\
    o \rightsquigarrow o'
    \textbf{ or} \\
    \textrm{there is some } o'' \textrm{ such that } o \to o'' \textrm{ and } o'' \to o'
  \end{cases}
  \]
  If $o \to o'$, we say $o$ causally precedes $o'$.
\end{definition}

We can now define causally consistent executions of memory operations.

\begin{definition}[Causal consistency]
  \label{def:causalconsistency}
  A history is causally consistent if each process (acts like it) sees
  write requests take place in some order that is consistent with
  causal order for some choice of writes-into order.
\end{definition}

\begin{figure}[p]
  \begin{subfigure}{1\textwidth}
    \centering
    \input{images/pgf/smEx4S3.pgf}
    \caption{}
    \label{fig:smEx1L1}
  \end{subfigure}

  \begin{subfigure}{1\textwidth}
    \input{images/pgf/smEx4S4.pgf}
    \caption{}
    \label{fig:smEx5}
  \end{subfigure}

  \caption{PLACEHOLDER: Causally consistent and inconsistent executions}
  \label{fig:smEx3}
\end{figure}

\afterpage{\clearpage}

\subsection{The CAP Theorem}
\label{the-cap-theorem}

Real-world systems often fall short of behaving as a single perfectly
coherent system. The root of this phenomenon is a deep and
well-understood tradeoff between system coherence and performance.
Enforcing consistency comes at the cost of additional communications,
and communications impose overheads, often unpredictable ones.

Fox and Brewer \cite{1999foxbrewer} are crediting with observing a
particular tension between the three competing goals of consistency,
availability, and partition-tolerance. This tradeoff was precisely
stated and proved in 2002 by Gilbert and Lynch
\cite{2002gilbertlynchCAP}.  The theorem is often somewhat
misunderstood, as we discuss, so it is worth clarifying the terms
used.

\begin{description}
  \item[Consistency] Gilbert and Lynch define a consistency system as one whose executions
are always linearizable.
\item[Availability] A CAP-available system is one that will definitely respond to every
client request at some point.
\item[Partition tolerance]
A partition-tolerant system continues to function, and ensure whatever
guarantees it is meant to provide, in the face of arbitrary partitions
in the network (i.e., an inability for some nodes to communicate with
others). It is possible that a partition never recovers, say if a
critical communications cable is permanently severed.
\end{description}

A partition-tolerant CAP-available system cannot indefinitely suspend
handling a request to wait for network activity like receiving a
message. In the event of a partition that never recovers, this would
mean the process could wait indefinitely for the partition to heal,
violating availability. On the other hand, a CAP-consistent system is
not allowed to return anything but the most up-to-date value in
response to client requests. Keep in mind that any (other) process may
be the originating replica for an update. Some reflection shows that
the full set of requirements is unattainable---a partition tolerant
system simply cannot enforce both consistency and availability. We
give only the informal sketch here, leaving the interested reader to
consult the more formal analysis by Gilbert and Lynch. The key
technical assumption is that a processes' behavior can only be
influenced by the messages it actually receives---it cannot be
affected by messages that are sent to it but never delivered.

\begin{theorem}[The CAP Theorem]
  \label{thm:cap}
  In the presense of indefinite network partitions, a distributed system
  cannot guarantee both linearizability and eventual-availability.
\end{theorem}
\begin{proof}
  Consider the concurrent writes by separate processes in Figure
  \ref{fig:smEx2}.  Figure \ref{fig:smEx3} showed that both possible
  linearizations of this history require all reads to return the same
  value, either $3$ or $5$. Now suppose the network is unavailable, so
  the processes cannot communicate, in particular to share write
  updates.

  Now there are only two possibilities. First, the processes could
  proceed despite the lack of communication. In this case, because
  processes do not otherwise affect each other, $P_1$ could not see
  the $W(x,5)$ operation and its read must return the value
  $3$. Likewise, $P_2$ does not see $W(x,3)$ and its reads return
  $5$. This situation violates linearizability, hence the 'C' of CAP.

  Alternatively, the processes might detect that the network is
  unavailable and refuse to respond to read requests. However, this
  indefinitely suspects progress until the network recovers, which
  violates the 'A' of CAP.
\end{proof}

In the previous proof, the last remaining option is to assume that the
network never suffers from partitions. That is, to sacrifice the 'P'
of CAP. Clearly this is unacceptable in the systems we are
considering. Consequently, our systems cannot ensure atomic
consistency and availability.

Recall Figure \ref{fig:smEx1L1}. This example demonstrated that
sequential consistency allows the history in Figure \ref{fig:smEx2} to
return non-equal values for the two read operations. That is, if $P_1$
reads $3$, and both reads on $P_2$ return $5$, the execution is
sequentially consistent. This raises the possibility that sequential
consistency is not subject to the limits of the CAP
theorem. Unfortunately the hope is fleeting: like linearizability,
sequential consistency is CAP-unavailabile.

\begin{lemma}[CAP for sequential consistency]
  \label{thm:cap-sequential}
  An eventually-available system cannot provide sequential consistency in the presense of network partitions.
\end{lemma}
\begin{proof}
  The proof is an adaptation of Theorem \ref{thm:cap}. Suppose $P_1$ and
  $P_2$ form of CAP-available distributed system and consider the
  following execution: $P_1$ reads $x$, then assigns $y$ the value
  $1$. $P_2$ reads $y$, then assigns $x$ the value $1$. (Note that this
  is the sequence of requests shown in Figure \ref{fig:nonsequential1},
  but we make no assumptions about the values returned by the read
  requests). By availability, we know the requests will be handled (with
  responses sent back to clients) after a finite amount of time. Now
  suppose $P_1$ and $P_2$ are separated by a partition so they cannot
  read each other's writes during this process. For contradiction,
  suppose the execution is equivalent to a sequential order.

  If $W(y,1)$ precedes $R(y)$ in the sequential order, then $R(y)$ would
  be constrained to return to $1$. But $P_2$ cannot pass information to
  $P_1$, so this is ruled out. To avoid this situation, suppose the
  sequential order places $R(y)$ before $W(y,1)$, in which case $R(y)$
  could correctly return the initial value of $0$. However, by
  transitivity the $R(x)$ event would occur after $W(x,1)$ event, so it
  would have to return $1$. But there is no way to pass this information
  from $P_1$ to $P_2$. Thus, any attempt to consistently order the
  requests would require commuting $W(y,1)$ with $R(x)$ or $W(x,1)$ with
  $R(y)$, which would violate program order.
\end{proof}

\begin{lemma}[Causal consistency is CAP-available]
  \label{thm:cap-causal}
  Causal consistency can be enforced during network partitions. That
  is, causal consistency is not subject to the CAP thereom.
\end{lemma}
\begin{proof}
  Consider processes that execute read and write operations purely
  locally. That is, they never send messages to other processes and
  they only read to their own writes. In this situation, the causal
  relation generated by Definition \ref{def:memorycausalprecedence} is
  essentially unique: operations at $P_i$ are causally preceded by the
  previous operations at $P_i$, and no others. In the absence of a
  non-trivial causal order, causal consistency imposes no constraints
  and the execution is vacuously consistent.
\end{proof}

Unfortunately, the proof of the prior lemma has more to do the
weakness of the promises made causal consistency. The example allows
different processes to deviate arbitrary far from consistency, in the
sense that no processes need to agree on any of the updates applied to
the database. The effect is that causal consistency is too weak to
apply any kind of bound on divergence, which suggests it is not strong
enough for the kinds of safetly-related applications we have in mind.

\subsubsection{Consequences of CAP}
\label{interpretation-of-the-cap-theorem}
While the CAP theorem is conceptually simple, its interpretation is
subtle and has been the subject of much discussion since it was
introduced \cite{2012CAP12Years}. It is sometimes assumed that the CAP
theorem claims that a distributed system can only offer two of the
properties C, A, and P.  In fact, the theorem constrains, but does not
prohibit the existence of, applications that apply some relaxed amount
of all three features. The CAP theorem only rules out their
combination when all three are interpreted in a highly idealized
sense.

In practice, applications can tolerate much weaker levels of
consistency than linearizability. Furthermore, network partitions are
usually not as dramatic as an indefinite communications blackout. Real
conditions in our context are likely to be chaotic, featuring many
smaller disruptions and delays and sometimes larger
ones. Communications between different clients may be affected
differently, with nearby agents generally likely to have better
communication channels between them than agents that are far
apart. Finally, CAP-availability is a suprisingly weak condition.
Generally one cares about the actual time it takes to handle user
requests, but the CAP theorem exposes difficulties just ensuring the
system handles requests at all. Altogether, the extremes of C, A, and
P in the CAP theorem are not the appropriate conditions to apply to
many, perhaps most, real-world applications.

\subsection{Summary}
\label{sec:background-summary}

We have seen that a distributed system is built from geographically
distant components that must communicate over a network. Sending
messages over the network causes messages to suffer unpredictable
delays. Particularly in the context of broadcasts sent to multiple
members of a group at once, this varying delay often causes messages
to arrive in different orders to different members of the group, which
can lead to chaos if a message-ordering discipline is not imposed.

As part of taming this chaos, it is critical for distributed systems
to track the causal precedence relation between events. Physical
clocks cannot usually be relied upon to maintain adequate
synchronization for this purpose. Instead, logical clocks may be
employed. The different paradigms---scalar, vector, and matrix
clocks---vary in how much information they track and how much overhead
they impose on the system with respect to bandwidth and storage
space. The latter two require nodes to know about every other member
of the group, as is typical for many mechanisms in distributed
systems. If groups can change dynamically, as in our scenarios, then a
group membership protocol is also needed.

Programmers may find it easier to frame distributed applications in
terms of reading and writing from a shared pool of virtual memory
instead of sending messages over a network. However, the fact that
many nodes can access this virtual memory at the same time leads to
non-determinism in how the memory accesses are ordered, which makes it
non-trivial to decide what it means for the system to be
consistent. The two strongest notions of consistency---linearizability
and sequential consistency---essentially provide the illusion of a
single source of truth, but the CAP theorem makes it virtually
impossible to realize these consistency models in the kinds of chaotic
networks we are considering. The causal consistency model ensures a
minimum of coherence and has the advantage that it is not subject to
the limitations of the CAP theorem. However, it makes no guarantees
about how far apart replicas of the same data may diverge, which makes
this model too weak for the kinds of safety-related applications we
have in mind.

\section{Resilient Network Architectures}
\label{sec:networking}



A priori, a network may not deliver a message at all. Alternatively it
might deliver the message multiple times, for example if a device is
unaware that a message has already been delivered. In either case, the
network itself does not alert the sender or receiver to the fact---the
responsibility for detecting such conditions belongs to the sorts of
protocols considered here and in Section \ref{sec:networking}.

If we were to inspect the network blackbox, we would expect to find
so-called ``transport'' protocols like TCP\citationneeded being used
to provide the abstraction of reliable delivery over an otherwise
unreliable network. For example, they might add metadata to messages
that allow the receiver to arrange them in their intended order,
discard duplicates, and verify their integrity. Transport protocols
might also handle retranmission when messages become corrupted in
transit. We consider these kinds of low-level details in Section
\ref{sec:networking}; for now, one could say we are working at a level
of abstraction above the transport layer. Note that transport
reliability mechanisms contribute to the latency in passing messages,
so they cannot solve the problems under consideration in this section.


\subsection{Ad-hoc networking}
\label{ad-hoc-networking}

\subsubsection{Physical communications}
\label{physical-communications}

The details of the physical communication between processes is outside
the scope of this memo. We make just a few high-level observations about
the possibilities, as the details of the network layer are likely to
have an impact on distributed applications, such as the shared memory
abstraction we discuss below and in Section
\ref{sec:continuous-consistency}. For such applications, it may be
important to optimize for the sorts of usage patterns encountered in
real scenarios, which are affected by (among other things) the low-level
details of the network.

The \emph{celluar} model (Figure \ref{fig:centralized}) assumes nodes
are within range of a powerful, centralized transmission station that
performs routing functions. Message passing takes place by transmitting
to the base station (labeled \(R\)), which routes the message to its
destination. Such a model could be supported by the ad-hoc deployment of
portable cellphone towers transported into the field, for instance.

The \emph{ad-hoc} model (Figure \ref{fig:decentralized}) assumes nodes
communicate by passing messages directly to each other. This requires
nodes to maintain information about things like routing and the
approximate location of other nodes in the system, increasing complexity
and introducing a possible source of inconsistency. However, it may be
more workable given (i) the geographic mobility of agents in our
scenarios (ii) difficult-to-access locations that prohibit setting up
communication towers (iii) the inherent need for system flexibility
during disaster scenarios.

\begin{figure}[h]
  \centering
  \begin{subfigure}[b]{0.48\textwidth}
    \centering
    \includegraphics[width=\textwidth]{images/Centralized.png}
    \caption{Cellular network topology}
    \label{fig:centralized}
  \end{subfigure}
  \hfill
  \begin{subfigure}[b]{0.48\textwidth}
    \centering
    \includegraphics[width=\textwidth]{images/Decentralized.png}
    \caption{Ad-hoc network topology}
    \label{fig:decentralized}
  \end{subfigure}
  \caption{Network topology models for geodistributed agents. Edges represent communication links (bidirectional for simplicity).}
  \label{fig:nettopology}
\end{figure}

One can also imagine hybrid models, such as an ad-hoc arrangement of
localized cells. In general, one expects more centralized topologies to
be simpler for application developers to reason about, but to require
more physical infrastructure and support. On the other hand, the ad-hoc
model is more fault resistant, but more complicated to implement and
potentially offering fewer assurancess about performance. In either
case, higher-level applications such as shared memory abstractions
should be tuned for the networking environment. It would be even better
if this tuning can take place dynamically, with applications
reconfiguring manually or automatically to the particulars of the
operating environment. This requires examining the relationship between
the application and networking layers, rather than treating them as
separate blackboxes.

\hypertarget{delay-tolerant-networking}{%
  \subsection{Delay-tolerant networking}\label{delay-tolerant-networking}}

\hypertarget{ad-hoc-dtns}{%
  \subsection{Ad-hoc DTNs}\label{ad-hoc-dtns}}

An interesting possibility is for the \emph{network} to automatically
configure itself to the quality-of-service needs of the application. For
example, a client that receives a lot of requests may be marked as a
preferred client and given higher-priority access to the network. If UAV
vehicles can be used to route messages by acting as mobile transmission
base stations, one can imagine selecting a flight pattern based on
networking needs. For example, if the communication between two
firefighting teams is obstructed by a geographical feature, a UAV could
be dispatched to provide overhead communication support. Such an
arrangement could greatly blur the line between the networking and
application layers.

\hypertarget{software-defined-networking}{%
  \subsection{Software-defined
    networking}\label{software-defined-networking}}

\hypertarget{verification-of-networking-protocols}{%
  \subsection{Verification of networking
    protocols}\label{verification-of-networking-protocols}}

\newpage

\hypertarget{continuous-consistency}{%
  \section{Continuous Consistency}\label{continuous-consistency}}

\label{sec:continuous-consistency}


%We shall consider the DSM abstraction and some of the different
%consistency requirements we can enforce in this setting.

%A fundamental distributed application is the \emph{shared distributed
%memory} abstraction. We shall assume that all processes maintain a
%local replica of a globally shared data object, as replication
%increases system fault tolerance. For simplicitly, we shall discuss
%the data store as a simple key-value store, but it could be something
%else like a database, filesystem, persistent object, etc.



Strong consistency is a discrete proposition: an application provides
strong consistency or it does not. For many real-world applications, it
evidently makes sense to work with data that is consistent up to some
\(\epsilon \in \mathbb{R}^{\geq 0}\). Thus, we shift from thinking about
consistency as an all-or-nothing condition, towards consistency as a
bound on inconsistency.

The definition of \(\epsilon\) evidently requires a more or less
application-specific notion of divergence between replicas of a shared
data object. Take, say, an application for disseminating the most
up-to-date visualization of the location of a fire front. It may be
acceptable if this information appears 5 minutes out of date to a
client, but unacceptable if it is 30 minutes out of date. That is, we
could measure consistency with respect to \emph{time}. One should expect
the exact tolerance for \(\epsilon\) will be depend very much on the
client, among other things. For example, firefighters who are very close
to a fire have a lower tolerance for stale information than a central
client keeping only a birds-eye view of several fire fronts
simultaneously.

Now suppose many disaster-response agencies coordinate with to update
and propagate information about the availability of resources. A client
may want to lookup the number of vehicles of a certain type that are
available to be dispatched within a certain geographic range. We may
stipulate that the value read by a client should always be \(4\) of the
actual number, i.e.~we could measure inconsistency with respect to some
numerical value.

In the last example, the reader may wonder we should tolerate a client
to read a value that is incorrect by 4, when clearly it is better to be
incorrect by 0. Intuitively, the practical benefit of tolerating weaker
values is to tolerate a greater level of imperfection in network
communications. For example, suppose Alice and Bob are individually
authorized to dispatch vehicles from a shared pool. In the event that
they cannot share a message.

Or, would could ask that the the value is a conservative estimate,
possibly lower but not higher than the actual amount. In these examples,
we measure inconsistency in terms of a numerical value.

As a third example,

By varying \(\epsilon\), one can imagine consistency as a continuous
spectrum. In light of the CAP theorem, we should likewise expect that
applications with weaker consistency requirements (high \(\epsilon\))
should provide higher availability, all other things being equal.

Yu and Vahdat explored the CAP tradeoff from this perspective in a
series of papers \cite{2000tact,2000tactalgorithms,10.5555/1251229.1251250,DBLP:conf/icdcs/YuV01,2002tact}
propose a theory of \emph{conits}, a logical unit of data subject to
their three metrics for measuring consistency. By controlling the
threshold of acceptable inconsistency of each conit as a continuous
quantity, applications can exercise precise control the tradeoff between
consistency and performance, trading one for the other in a gradual
fashion.

They built a prototype toolkit called TACT, which allows applications to
specify precisely their desired levels of consistency for each conit. An
interesting aspect of this work is that consistency can be tuned
\emph{dynamically}. This is desirable because one does not know a priori
how much consistency or availability is acceptable.

The biggest question one must answer is the competing goals of
generality and practicality. Generality means providing a general notion
of measuring \(\epsilon\), while practicality means enforcing
consistency in a way that can exploit weakened consistency requirements
to offer better overall performance.

\begin{itemize}
\item
  The tradeoff of CAP is a continuous spectrum between linearizability
  and high-availability. More importantly, it can be tuned in real time.
\item
  TACT captures neither CAP-consistency (i.e.~neither atomic nor
  sequential consistency) nor CAP-availability (read and write requests
  may be delayed indefinitely if the system is unable to enforce
  consistency requirements because of network issues).
\end{itemize}

\hypertarget{causal-consistency-1}{%
  \subsection{Causal consistency}\label{causal-consistency-1}}

Causal consistency is that each clients is consistent with a total order
that contains the happened-before relation. It does not put a bound on
divergence between replicas. Violations of causal consistency can
present clients with deeply counterintuitive behavior.

\begin{itemize}
  \tightlist
\item
  In a group messaing application, Alice posts a message and Bob
  replies. On Charlie's device, Bob's reply appears before Alice's
  original message.
\item
  Alice sees a deposit for \$100 made to her bank account and, because
  of this, decides to withdraw \$50. When she refreshes the page, the
  deposit is gone and her account is overdrawn by \(50\). A little while
  later, she refreshes the page and the deposit reappears, but a penalty
  has been assessed for overdrawing her account.
\end{itemize}

In these scenarios, one agent takes an action \emph{in response to} an
event, but other processes observe these causally-related events taking
place in the opposite order. In the first example, Charlie is able to
observe a response to a message he does not see, which does not make
sense to him. In the second example, Alice's observation at one instance
causes her to take an action, but at a later point the cause for her
actions appears to have occurred after her response to it. Both of these
scenarios already violate atomic and sequential consistency because
those models enforce a system-wide total order of events. Happily, they
are also ruled out by causally consistent systems. The advantage of the
causal consistency model is that it rules out this behavior without
sacrificing system availability, as shown below.

Causal consistency enforces a global total order on events that are
\emph{causally related}. Here, causal relationships are estimated very
conservatively: two events are potentially causally if there is some way
that the outcome of one could have influenced another.

\begin{figure}
  \center
  \includegraphics[scale=0.4]{images/causal1.png}
  \caption{A causally consistent, non-sequentially-consistent execution}
\end{figure}

\begin{lemma}
  Sequential consistency implies causal consistency.
\end{lemma}
\begin{proof}
  This is immediate from the definitions. Sequential consistency
  requires all processes to observe the same total order of events,
  where this total order must respect program order. Causal consistency
  only requires processes to agree on events that are potentially
  causally related. Program order is a subset of causal order, so any
  sequential executions also respects causal order.
\end{proof}

However, causal consistency is not nearly as strong as sequential
consistency, as processes do not need to agree on the order of events
with no causal relation between them. This weakness is evident in the
fact that the CAP theorem does not rule out highly available systems
that maintain causal consistency even during network partitions.

\begin{lemma}
  A causally consistent system need not be unavailabile during partitions.
\end{lemma}
\begin{proof}

  Suppose $P_1$ and $P_2$ maintain replicas of a key-value store, as
  before, and suppose they are separated by a partition. The strategy is
  simple: each process immediately handles read requests by reading from
  its local replica, and handles write requests by applying the update
  to its local replica. It is easy to see this leads to causally
  consistent histories. Intuitively, the fact that no information flows
  between the processes also means the events of each process are not
  related by causality, so causality is not violated.  \end{proof}

Note that in this scenario, a client's requests are always routed to the
same processor. If a client's requests can be routed to any node, causal
consistency cannot be maintained without losing availability. One
sometimes says that causal consistency is ``sticky available'' because
clients must stick to the same processor during partitions.

The fact that causal consistency can be maintained during partitions
suggests it is too weak. Indeed, there are no guarantees about the
difference in values for \(x\) and \(y\) across the two replicas.

\hypertarget{tact-system-model}{%
  \subsection{TACT system model}\label{tact-system-model}}

As in Section \ref{sec:background}, we assume a distributed set of
processes collaborate to maintain local replicas of a shared data object
such as a database. Processes accept read and write requests from
clients to update items, and they communicate with each other to ensure
to ensure that all replicas remain consistent.

However, access to the data store is mediated by a middleware library,
which sits between the local copy of the replica and the client. At a
high level, TACT will allow an operation to take place if it does not
violate user-specific consistency bounds. If allowing an operation to
proceed would violate consistency constraints, the operation blocks
until TACT synchronizes with one or more other remote replicas. The
operation remains blocked until TACT ensures that executing it would not
violate consistency requirements.

\[\textrm{Consistency} = \langle \textrm{Numerical error, \textrm{Order error}, \textrm{Staleness}} \rangle.\]

Processes forward accesses to TACT, which handles commiting them to the
store. TACT may not immediately process the request---instead it may
need to coordinate with other processes to enforce consistency. When
write requests are processed (i.e.~when a response is sent to the
originating client), they are only commited in a \emph{tenative} state.
Tentative writes eventually become fully committed at some point in the
future, but when they are commited, they may be reordered. After
fullying committing, writes are in a total order known to all processes.

\begin{figure}[h]
  \center
  \includegraphics[scale=0.4]{images/TACT Logs.png}
  \caption{Snapshot of two local replicas using TACT}
  \label{fig:tact_logs}
\end{figure}

A write access \(W\) can separately quantify its \emph{numerical weight}
and \emph{order weight} on conit \(F\). Application programmers have
multiple forms of control:

Consistency is enforced by the application by setting bounds on the
consistency of read accesses. The TACT framework then enforces these
consistency levels.

\hypertarget{measuring-consistency-on-conits}{%
  \subsection{Measuring consistency on
    conits}\label{measuring-consistency-on-conits}}

\hypertarget{numerical-consistency}{%
  \paragraph{Numerical consistency}\label{numerical-consistency}}

\hypertarget{order-consistency}{%
  \paragraph{Order consistency}\label{order-consistency}}

When the number of tentative (uncommitted) writes is high, TACT executes
a write commitment algorithm. This is a \emph{pull-based} approach which
pulls information from other processes in order to advance \(P_i\)'s
vector clock, raising the watermark and hence allowing \(P_i\) to commit
some of its writes.

\hypertarget{real-time-consistency}{%
  \paragraph{Real time consistency}\label{real-time-consistency}}

\hypertarget{enforcing-inconsistency-bounds}{%
  \subsection{Enforcing inconsistency
    bounds}\label{enforcing-inconsistency-bounds}}

\hypertarget{numerical-consistency-1}{%
  \paragraph{Numerical consistency}\label{numerical-consistency-1}}

We describe split-weight AE. Yu and Vahdat also describe two other
schemes for bounding numerical error. One, compound AE, bounds absolute
error trading space for communication overhead. In their simulations,
they found minimal benefits to this tradeoff in general. It is possible
that for specific applications the savings are worth it. They also
consider a scheme, Relative NE, which bounds the relative error.

\hypertarget{order-consistency-1}{%
  \paragraph{Order consistency}\label{order-consistency-1}}

\hypertarget{real-time-consistency-1}{%
  \paragraph{Real time consistency}\label{real-time-consistency-1}}

\hypertarget{future-work}{%
  \subsection{Future work}\label{future-work}}

\hypertarget{data-fusion}{%
  \section{Data Fusion}\label{data-fusion}}

\cite{1999:lucien-datafusion}

\label{sec:data-fusion}

Strong consistency models provide the abstraction of an idealized global
truth. In the case of conits, the numerical, commit-order, and real-time
errors are measured with respect to an idealized global state of the
database. This state may not exist on any one replica, but it is the
state each replica would converge to if it were to see all remaining
unseen updates.

We consider distributed applications that receive data from many
different sources, such as from a sensor network (broadly defined). It
will often be the case that some sources of data should be expected to
agree with each other, but they may not. A typical scenario, we want to
integrate these data into a larger model of some kind. Essentially take
a poll, and attempt to synthesize a global picture that agrees as much
as possible with the data reported from the sensor network.

Here, we need a consistency model to measure how successful our attempts
are to synthesize a global image. And to tell us how much our sensors
agree. Ideally, we could use this system to diagnose disagreements
between sensors, identifying sensors that appear to be malfunctioning,
or to detect abberations that necessitate a response.

\hypertarget{fusion-centers}{%
  \subsection{Fusion centers}\label{fusion-centers}}

To be written.

\hypertarget{sheaf-theory}{%
  \subsection{Sheaf theory}\label{sheaf-theory}}

\hypertarget{introduction-to-presheaves}{%
  \subsubsection{Introduction to
    presheaves}\label{introduction-to-presheaves}}

\begin{definition}
  A \emph{partially order-indexed family of sets} is a family of sets indexed by a partially-ordered set,
  such that orders between the indices correspond to functions between the sets.
\end{definition}

We can also set \((P, \leq)\) \emph{acts on} the set
\(\{S_i\}_{i \in I}\).

\begin{definition}
  A \emph{semiautomaton} is a monoid paired with a set.
\end{definition}

This is also called a \emph{monoid action} on the set.

\begin{definition}
  A copresheaf is a *category acting on a family of sets*.
\end{definition}

\begin{definition}
  A presheaf is a *category acting covariantly on a family of sets*.
\end{definition}

\hypertarget{introduction-to-sheaves}{%
  \subsubsection{Introduction to sheaves}\label{introduction-to-sheaves}}

To be written.

\hypertarget{the-consistency-radius}{%
  \subsubsection{The consistency radius}\label{the-consistency-radius}}

To be written.

\hypertarget{conclusion}{%
  \section{Conclusion}\label{conclusion}}

\label{sec:conclusion}

To be written.

\section*{Bibliography}\label{bibliography}
\addcontentsline{toc}{section}{Bibliography}

\bibliographystyle{abbrv}
\bibliography{bibliography}
\end{document}

%%% Local Variables:
%%% mode: LaTeX
%%% TeX-master: t
%%% End:


\begin{figure}[p]
  \begin{subfigure}[a]{1\textwidth} \center
    \includegraphics[scale=0.4]{images/linear1.png} \caption{A
      linearizable execution. Any choice of linearization works here.}
    \label{fig:linear_example11} \end{subfigure}
  \begin{subfigure}[b]{1\textwidth} \center
    \includegraphics[scale=0.4]{images/nonlinear0.png} \caption{A
      non-linearizable execution. The request to read $y$ returns a
      stale value. } \label{fig:linear_example12} \end{subfigure}
  \caption{A linearizable and non-linearizable execution.}
  \label{fig:linear_example1} \end{figure}

\begin{figure}[p]
  \begin{subfigure}[a]{1\textwidth}
    \center
    \includegraphics[scale=0.4]{images/linearTemplate.png}
    \caption{An execution with read responses left unspecified.}
    \label{fig:nonlinear}
  \end{subfigure}
  \begin{subfigure}[b]{1\textwidth}
    \center
    \includegraphics[scale=0.4]{images/linear3.png}
    \caption{A linearizable execution for which both reads return $1$.}
  \end{subfigure}
  \begin{subfigure}[c]{1\textwidth}
    \center
    \includegraphics[scale=0.4]{images/linear2.png}
    \caption{A linearizable execution for which both reads return $2$.}
  \end{subfigure}
  \caption{Two linearizable executions of the same underlying events that return different responses. Possible linearization points are shown in red.}
  \label{fig:linearization}
\end{figure}

\begin{figure}[p]
  \begin{subfigure}[a]{1\textwidth}
    \center
    \includegraphics[scale=0.4]{images/nonlinear1.png}
    \caption{A nonlinearizable execution with the read access returning disagreeing values. We will see later (Figure \ref{fig:sequential}) that this execution is still sequentially consistent. }
    \label{fig:nonlinear1}
  \end{subfigure}
  \begin{subfigure}[b]{1\textwidth}
    \center
    \includegraphics[scale=0.4]{images/nonlinear2.png}
    \caption{Another nonlinearizable execution with read access values swapped. This execution is not sequentially consistent.}
    \label{fig:nonlinear2}
  \end{subfigure}
  \caption{Two non-linearizable executions of the same events shown in Figure \ref{fig:linearization}.}
  \label{fig:nonlinearizable}
\end{figure}

\begin{figure}
  \begin{subfigure}[a]{1\textwidth}
    \center
    \includegraphics[scale=0.4]{images/sequential1.png}
    \caption{A non-linearizable, sequentially consistent execution.}
    \label{fig:sequential1}
  \end{subfigure}
  \begin{subfigure}[b]{1\textwidth}
    \center
    \includegraphics[scale=0.4]{images/sequential2.png}
    \caption{An equivalent interleaving of \ref{fig:sequential1}.}
    \label{fig:interleaving1}
  \end{subfigure}
  \caption{A sequentially consistent execution and a possible interleaving.}
  \label{fig:sequential}
\end{figure}

\begin{figure}
  \begin{subfigure}[a]{1\textwidth}
    \center
    \includegraphics[scale=0.4]{images/nonsequential1.png}
    \caption{A non-sequentially consistent execution.}
    \label{fig:nonsequential1}
  \end{subfigure}
  \begin{subfigure}[b]{1\textwidth}
    \center
    \includegraphics[scale=0.4]{images/nonsequential_x.png}
    \caption{The sequentially consistent history of $x$.}
    \label{fig:sequentialx}
  \end{subfigure}
  \begin{subfigure}[b]{1\textwidth}
    \center
    \includegraphics[scale=0.4]{images/nonsequential_y.png}
    \caption{The sequentially consistent history of $y$.}
    \label{fig:sequentialy}
  \end{subfigure}
  \caption{A non-sequentially consistent execution with sequentially-consistent executions at each variable.}
  \label{fig:nonsequential}
\end{figure}
